{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "cnn_model-balanced-colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fwangliberty/AIoTDesign-Frontend/blob/master/cnn_model_balanced_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UHrlHCYsVhVI"
      },
      "source": [
        "# Intrusion Detection based on CICIDS 2017 Data Set (2)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOM-A--9VhVS"
      },
      "source": [
        "We use the pre-processing dataset from mlp4nids (Multi-layer perceptron for network intrusion detection). https://github.com/ArnaudRosay/mlp4nids. Use another colab script to conver parquet files to csv files. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTe0kED3VhVT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af87e7eb-5c13-4e6f-d8b8-0ce97e4d43af"
      },
      "source": [
        "import os\n",
        "from os.path import join\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "%load_ext autoreload"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iI9jwuFVhVV"
      },
      "source": [
        "def display_all(df):\n",
        "    with pd.option_context(\"display.max_rows\", 100, \"display.max_columns\", 100): \n",
        "        print(df)"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xZFOb0S1VhVV"
      },
      "source": [
        "def make_value2index(attacks):\n",
        "    #make dictionary\n",
        "    attacks = sorted(attacks)\n",
        "    d = {}\n",
        "    counter=0\n",
        "    for attack in attacks:\n",
        "        d[attack] = counter\n",
        "        counter+=1\n",
        "    return d"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hhLnsYxeVhVW"
      },
      "source": [
        "# chganges label from string to integer/index\n",
        "def encode_label(Y_str):\n",
        "    labels_d = make_value2index(np.unique(Y_str))\n",
        "    Y = [labels_d[y_str] for y_str  in Y_str]\n",
        "    Y = np.array(Y)\n",
        "    return np.array(Y)"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "32nfwRGiVhVW"
      },
      "source": [
        "## Step 1. Loading csv files"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dt7z9tDHVxkD"
      },
      "source": [
        "Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATXKRQN5VwRk",
        "outputId": "df46a064-69c0-4438-9028-d3d625c1062a"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ASr9N_gQVhVW"
      },
      "source": [
        "# All columns\n",
        "col_names = np.array(['Source Port', 'Destination Port',\n",
        "                      'Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets',\n",
        "                      'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean',\n",
        "                      'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\n",
        "                      'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total',\n",
        "                      'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\n",
        "                      'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Length', 'Bwd Header Length',\n",
        "                      'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std',\n",
        "                      'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\n",
        "                      'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size',\n",
        "                      'Avg Bwd Segment Size','Subflow Fwd Packets', 'Subflow Fwd Bytes',\n",
        "                      'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward',\n",
        "                      'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\n",
        "                      'Idle Std', 'Idle Max', 'Idle Min', 'Label'])"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uILOayLZVhVY"
      },
      "source": [
        "# load three csv files generated by mlp4nids (Multi-layer perceptron for network intrusion detection )\n",
        "# first load the train set\n",
        "df_train = pd.read_csv('/content/drive/My Drive/CICIDS2017/train_set.csv',names=col_names, skiprows=1)  "
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRPkCsdlVhVZ",
        "outputId": "82438870-8efa-4ed2-aae9-a6bc0b9f339d"
      },
      "source": [
        "# Here we can see the number of rows and columns for each table.\n",
        "print(df_train.shape)"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(556548, 72)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ICny2va3VhVb"
      },
      "source": [
        "Count the number of attacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Yanv7FqVhVb",
        "outputId": "94700a0a-3b4a-48e8-b2d2-9e3463c63929"
      },
      "source": [
        "df_train['Label'].value_counts()"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        278274\n",
              "DoS Hulk                      115062\n",
              "PortScan                       79402\n",
              "DDoS                           64012\n",
              "DoS GoldenEye                   5146\n",
              "FTP-Patator                     3967\n",
              "SSH-Patator                     2948\n",
              "DoS slowloris                   2898\n",
              "DoS Slowhttptest                2749\n",
              "Bot                              978\n",
              "Web Attack  Brute Force         753\n",
              "Web Attack  XSS                 326\n",
              "Infiltration                      18\n",
              "Web Attack  Sql Injection        10\n",
              "Heartbleed                         5\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_v-dyNq5VhVc"
      },
      "source": [
        "Read test and validation sets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wBfNHRNJVhVd",
        "outputId": "35b2c53f-aeba-415c-d076-1a1f528832ad"
      },
      "source": [
        "df_test = pd.read_csv('/content/drive/My Drive/CICIDS2017/test_set.csv',names=col_names, skiprows=1)  \n",
        "print('Test set size: ', df_test.shape)\n",
        "\n",
        "df_val = pd.read_csv('/content/drive/My Drive/CICIDS2017/crossval_set.csv',names=col_names, skiprows=1)  \n",
        "print('Validation set size: ', df_val.shape)"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set size:  (278270, 72)\n",
            "Validation set size:  (278270, 72)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1_j8CVGVhVe"
      },
      "source": [
        "Distribution of different attack cases"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTVt_7VXVhVe",
        "outputId": "cf822e47-578b-43c9-9a6d-db509bf14cf5"
      },
      "source": [
        "print('Test set: ')\n",
        "df_test['Label'].value_counts()"
      ],
      "execution_count": 66,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        139135\n",
              "DoS Hulk                       57531\n",
              "PortScan                       39701\n",
              "DDoS                           32006\n",
              "DoS GoldenEye                   2573\n",
              "FTP-Patator                     1983\n",
              "SSH-Patator                     1474\n",
              "DoS slowloris                   1449\n",
              "DoS Slowhttptest                1374\n",
              "Bot                              489\n",
              "Web Attack  Brute Force         376\n",
              "Web Attack  XSS                 163\n",
              "Infiltration                       9\n",
              "Web Attack  Sql Injection         5\n",
              "Heartbleed                         2\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 66
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2o3LPdwDVhVe",
        "outputId": "a83c1d65-eb53-4599-f327-d106347d9413"
      },
      "source": [
        "print('Validation set: ')\n",
        "df_val['Label'].value_counts()"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation set: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        139135\n",
              "DoS Hulk                       57531\n",
              "PortScan                       39701\n",
              "DDoS                           32006\n",
              "DoS GoldenEye                   2573\n",
              "FTP-Patator                     1983\n",
              "SSH-Patator                     1474\n",
              "DoS slowloris                   1449\n",
              "DoS Slowhttptest                1374\n",
              "Bot                              489\n",
              "Web Attack  Brute Force         376\n",
              "Web Attack  XSS                 163\n",
              "Infiltration                       9\n",
              "Web Attack  Sql Injection         5\n",
              "Heartbleed                         2\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wC4QBhkAVhVf"
      },
      "source": [
        "## Step 2. Normalization\n",
        "\n",
        "The continuous feature values are normalized into the same feature space. This is important when using features that have different measurements, and is a general requirement of many machine learning algorithms. Therefore, the values for this dataset are also normalized using the Min-Max scaling technique, bringing them all within a range of [0,1]."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFgZB_NfVhVf"
      },
      "source": [
        "### Step 2.1 Encoding train dataset\n",
        "Encoding the labels, and generate numpy array. Note that the label has not been encoded as one-hot coding. We will use one-hot code later. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IQxKTIT9VhVg"
      },
      "source": [
        "df_label = df_train['Label']\n",
        "data = df_train.drop(columns=['Label'])\n",
        "X_train = data.values\n",
        "y_train = encode_label(df_label.values)"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SieMK6-cVhVg"
      },
      "source": [
        "### Step 2.2 Normalizing train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IKi1kteuVhVh"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ynaAVC63VhVh",
        "outputId": "03a51a6e-2855-455d-c06d-4108831e458d"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_train"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
              "        1.64199192e-01, 2.67500000e-01, 6.67991250e-02],\n",
              "       [8.60215054e-04, 3.36651758e-04, 0.00000000e+00, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 0.00000000e+00, 0.00000000e+00, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       ...,\n",
              "       [0.00000000e+00, 6.98552397e-03, 1.61354247e-02, ...,\n",
              "        0.00000000e+00, 6.92500000e-01, 6.92500000e-01],\n",
              "       [0.00000000e+00, 3.19819170e-03, 2.81067803e-03, ...,\n",
              "        9.18180611e-05, 2.06666667e-01, 2.06666667e-01],\n",
              "       [0.00000000e+00, 4.59155592e-02, 5.83030421e-02, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 70
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mcAOekqBVhVh"
      },
      "source": [
        "### Step 2.3. Encoding test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "id": "FHh3Xfs6VhVi"
      },
      "source": [
        "df_label = df_test['Label']\n",
        "data = df_test.drop(columns=['Label'])\n",
        "X_test = data.values\n",
        "y_test = encode_label(df_label.values)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R_K42JmRVhVi"
      },
      "source": [
        "### Step 2.4. Normalizing test dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6FhdSoOHVhVj",
        "outputId": "41064963-b3e1-4de7-fed0-64bfc1cc2991"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_test = scaler.fit_transform(X_test)\n",
        "X_test"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.00000000e+00, 1.88574865e-02, 3.72734867e-02, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [2.61501211e-02, 9.09200241e-03, 0.00000000e+00, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [1.79176755e-02, 6.22970535e-03, 0.00000000e+00, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       ...,\n",
              "       [1.45278450e-02, 5.05111245e-03, 0.00000000e+00, ...,\n",
              "        0.00000000e+00, 0.00000000e+00, 0.00000000e+00],\n",
              "       [0.00000000e+00, 5.40889958e-03, 1.83091266e-02, ...,\n",
              "        8.72072356e-05, 8.33333333e-02, 8.32840583e-02],\n",
              "       [0.00000000e+00, 6.50330728e-03, 1.88504848e-02, ...,\n",
              "        0.00000000e+00, 8.22500000e-01, 8.22500000e-01]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TO6uaTIJVhVj"
      },
      "source": [
        "### Step 2.5 Encoding validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yAE5BhkAVhVk"
      },
      "source": [
        "df_label = df_val['Label']\n",
        "data = df_val.drop(columns=['Label'])\n",
        "X_val = data.values\n",
        "y_val = encode_label(df_label.values)"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QqGf7AViJ9J"
      },
      "source": [
        "### Step 2.6. Normalizing validation dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_6siB3IBiHLz",
        "outputId": "9e4d9b47-b515-4dc8-b742-27820a68f081"
      },
      "source": [
        "scaler = MinMaxScaler()\r\n",
        "X_val = scaler.fit_transform(X_val)\r\n",
        "X_val"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.0234824 , 0.03596298, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.00078058, 0.00059772, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.00169125, 0.00177089, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.03922518, 0.01580671, 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gMCdGVokVhVk"
      },
      "source": [
        "## Step 3 One-hot encoding for labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VjZYdE6DVhVl"
      },
      "source": [
        "y_train, y_test and y_val have to be one-hot-encoded. That means they must have dimension (number_of_samples, 15), where 15 denotes number of classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sJLUjCbDVhVm"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jm9z7MNgVhVm"
      },
      "source": [
        "y_train = to_categorical(y_train, 15)\n",
        "y_test = to_categorical(y_test, 15)\n",
        "y_val = to_categorical(y_val, 15)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uATqQNwsVhVm"
      },
      "source": [
        "## Step 4. Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0apHzPg8VhVn"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, BatchNormalization, Flatten, Dense, Activation,Dropout\n",
        "from tensorflow.keras.constraints import max_norm"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CnoOCpmMVhVn",
        "outputId": "8ddc2500-c4cd-4475-8313-e86dbf1dfe52"
      },
      "source": [
        "#hyper-params\n",
        "batch_size = 1024 # increasing batch size with more gpu added\n",
        "input_dim = X_train.shape[1]\n",
        "num_class = 15                   # 15 intrusion classes, including benign traffic class\n",
        "num_epochs = 30\n",
        "learning_rates = 1e-3\n",
        "regularizations = 1e-3\n",
        "optim = tf.keras.optimizers.Adam(lr=learning_rates, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\n",
        "\n",
        "print(input_dim)\n",
        "print(num_class)"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71\n",
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bLwYgMMEVhVo",
        "outputId": "637f99f1-ee3e-4cb9-d002-86cda347bd78"
      },
      "source": [
        "#X_train_r = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], 1))\n",
        "X_train_r = np.zeros((len(X_train), input_dim, 1))\n",
        "X_train_r[:, :, 0] = X_train[:, :input_dim]\n",
        "print(X_train_r.shape)"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(556548, 71, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HDWet-dFVhVp",
        "outputId": "afca0f2b-29b0-4e9a-c1e3-8624339f020e"
      },
      "source": [
        "X_test_r = np.zeros((len(X_test), input_dim, 1))\n",
        "X_test_r[:, :, 0] = X_test[:, :input_dim]\n",
        "print(X_test_r.shape)"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(278270, 71, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vzf1idPdVhVq",
        "outputId": "49ca2764-7a05-4108-c8f7-93ed0da702bd"
      },
      "source": [
        "X_val_r = np.zeros((len(X_val), input_dim, 1))\n",
        "X_val_r[:, :, 0] = X_val[:, :input_dim]\n",
        "print(X_val_r.shape)"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(278270, 71, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rBezIX_VhVr",
        "outputId": "8beb16f3-f4b9-4572-a5bd-be0362ffe002"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# input layer\n",
        "model.add(Conv1D(filters=64, kernel_size=3, padding='same', input_shape=(71,1)))\n",
        "model.add(BatchNormalization(axis=1))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "\n",
        "model.add(Conv1D(filters=128, kernel_size=3))\n",
        "model.add(BatchNormalization(axis=1))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(num_class))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_2 (Conv1D)            (None, 71, 64)            256       \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 71, 64)            284       \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 71, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_3 (Conv1D)            (None, 69, 128)           24704     \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 69, 128)           276       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 69, 128)           0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 8832)              0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               883300    \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 15)                1515      \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 15)                0         \n",
            "=================================================================\n",
            "Total params: 910,335\n",
            "Trainable params: 910,055\n",
            "Non-trainable params: 280\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mRyLt02XVhVs"
      },
      "source": [
        "model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy']) "
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDH9NG7WVhVs"
      },
      "source": [
        "## Step 5. Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vp7vl70tVhVt",
        "outputId": "b9c40baa-7b41-4a2d-8795-b326738fe055"
      },
      "source": [
        "# fit network\n",
        "model.fit(X_train_r, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_val_r, y_val), verbose=1)"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/30\n",
            "544/544 [==============================] - 14s 24ms/step - loss: 0.3393 - accuracy: 0.8928 - val_loss: 0.1502 - val_accuracy: 0.9589\n",
            "Epoch 2/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0926 - accuracy: 0.9681 - val_loss: 0.1158 - val_accuracy: 0.9645\n",
            "Epoch 3/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0792 - accuracy: 0.9720 - val_loss: 0.0883 - val_accuracy: 0.9711\n",
            "Epoch 4/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0732 - accuracy: 0.9738 - val_loss: 0.0956 - val_accuracy: 0.9571\n",
            "Epoch 5/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0637 - accuracy: 0.9774 - val_loss: 0.0856 - val_accuracy: 0.9661\n",
            "Epoch 6/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0588 - accuracy: 0.9793 - val_loss: 0.0743 - val_accuracy: 0.9762\n",
            "Epoch 7/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0563 - accuracy: 0.9801 - val_loss: 0.0666 - val_accuracy: 0.9798\n",
            "Epoch 8/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0521 - accuracy: 0.9815 - val_loss: 0.0866 - val_accuracy: 0.9669\n",
            "Epoch 9/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0514 - accuracy: 0.9820 - val_loss: 0.1136 - val_accuracy: 0.9610\n",
            "Epoch 10/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0447 - accuracy: 0.9843 - val_loss: 0.1016 - val_accuracy: 0.9689\n",
            "Epoch 11/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0362 - accuracy: 0.9875 - val_loss: 0.0855 - val_accuracy: 0.9733\n",
            "Epoch 12/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0343 - accuracy: 0.9890 - val_loss: 0.0595 - val_accuracy: 0.9736\n",
            "Epoch 13/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0300 - accuracy: 0.9904 - val_loss: 0.0703 - val_accuracy: 0.9784\n",
            "Epoch 14/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0420 - accuracy: 0.9871 - val_loss: 0.0973 - val_accuracy: 0.9623\n",
            "Epoch 15/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0313 - accuracy: 0.9896 - val_loss: 0.0790 - val_accuracy: 0.9671\n",
            "Epoch 16/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0235 - accuracy: 0.9926 - val_loss: 0.0865 - val_accuracy: 0.9728\n",
            "Epoch 17/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0365 - accuracy: 0.9892 - val_loss: 0.1768 - val_accuracy: 0.9473\n",
            "Epoch 18/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0239 - accuracy: 0.9928 - val_loss: 0.1596 - val_accuracy: 0.9537\n",
            "Epoch 19/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0218 - accuracy: 0.9933 - val_loss: 0.1508 - val_accuracy: 0.9549\n",
            "Epoch 20/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0206 - accuracy: 0.9934 - val_loss: 0.1424 - val_accuracy: 0.9543\n",
            "Epoch 21/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0367 - accuracy: 0.9887 - val_loss: 0.0630 - val_accuracy: 0.9815\n",
            "Epoch 22/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0287 - accuracy: 0.9904 - val_loss: 0.1232 - val_accuracy: 0.9740\n",
            "Epoch 23/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0212 - accuracy: 0.9933 - val_loss: 0.1366 - val_accuracy: 0.9617\n",
            "Epoch 24/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0252 - accuracy: 0.9920 - val_loss: 0.1156 - val_accuracy: 0.9566\n",
            "Epoch 25/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0684 - accuracy: 0.9807 - val_loss: 0.1262 - val_accuracy: 0.9619\n",
            "Epoch 26/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0205 - accuracy: 0.9937 - val_loss: 0.1386 - val_accuracy: 0.9589\n",
            "Epoch 27/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0214 - accuracy: 0.9932 - val_loss: 0.1737 - val_accuracy: 0.9580\n",
            "Epoch 28/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0189 - accuracy: 0.9941 - val_loss: 0.1122 - val_accuracy: 0.9746\n",
            "Epoch 29/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0185 - accuracy: 0.9941 - val_loss: 0.1299 - val_accuracy: 0.9721\n",
            "Epoch 30/30\n",
            "544/544 [==============================] - 13s 23ms/step - loss: 0.0504 - accuracy: 0.9864 - val_loss: 0.1164 - val_accuracy: 0.9721\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f523913c7b8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 86
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tCgwcslPVhVt"
      },
      "source": [
        "# evaluate model\n",
        "accuracy = model.evaluate(X_test_r, y_test, batch_size=batch_size, verbose=0)"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1UGo1sd3-Qo"
      },
      "source": [
        "Save the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_rq2fiC39qY"
      },
      "source": [
        "model.save('/content/drive/My Drive/CICIDS2017/cicids2017cnn.h5')"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idbRflOkkunA"
      },
      "source": [
        "## Step 6. Calculate Precision, Recall, and F-sore"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WihpW4LbVhVt"
      },
      "source": [
        "Classification accuracy is the total number of correct predictions divided by the total number of predictions made for a dataset. As a performance measure, accuracy is inappropriate for imbalanced classification problems. The main reason is that the overwhelming number of examples from the majority class (or classes) will overwhelm the number of examples in the minority class, meaning that even unskillful models can achieve accuracy scores of 90 percent, or 99 percent, depending on how severe the class imbalance happens to be.\r\n",
        "\r\n",
        "An alternative to using classification accuracy is to use precision and recall metrics."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mg6vs3vPtRiv"
      },
      "source": [
        "# demonstration of calculating metrics for a neural network model using sklearn\r\n",
        "from sklearn.metrics import accuracy_score\r\n",
        "from sklearn.metrics import precision_score\r\n",
        "from sklearn.metrics import recall_score\r\n",
        "from sklearn.metrics import f1_score\r\n",
        "from sklearn.metrics import cohen_kappa_score\r\n",
        "from sklearn.metrics import roc_auc_score\r\n",
        "from sklearn.metrics import confusion_matrix"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MUvtnIroziHC"
      },
      "source": [
        "df_label = df_test['Label']\r\n",
        "ytest = encode_label(df_label.values)"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GMMvAP93tLmz",
        "outputId": "38e04596-0483-4920-82a1-1acda26ef966"
      },
      "source": [
        "# predict probabilities for test set\r\n",
        "yhat_probs = model.predict(X_test_r, verbose=0)\r\n",
        "\r\n",
        "# predict crisp classes for test set\r\n",
        "yhat_classes = model.predict_classes(X_test_r, verbose=0)\r\n",
        "\r\n",
        "# reduce to 1d array\r\n",
        "yhat_probs = yhat_probs[:, 0]\r\n",
        "#yhat_classes = yhat_classes[:, 0]"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype(\"int32\")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).\n",
            "  warnings.warn('`model.predict_classes()` is deprecated and '\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UkH69KHwyGTZ",
        "outputId": "1afa6b9d-3016-4b35-ec6d-3c6c5d35a2cc"
      },
      "source": [
        "print(yhat_probs.shape)\r\n",
        "print(ytest.shape)"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(278270,)\n",
            "(278270,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tId99uaYto6f"
      },
      "source": [
        "**Accuracy**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jVqd5_BAtoMn",
        "outputId": "016ac288-8fb5-4bda-a007-db92a09f06cb"
      },
      "source": [
        "# accuracy: (tp + tn) / (p + n)\r\n",
        "accuracy = accuracy_score(ytest, yhat_classes)\r\n",
        "print('Accuracy: %f' % accuracy)"
      ],
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.983814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTm8k-rokOeq"
      },
      "source": [
        "**Precision**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fPV7hHbBpO6C"
      },
      "source": [
        "Precision calculates the accuracy for the minority class. It is calculated as the ratio of correctly predicted positive examples divided by the total number of positive examples that were predicted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U6mMkUVzn_Hs",
        "outputId": "f174a89b-b35d-4493-fe0d-b1ff2109a6dd"
      },
      "source": [
        "# precision tp / (tp + fp)\r\n",
        "precision = precision_score(ytest, yhat_classes, labels=[1,2], average='micro')\r\n",
        "# labels is a list of all possible class labels\r\n",
        "print('Precision: %f' % precision)"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Precision: 0.999441\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YknKgTY4iyBK"
      },
      "source": [
        "**Recall**\r\n",
        "\r\n",
        "Recall is the ratio of true positives to the ground-truth positives in the sample. Unlike Precision, Recall also considers the number of positive (minority) cases that were not classified as such."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C11PO0DEjRR6",
        "outputId": "8972079a-082b-45b9-8f60-21d73fe7ee3c"
      },
      "source": [
        "# recall: tp / (tp + fn)\r\n",
        "recall = recall_score(ytest, yhat_classes, average='weighted')\r\n",
        "print('Recall: %f' % recall)"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Recall: 0.983814\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X7FcTR0endeX"
      },
      "source": [
        "**F-Score**\r\n",
        "\r\n",
        "Precision is appropriate when we are more concerned with minimizing false positives, while Recall is appropriate when the number of false negatives is more critica"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PzTvXOpUuUgB",
        "outputId": "9f0e0c90-40de-4233-97cb-f02b40b80df9"
      },
      "source": [
        "# f1: 2 tp / (2 tp + fp + fn)\r\n",
        "f1 = f1_score(ytest, yhat_classes, average='weighted')\r\n",
        "print('F1 score: %f' % f1)"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "F1 score: 0.982994\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j2rk3q0k1mcj",
        "outputId": "7e73e072-5d0a-4fa1-9ddd-b6b5505818c7"
      },
      "source": [
        "# kappa\r\n",
        "kappa = cohen_kappa_score(ytest, yhat_classes)\r\n",
        "print('Cohens kappa: %f' % kappa)\r\n",
        "\r\n",
        "# confusion matrix\r\n",
        "matrix = confusion_matrix(ytest, yhat_classes)\r\n",
        "print(matrix)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cohens kappa: 0.975991\n",
            "[[136519      4     14     36    411     40      7      5      0      0\n",
            "    1983      0    111      0      5]\n",
            " [   307    182      0      0      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [    18      0  31984      3      1      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [    10      0      0   2539     22      2      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [   391      0      0      2  57138      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [    14      0      0      1      0   1346     10      0      0      0\n",
            "       0      0      3      0      0]\n",
            " [    43      0      0      0      0      8   1397      0      0      0\n",
            "       0      0      1      0      0]\n",
            " [    29      0      0      0      0      0      0   1948      0      0\n",
            "       0      5      1      0      0]\n",
            " [     0      0      0      0      0      0      0      0      2      0\n",
            "       0      0      0      0      0]\n",
            " [     7      0      0      0      0      0      0      0      0      2\n",
            "       0      0      0      0      0]\n",
            " [    38      0      0      0     10      0      0      0      0      0\n",
            "   39653      0      0      0      0]\n",
            " [   756      0      0      0      2      0      0      0      0      0\n",
            "       0    716      0      0      0]\n",
            " [    37      0      0      0      0      0      0      0      0      0\n",
            "       0      0    338      0      1]\n",
            " [     4      0      0      1      0      0      0      0      0      0\n",
            "       0      0      0      0      0]\n",
            " [     7      0      0      1      0      0      0      0      0      0\n",
            "       0      0    153      0      2]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}