{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "cnn_small_cicids2017_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fwangliberty/AIoTDesign-Frontend/blob/master/cnn_small_cicids2017_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HakdgCXBymAw"
      },
      "source": [
        "# Intrusion Detection by using small CICIDS 2017 DataSet with 2000 examples each type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqdB1_F0ymAz"
      },
      "source": [
        "import os\n",
        "from os.path import join\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7LnjNAcvll6"
      },
      "source": [
        "def make_value2index(attacks):\r\n",
        "    #make dictionary\r\n",
        "    attacks = sorted(attacks)\r\n",
        "    d = {}\r\n",
        "    counter=0\r\n",
        "    for attack in attacks:\r\n",
        "        d[attack] = counter\r\n",
        "        counter+=1\r\n",
        "    return d"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bJxEKODvmH5"
      },
      "source": [
        "# chganges label from string to integer/index\r\n",
        "def encode_label(Y_str):\r\n",
        "    labels_d = make_value2index(np.unique(Y_str))\r\n",
        "    Y = [labels_d[y_str] for y_str  in Y_str]\r\n",
        "    Y = np.array(Y)\r\n",
        "    return np.array(Y)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW4i5wbIrCfT"
      },
      "source": [
        "def get_dataframe_ofType(df, traffic_type):\r\n",
        "    \"\"\"\r\n",
        "    Analyze traffic distribution of pandas data frame containing IDS2017 CSV\r\n",
        "    file with labelled traffic\r\n",
        "\r\n",
        "    Parameter\r\n",
        "    ---------\r\n",
        "    df: DataFrame\r\n",
        "        Pandas DataFrame corresponding to the content of a CSV file\r\n",
        "    traffic_type: string\r\n",
        "        name corresponding to traffic type\r\n",
        "\r\n",
        "    Return\r\n",
        "    ------\r\n",
        "    req_df: DataFrame\r\n",
        "        Pandas DataFrame containing only the requested traffic type\r\n",
        "    \"\"\"\r\n",
        "    req_df = df.loc[df['Label'] == traffic_type]\r\n",
        "    # don't keep original indexes\r\n",
        "    #req_df = req_df.reset_index()\r\n",
        "    return req_df"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crMkU4GxphLu"
      },
      "source": [
        "def get_typelist(df):\r\n",
        "    \"\"\"\r\n",
        "    Extract traffic type from a pandas data frame containing IDS2017 CSV\r\n",
        "    file with labelled traffic\r\n",
        "\r\n",
        "    Parameter\r\n",
        "    ---------\r\n",
        "    df: DataFrame\r\n",
        "        Pandas DataFrame corresponding to the content of a CSV file\r\n",
        "\r\n",
        "    Return\r\n",
        "    ------\r\n",
        "    traffic_type_list: list\r\n",
        "        List of traffic types contained in the DataFrame\r\n",
        "    \"\"\"\r\n",
        "    traffic_type_list = df['Label'].value_counts().index.tolist()\r\n",
        "    return traffic_type_list"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyxQXnv_uJok"
      },
      "source": [
        "#We balance data as follows:\r\n",
        "#1) oversample small classes so that their population/count is equal to mean_number_of_samples_per_class\r\n",
        "#2) undersample large classes so that their count is equal to mean_number_of_samples_per_class\r\n",
        "def balance_data(X,y,seed, mean_samples):\r\n",
        "    np.random.seed(seed)\r\n",
        "    unique,counts = np.unique(y,return_counts=True)\r\n",
        "    mean_samples_per_class = mean_samples # int(round(np.mean(counts)))\r\n",
        "    N,D = X.shape #(number of examples, number of features)\r\n",
        "    new_X = np.empty((0,D)) \r\n",
        "    new_y = np.empty((0),dtype=int)\r\n",
        "    for i,c in enumerate(unique):\r\n",
        "        temp_x = X[y==c]\r\n",
        "        indices = np.random.choice(temp_x.shape[0],mean_samples_per_class) # gets `mean_samples_per_class` indices of class `c`\r\n",
        "        new_X = np.concatenate((new_X,temp_x[indices]),axis=0) # now we put new data into new_X \r\n",
        "        temp_y = np.ones(mean_samples_per_class,dtype=int)*c\r\n",
        "        new_y = np.concatenate((new_y,temp_y),axis=0)\r\n",
        "        \r\n",
        "    # in order to break class order in data we need shuffling\r\n",
        "    indices = np.arange(new_y.shape[0])\r\n",
        "    np.random.shuffle(indices)\r\n",
        "    new_X =  new_X[indices,:]\r\n",
        "    new_y = new_y[indices]\r\n",
        "    return (new_X,new_y)"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-kNtwdKymAz"
      },
      "source": [
        "## Step 1. Read cleaned CICIDS2017 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is2az0Giy0iC"
      },
      "source": [
        "Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9zJ-0oQy1Gy",
        "outputId": "d7f89085-4d2c-4526-a567-10ae29cc3c36"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aL5u5kynL_B"
      },
      "source": [
        "# All columns\r\n",
        "col_names = np.array(['Source Port', 'Destination Port',\r\n",
        "                      'Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets',\r\n",
        "                      'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean',\r\n",
        "                      'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\r\n",
        "                      'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total',\r\n",
        "                      'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\r\n",
        "                      'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Length', 'Bwd Header Length',\r\n",
        "                      'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std',\r\n",
        "                      'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\r\n",
        "                      'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size',\r\n",
        "                      'Avg Bwd Segment Size','Subflow Fwd Packets', 'Subflow Fwd Bytes',\r\n",
        "                      'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward',\r\n",
        "                      'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\r\n",
        "                      'Idle Std', 'Idle Max', 'Idle Min', 'Label'])"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSfAZhiPCwpu"
      },
      "source": [
        "col_important = np.array(['Source Port', 'Destination Port', 'Fwd IAT Min', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'Flow IAT Min',\r\n",
        "                      'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets',\r\n",
        "                      'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Protocol', 'Fwd Packet Length Min', 'Fwd Packet Length Mean',\r\n",
        "                      'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\r\n",
        "                      'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Fwd IAT Total',\r\n",
        "                      'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\r\n",
        "                      'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Length', 'Bwd Header Length',\r\n",
        "                      'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std',\r\n",
        "                      'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\r\n",
        "                      'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size',\r\n",
        "                      'Avg Bwd Segment Size','Subflow Fwd Packets', 'Subflow Fwd Bytes',\r\n",
        "                      'Subflow Bwd Packets', 'Subflow Bwd Bytes', \r\n",
        "                      'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\r\n",
        "                      'Idle Std', 'Idle Max', 'Idle Min', 'Label'])"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E17GpxaNymAz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "9b8e6702-9b6e-4afb-b215-f9cc495e8cc1"
      },
      "source": [
        "# load train data\n",
        "#df_train = pd.read_csv('/content/drive/My Drive/CICIDS2017/train_set.csv',names=col_names, skiprows=1)  \n",
        "df_train = pd.read_csv('/content/drive/My Drive/CICIDS2017/train_set.csv',names=col_important, skiprows=1) \n",
        "df_train.head()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Source Port</th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5141</th>\n",
              "      <th>172.16.0.1-192.168.10.50-50294-80-6</th>\n",
              "      <th>172.16.0.1</th>\n",
              "      <th>50294.0</th>\n",
              "      <th>192.168.10.50</th>\n",
              "      <th>80.0</th>\n",
              "      <th>6.0</th>\n",
              "      <th>5/7/2017 10:33</th>\n",
              "      <th>63101744.0</th>\n",
              "      <th>7.0</th>\n",
              "      <th>0.0</th>\n",
              "      <th>0.0</th>\n",
              "      <th>0.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.110932</td>\n",
              "      <td>10500000.0</td>\n",
              "      <td>1.190000e+07</td>\n",
              "      <td>32100000.0</td>\n",
              "      <td>998158.0</td>\n",
              "      <td>63100000.0</td>\n",
              "      <td>1.050000e+07</td>\n",
              "      <td>1.190000e+07</td>\n",
              "      <td>32100000.0</td>\n",
              "      <td>998158.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>280.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.110932</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>280.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29200.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>7006133.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7006133.0</td>\n",
              "      <td>7006133.0</td>\n",
              "      <td>18700000.0</td>\n",
              "      <td>12200000.0</td>\n",
              "      <td>32100000.0</td>\n",
              "      <td>8015895.0</td>\n",
              "      <td>DoS Slowhttptest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40963</th>\n",
              "      <th>172.16.0.1-192.168.10.50-37796-1199-6</th>\n",
              "      <th>172.16.0.1</th>\n",
              "      <th>37796.0</th>\n",
              "      <th>192.168.10.50</th>\n",
              "      <th>1199.0</th>\n",
              "      <th>6.0</th>\n",
              "      <th>7/7/2017 2:52</th>\n",
              "      <th>62.0</th>\n",
              "      <th>1.0</th>\n",
              "      <th>1.0</th>\n",
              "      <th>2.0</th>\n",
              "      <th>6.0</th>\n",
              "      <th>2.0</th>\n",
              "      <td>2.0</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.290323e+05</td>\n",
              "      <td>32258.064520</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>62.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>16129.032260</td>\n",
              "      <td>16129.032260</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>2.309401</td>\n",
              "      <td>5.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>PortScan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27718</th>\n",
              "      <th>172.217.6.200-192.168.10.12-443-42634-6</th>\n",
              "      <th>172.217.6.200</th>\n",
              "      <th>443.0</th>\n",
              "      <th>192.168.10.12</th>\n",
              "      <th>42634.0</th>\n",
              "      <th>6.0</th>\n",
              "      <th>03/07/2017 09:49:12</th>\n",
              "      <th>3.0</th>\n",
              "      <th>2.0</th>\n",
              "      <th>0.0</th>\n",
              "      <th>0.0</th>\n",
              "      <th>0.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>666666.666667</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>666666.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>357.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106492</th>\n",
              "      <th>192.168.10.8-23.208.79.206-52235-443-6</th>\n",
              "      <th>192.168.10.8</th>\n",
              "      <th>52235.0</th>\n",
              "      <th>23.208.79.206</th>\n",
              "      <th>443.0</th>\n",
              "      <th>6.0</th>\n",
              "      <th>4/7/2017 11:46</th>\n",
              "      <th>5007496.0</th>\n",
              "      <th>7.0</th>\n",
              "      <th>4.0</th>\n",
              "      <th>1679.0</th>\n",
              "      <th>152.0</th>\n",
              "      <th>1080.0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>239.857143</td>\n",
              "      <td>415.237052</td>\n",
              "      <td>152.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>3.656518e+02</td>\n",
              "      <td>2.196707</td>\n",
              "      <td>500749.6</td>\n",
              "      <td>1.543257e+06</td>\n",
              "      <td>4892570.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5007496.0</td>\n",
              "      <td>8.345827e+05</td>\n",
              "      <td>2.018795e+06</td>\n",
              "      <td>4955369.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>87090.0</td>\n",
              "      <td>29030.0</td>\n",
              "      <td>31709.63089</td>\n",
              "      <td>63179.0</td>\n",
              "      <td>515.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>1.397904</td>\n",
              "      <td>0.798802</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1080.0</td>\n",
              "      <td>152.583333</td>\n",
              "      <td>327.660428</td>\n",
              "      <td>107361.356100</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>166.454545</td>\n",
              "      <td>239.857143</td>\n",
              "      <td>38.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1679.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>8192.0</td>\n",
              "      <td>946.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63559</th>\n",
              "      <th>192.168.10.14-52.84.30.203-59835-80-6</th>\n",
              "      <th>52.84.30.203</th>\n",
              "      <th>80.0</th>\n",
              "      <th>192.168.10.14</th>\n",
              "      <th>59835.0</th>\n",
              "      <th>6.0</th>\n",
              "      <th>6/7/2017 10:04</th>\n",
              "      <th>4.0</th>\n",
              "      <th>1.0</th>\n",
              "      <th>1.0</th>\n",
              "      <th>6.0</th>\n",
              "      <th>6.0</th>\n",
              "      <th>6.0</th>\n",
              "      <td>6.0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.000000e+06</td>\n",
              "      <td>500000.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>250000.000000</td>\n",
              "      <td>250000.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>256.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                           Source Port  ...             Label\n",
              "5141   172.16.0.1-192.168.10.50-50294-80-6     172.16.0.1    50294.0 192.168.10.50 80.0    6.0 5/7/2017 10:33      63101744.0 7.0 0.0 0.0    0.0   0.0             0.0  ...  DoS Slowhttptest\n",
              "40963  172.16.0.1-192.168.10.50-37796-1199-6   172.16.0.1    37796.0 192.168.10.50 1199.0  6.0 7/7/2017 2:52       62.0       1.0 1.0 2.0    6.0   2.0             2.0  ...          PortScan\n",
              "27718  172.217.6.200-192.168.10.12-443-42634-6 172.217.6.200 443.0   192.168.10.12 42634.0 6.0 03/07/2017 09:49:12 3.0        2.0 0.0 0.0    0.0   0.0             0.0  ...            BENIGN\n",
              "106492 192.168.10.8-23.208.79.206-52235-443-6  192.168.10.8  52235.0 23.208.79.206 443.0   6.0 4/7/2017 11:46      5007496.0  7.0 4.0 1679.0 152.0 1080.0          0.0  ...            BENIGN\n",
              "63559  192.168.10.14-52.84.30.203-59835-80-6   52.84.30.203  80.0    192.168.10.14 59835.0 6.0 6/7/2017 10:04      4.0        1.0 1.0 6.0    6.0   6.0             6.0  ...            BENIGN\n",
              "\n",
              "[5 rows x 72 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 77
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXMkWoEUymA0",
        "outputId": "b6ea69c9-5c89-42f5-ac6b-08602f74c0a3"
      },
      "source": [
        "#df_test = pd.read_csv('/content/drive/My Drive/CICIDS2017/test_set.csv',names=col_names, skiprows=1)  \n",
        "df_test = pd.read_csv('/content/drive/My Drive/CICIDS2017/test_set.csv',names=col_important, skiprows=1) \n",
        "print('Test set size: ', df_test.shape)\n",
        "\n",
        "#df_val = pd.read_csv('/content/drive/My Drive/CICIDS2017/crossval_set.csv',names=col_names, skiprows=1)  \n",
        "df_val = pd.read_csv('/content/drive/My Drive/CICIDS2017/crossval_set.csv',names=col_important, skiprows=1) \n",
        "print('Validation set size: ', df_val.shape)"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set size:  (278270, 72)\n",
            "Validation set size:  (278270, 72)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPLtWtJ8ymA1",
        "outputId": "3a7002fb-8f5a-400b-b0ef-ab4308a5daed"
      },
      "source": [
        "# Here we can see the number of rows and columns for each table.\n",
        "print(df_train.shape)"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(556548, 72)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVGdQOpFnhMT"
      },
      "source": [
        "Count the number of attacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK5kP4X3ngMA",
        "outputId": "f13fbd31-f706-454d-c0d3-dd17dbce4b4b"
      },
      "source": [
        "df_train['Label'].value_counts()"
      ],
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        278274\n",
              "DoS Hulk                      115062\n",
              "PortScan                       79402\n",
              "DDoS                           64012\n",
              "DoS GoldenEye                   5146\n",
              "FTP-Patator                     3967\n",
              "SSH-Patator                     2948\n",
              "DoS slowloris                   2898\n",
              "DoS Slowhttptest                2749\n",
              "Bot                              978\n",
              "Web Attack  Brute Force         753\n",
              "Web Attack  XSS                 326\n",
              "Infiltration                      18\n",
              "Web Attack  Sql Injection        10\n",
              "Heartbleed                         5\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCjcgUmlnr0T",
        "outputId": "0582a5e6-9249-401b-dbf0-b6ec6f01814a"
      },
      "source": [
        "print('Test set: ')\r\n",
        "df_test['Label'].value_counts()"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        139135\n",
              "DoS Hulk                       57531\n",
              "PortScan                       39701\n",
              "DDoS                           32006\n",
              "DoS GoldenEye                   2573\n",
              "FTP-Patator                     1983\n",
              "SSH-Patator                     1474\n",
              "DoS slowloris                   1449\n",
              "DoS Slowhttptest                1374\n",
              "Bot                              489\n",
              "Web Attack  Brute Force         376\n",
              "Web Attack  XSS                 163\n",
              "Infiltration                       9\n",
              "Web Attack  Sql Injection         5\n",
              "Heartbleed                         2\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 81
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sIo9YzUnuY5",
        "outputId": "73e8aa2f-66d5-463a-dd1e-e4c23962d08a"
      },
      "source": [
        "print('Validation set: ')\r\n",
        "df_val['Label'].value_counts()"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation set: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        139135\n",
              "DoS Hulk                       57531\n",
              "PortScan                       39701\n",
              "DDoS                           32006\n",
              "DoS GoldenEye                   2573\n",
              "FTP-Patator                     1983\n",
              "SSH-Patator                     1474\n",
              "DoS slowloris                   1449\n",
              "DoS Slowhttptest                1374\n",
              "Bot                              489\n",
              "Web Attack  Brute Force         376\n",
              "Web Attack  XSS                 163\n",
              "Infiltration                       9\n",
              "Web Attack  Sql Injection         5\n",
              "Heartbleed                         2\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzHvvoMDotTj"
      },
      "source": [
        "## Step 2. Randomly Selecting 2000 examples from each type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asjXEVY7yrXa"
      },
      "source": [
        "First, selecting 4000 examples for each type in train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW88xIvao0GO"
      },
      "source": [
        "df_label = df_train['Label']\r\n",
        "data = df_train.drop(columns=['Label'])\r\n",
        "X = data.values\r\n",
        "y = encode_label(df_label.values)"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m32yYQ-prx45",
        "outputId": "41d84f6e-f832-46be-a2e0-c984492e63c6"
      },
      "source": [
        "print(X.shape)\r\n",
        "print(y.shape)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(556548, 71)\n",
            "(556548,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jo9psJWmx7y7",
        "outputId": "dc95f70a-fe1b-43b7-c581-d9ce3d510c28"
      },
      "source": [
        "unique, counts = np.unique(y, return_counts=True)\r\n",
        "print(unique, counts)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14] [278274    978  64012   5146 115062   2749   2898   3967      5     18\n",
            "  79402   2948    753     10    326]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDTCXiY-qtFZ"
      },
      "source": [
        "SEED = 2\r\n",
        "X_train,y_train = balance_data(X,y,seed=SEED, mean_samples=4000)"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks1hpGiFyRi_",
        "outputId": "74183de4-f6d7-4884-f2aa-aed9457e06ff"
      },
      "source": [
        "print(X_train.shape)\r\n",
        "print(y_train.shape)\r\n",
        "unique, counts = np.unique(y_train, return_counts=True)\r\n",
        "print(unique, counts)"
      ],
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 71)\n",
            "(60000,)\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14] [4000 4000 4000 4000 4000 4000 4000 4000 4000 4000 4000 4000 4000 4000\n",
            " 4000]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gua98tTqypEx"
      },
      "source": [
        "Next, selecting 1000 examples from validation datesets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DbW1x3UzCzT"
      },
      "source": [
        "df_label = df_test['Label']\r\n",
        "data = df_test.drop(columns=['Label'])\r\n",
        "X = data.values\r\n",
        "y = encode_label(df_label.values)\r\n",
        "\r\n",
        "SEED = 2\r\n",
        "X_test,y_test = balance_data(X,y,seed=SEED, mean_samples=1000)"
      ],
      "execution_count": 88,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLLES1n7zjAC"
      },
      "source": [
        "Next, selecting 500 examples from test datesets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7VxrFq3zQ8M"
      },
      "source": [
        "df_label = df_val['Label']\r\n",
        "data = df_val.drop(columns=['Label'])\r\n",
        "X = data.values\r\n",
        "y = encode_label(df_label.values)\r\n",
        "\r\n",
        "SEED = 2\r\n",
        "X_val,y_val = balance_data(X,y,seed=SEED, mean_samples=500)"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtjNVDHfymA1"
      },
      "source": [
        "## Step 3. Normalization\n",
        "\n",
        "The continuous feature values are normalized into the same feature space. This is important when using features that have different measurements, and is a general requirement of many machine learning algorithms. Therefore, the values for this dataset are also normalized using the Min-Max scaling technique, bringing them all within a range of [0,1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euTfYdWHymA2"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5JH_i4EymA2",
        "outputId": "6dc5fb01-ee7f-4f52-dfdd-7764c181f28e"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_train"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.00109412, 0.00130398, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.0077991 , 0.0197178 , ..., 0.        , 0.05733978,\n",
              "        0.05733978]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PuA6ri3ymA2",
        "outputId": "2bb7510a-d8d9-4e89-b53a-9ff0eda5bab7"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 71)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 92
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry0R9AFFymA3"
      },
      "source": [
        "X_test = scaler.fit_transform(X_test)"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeFIseICymA3",
        "outputId": "d63d270b-3294-4096-be09-32447774e098"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15000, 71)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 94
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1mN1ZAI0gcL",
        "outputId": "6bb7f545-55d0-4643-d075-7036eddbf4b6"
      },
      "source": [
        "X_val = scaler.fit_transform(X_val)\r\n",
        "X_val"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.00095839, 0.02132314, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.00100857, 0.00039029, 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.01865954, 0.0270547 , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.00302572, 0.00117087, 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.02337832, 0.05175073, ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 95
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGN8Mww10uvV"
      },
      "source": [
        "## Step 4. One-hot encoding for labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XayluHqdymA3"
      },
      "source": [
        "y_train and y_test have to be one-hot-encoded. That means they must have dimension (number_of_samples, 15), where 15 denotes number of classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pMxFRGCymA4"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 96,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na5t3wpmymA4",
        "outputId": "563515de-b1e9-40c6-fdb2-baa23a6ec966"
      },
      "source": [
        "y_train_v = to_categorical(y_train, 15)\n",
        "y_test_v = to_categorical(y_test, 15)\n",
        "y_val_v = to_categorical(y_val, 15)\n",
        "print(y_train_v.shape)\n",
        "print(y_test_v.shape)\n",
        "print(y_val_v.shape)"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 15)\n",
            "(15000, 15)\n",
            "(7500, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79Q2ezxEymA6"
      },
      "source": [
        "## Step 5. Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYdmYYyWymA6"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, BatchNormalization, MaxPooling1D, Flatten, Dense, Activation,Dropout\n",
        "from tensorflow.keras.constraints import max_norm"
      ],
      "execution_count": 98,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VinchNXyymA7",
        "outputId": "d7fad3ab-2bff-4a4e-d4d7-b644d58326ec"
      },
      "source": [
        "#hyper-params\n",
        "batch_size = 256 # increasing batch size with more gpu added\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "num_class = 15                   # 15 intrusion classes, including benign traffic class\n",
        "num_epochs = 90\n",
        "\n",
        "print(input_dim)\n",
        "print(num_class)"
      ],
      "execution_count": 99,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71\n",
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SeAQGIoymA8",
        "outputId": "91c256b6-7e3b-4872-ee0a-77f7c6b36074"
      },
      "source": [
        "X_train_r = np.zeros((len(X_train), input_dim, 1))\n",
        "X_train_r[:, :, 0] = X_train[:, :input_dim]\n",
        "print(X_train_r.shape)"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 71, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlcgM-4311Ee",
        "outputId": "e65a15e0-a157-4d6d-c860-6f1e57f328ed"
      },
      "source": [
        "X_val_r = np.zeros((len(X_val), input_dim, 1))\r\n",
        "X_val_r[:, :, 0] = X_val[:, :input_dim]\r\n",
        "print(X_val_r.shape)"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7500, 71, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B96MRlI_ymA8",
        "outputId": "a43c789d-0543-4a08-cae1-e295bd1a50ae"
      },
      "source": [
        "X_test_r = np.zeros((len(X_test), input_dim, 1))\n",
        "X_test_r[:, :, 0] = X_test[:, :input_dim]\n",
        "print(X_test_r.shape)"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15000, 71, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKMiVwhCXx2I"
      },
      "source": [
        "**Model with 2 Con1D layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJqaGS9SX01m",
        "outputId": "df23a8fa-be29-4096-8eeb-5ed2049156e9"
      },
      "source": [
        "model2 = Sequential()\r\n",
        "\r\n",
        "# input layer\r\n",
        "model2.add(Conv1D(filters=60, kernel_size=11,  input_shape=(71,1)))\r\n",
        "#model2.add(BatchNormalization(axis=1))\r\n",
        "model2.add(Activation('relu'))\r\n",
        "model2.add(Dropout(0.1))\r\n",
        "\r\n",
        "model2.add(Conv1D(filters=60, kernel_size=3))\r\n",
        "#model2.add(BatchNormalization(axis=1))\r\n",
        "model2.add(Activation('relu'))\r\n",
        "model2.add(Dropout(0.1))\r\n",
        "\r\n",
        "model2.add(Conv1D(filters=60, kernel_size=7))\r\n",
        "#model2.add(BatchNormalization(axis=1))\r\n",
        "model2.add(Activation('relu'))\r\n",
        "model2.add(Dropout(0.1))\r\n",
        "\r\n",
        "model2.add(Flatten())\r\n",
        "#model2.add(Dropout(0.1))\r\n",
        "model2.add(Dense(128, activation='relu'))\r\n",
        "model2.add(Dense(num_class))\r\n",
        "model2.add(Activation('softmax'))\r\n",
        "\r\n",
        "model2.summary()"
      ],
      "execution_count": 112,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_7\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_23 (Conv1D)           (None, 61, 60)            720       \n",
            "_________________________________________________________________\n",
            "activation_30 (Activation)   (None, 61, 60)            0         \n",
            "_________________________________________________________________\n",
            "dropout_22 (Dropout)         (None, 61, 60)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_24 (Conv1D)           (None, 59, 60)            10860     \n",
            "_________________________________________________________________\n",
            "activation_31 (Activation)   (None, 59, 60)            0         \n",
            "_________________________________________________________________\n",
            "dropout_23 (Dropout)         (None, 59, 60)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_25 (Conv1D)           (None, 53, 60)            25260     \n",
            "_________________________________________________________________\n",
            "activation_32 (Activation)   (None, 53, 60)            0         \n",
            "_________________________________________________________________\n",
            "dropout_24 (Dropout)         (None, 53, 60)            0         \n",
            "_________________________________________________________________\n",
            "flatten_7 (Flatten)          (None, 3180)              0         \n",
            "_________________________________________________________________\n",
            "dense_14 (Dense)             (None, 128)               407168    \n",
            "_________________________________________________________________\n",
            "dense_15 (Dense)             (None, 15)                1935      \n",
            "_________________________________________________________________\n",
            "activation_33 (Activation)   (None, 15)                0         \n",
            "=================================================================\n",
            "Total params: 445,943\n",
            "Trainable params: 445,943\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u3i2MkUYNo3"
      },
      "source": [
        "learning_rates = 1e-3\r\n",
        "optim = tf.keras.optimizers.Adam(lr=learning_rates, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\r\n",
        "model2.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy']) "
      ],
      "execution_count": 115,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NGP29aXYPyw",
        "outputId": "81ab0cd4-3c6c-4a25-8bdf-169b69995591"
      },
      "source": [
        "model2.fit(X_train_r, y_train_v, epochs=200, batch_size=batch_size, validation_data=(X_val_r, y_val_v), verbose=1)"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "235/235 [==============================] - 2s 6ms/step - loss: 0.2397 - accuracy: 0.8896 - val_loss: 0.4177 - val_accuracy: 0.8757\n",
            "Epoch 2/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2110 - accuracy: 0.8952 - val_loss: 0.4158 - val_accuracy: 0.8587\n",
            "Epoch 3/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2021 - accuracy: 0.8996 - val_loss: 0.4424 - val_accuracy: 0.8696\n",
            "Epoch 4/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1980 - accuracy: 0.8993 - val_loss: 0.3969 - val_accuracy: 0.8655\n",
            "Epoch 5/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1939 - accuracy: 0.9044 - val_loss: 0.3859 - val_accuracy: 0.8715\n",
            "Epoch 6/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1909 - accuracy: 0.9031 - val_loss: 0.3927 - val_accuracy: 0.8788\n",
            "Epoch 7/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1912 - accuracy: 0.9037 - val_loss: 0.4066 - val_accuracy: 0.8647\n",
            "Epoch 8/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1860 - accuracy: 0.9058 - val_loss: 0.4184 - val_accuracy: 0.8780\n",
            "Epoch 9/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1814 - accuracy: 0.9075 - val_loss: 0.3958 - val_accuracy: 0.8877\n",
            "Epoch 10/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1850 - accuracy: 0.9067 - val_loss: 0.3725 - val_accuracy: 0.8745\n",
            "Epoch 11/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1855 - accuracy: 0.9060 - val_loss: 0.4302 - val_accuracy: 0.8809\n",
            "Epoch 12/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1830 - accuracy: 0.9100 - val_loss: 0.4546 - val_accuracy: 0.8836\n",
            "Epoch 13/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1811 - accuracy: 0.9067 - val_loss: 0.4186 - val_accuracy: 0.8824\n",
            "Epoch 14/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1801 - accuracy: 0.9111 - val_loss: 0.4323 - val_accuracy: 0.8705\n",
            "Epoch 15/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1806 - accuracy: 0.9118 - val_loss: 0.4331 - val_accuracy: 0.8895\n",
            "Epoch 16/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1772 - accuracy: 0.9137 - val_loss: 0.4162 - val_accuracy: 0.8884\n",
            "Epoch 17/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1749 - accuracy: 0.9140 - val_loss: 0.4329 - val_accuracy: 0.8808\n",
            "Epoch 18/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1758 - accuracy: 0.9163 - val_loss: 0.4613 - val_accuracy: 0.8868\n",
            "Epoch 19/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1710 - accuracy: 0.9152 - val_loss: 0.4022 - val_accuracy: 0.8833\n",
            "Epoch 20/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1740 - accuracy: 0.9158 - val_loss: 0.3821 - val_accuracy: 0.8699\n",
            "Epoch 21/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1713 - accuracy: 0.9169 - val_loss: 0.4457 - val_accuracy: 0.8849\n",
            "Epoch 22/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1716 - accuracy: 0.9196 - val_loss: 0.4911 - val_accuracy: 0.8725\n",
            "Epoch 23/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1704 - accuracy: 0.9179 - val_loss: 0.4765 - val_accuracy: 0.8759\n",
            "Epoch 24/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1676 - accuracy: 0.9206 - val_loss: 0.4420 - val_accuracy: 0.8757\n",
            "Epoch 25/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1658 - accuracy: 0.9202 - val_loss: 0.4724 - val_accuracy: 0.8896\n",
            "Epoch 26/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1629 - accuracy: 0.9201 - val_loss: 0.4584 - val_accuracy: 0.8713\n",
            "Epoch 27/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1666 - accuracy: 0.9183 - val_loss: 0.4676 - val_accuracy: 0.8757\n",
            "Epoch 28/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1663 - accuracy: 0.9212 - val_loss: 0.4754 - val_accuracy: 0.8733\n",
            "Epoch 29/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1645 - accuracy: 0.9206 - val_loss: 0.5053 - val_accuracy: 0.8769\n",
            "Epoch 30/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1586 - accuracy: 0.9233 - val_loss: 0.5005 - val_accuracy: 0.8912\n",
            "Epoch 31/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1600 - accuracy: 0.9222 - val_loss: 0.5074 - val_accuracy: 0.8735\n",
            "Epoch 32/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1590 - accuracy: 0.9250 - val_loss: 0.5279 - val_accuracy: 0.8901\n",
            "Epoch 33/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1600 - accuracy: 0.9230 - val_loss: 0.4527 - val_accuracy: 0.8724\n",
            "Epoch 34/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1583 - accuracy: 0.9235 - val_loss: 0.5043 - val_accuracy: 0.8779\n",
            "Epoch 35/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1565 - accuracy: 0.9271 - val_loss: 0.5235 - val_accuracy: 0.8819\n",
            "Epoch 36/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1578 - accuracy: 0.9262 - val_loss: 0.3802 - val_accuracy: 0.8943\n",
            "Epoch 37/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1548 - accuracy: 0.9243 - val_loss: 0.5938 - val_accuracy: 0.8729\n",
            "Epoch 38/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1601 - accuracy: 0.9238 - val_loss: 0.5596 - val_accuracy: 0.8875\n",
            "Epoch 39/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1561 - accuracy: 0.9246 - val_loss: 0.6064 - val_accuracy: 0.8887\n",
            "Epoch 40/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1562 - accuracy: 0.9252 - val_loss: 0.5751 - val_accuracy: 0.8857\n",
            "Epoch 41/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1494 - accuracy: 0.9290 - val_loss: 0.6033 - val_accuracy: 0.8847\n",
            "Epoch 42/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1570 - accuracy: 0.9270 - val_loss: 0.5937 - val_accuracy: 0.8757\n",
            "Epoch 43/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1517 - accuracy: 0.9289 - val_loss: 0.6176 - val_accuracy: 0.8756\n",
            "Epoch 44/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1515 - accuracy: 0.9289 - val_loss: 0.6070 - val_accuracy: 0.8817\n",
            "Epoch 45/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1524 - accuracy: 0.9284 - val_loss: 0.6365 - val_accuracy: 0.8895\n",
            "Epoch 46/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1502 - accuracy: 0.9294 - val_loss: 0.5947 - val_accuracy: 0.8839\n",
            "Epoch 47/200\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.1513 - accuracy: 0.9291 - val_loss: 0.6166 - val_accuracy: 0.8823\n",
            "Epoch 48/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1495 - accuracy: 0.9303 - val_loss: 0.6032 - val_accuracy: 0.8761\n",
            "Epoch 49/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1510 - accuracy: 0.9291 - val_loss: 0.5813 - val_accuracy: 0.8891\n",
            "Epoch 50/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1546 - accuracy: 0.9258 - val_loss: 0.5773 - val_accuracy: 0.8756\n",
            "Epoch 51/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1515 - accuracy: 0.9259 - val_loss: 0.5405 - val_accuracy: 0.8765\n",
            "Epoch 52/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1487 - accuracy: 0.9299 - val_loss: 0.6302 - val_accuracy: 0.8908\n",
            "Epoch 53/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1482 - accuracy: 0.9296 - val_loss: 0.4892 - val_accuracy: 0.8761\n",
            "Epoch 54/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1477 - accuracy: 0.9285 - val_loss: 0.5183 - val_accuracy: 0.8857\n",
            "Epoch 55/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1469 - accuracy: 0.9307 - val_loss: 0.5087 - val_accuracy: 0.8825\n",
            "Epoch 56/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1454 - accuracy: 0.9314 - val_loss: 0.5128 - val_accuracy: 0.8791\n",
            "Epoch 57/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1506 - accuracy: 0.9267 - val_loss: 0.5072 - val_accuracy: 0.8833\n",
            "Epoch 58/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1510 - accuracy: 0.9266 - val_loss: 0.5567 - val_accuracy: 0.8856\n",
            "Epoch 59/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1473 - accuracy: 0.9307 - val_loss: 0.5611 - val_accuracy: 0.8884\n",
            "Epoch 60/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1458 - accuracy: 0.9297 - val_loss: 0.5354 - val_accuracy: 0.8860\n",
            "Epoch 61/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1461 - accuracy: 0.9308 - val_loss: 0.5182 - val_accuracy: 0.8829\n",
            "Epoch 62/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1405 - accuracy: 0.9339 - val_loss: 0.5098 - val_accuracy: 0.8785\n",
            "Epoch 63/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1432 - accuracy: 0.9318 - val_loss: 0.5341 - val_accuracy: 0.8853\n",
            "Epoch 64/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1396 - accuracy: 0.9345 - val_loss: 0.5188 - val_accuracy: 0.8792\n",
            "Epoch 65/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1420 - accuracy: 0.9321 - val_loss: 0.4600 - val_accuracy: 0.8832\n",
            "Epoch 66/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1402 - accuracy: 0.9322 - val_loss: 0.4528 - val_accuracy: 0.8763\n",
            "Epoch 67/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1383 - accuracy: 0.9336 - val_loss: 0.4117 - val_accuracy: 0.8765\n",
            "Epoch 68/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1357 - accuracy: 0.9347 - val_loss: 0.5281 - val_accuracy: 0.8939\n",
            "Epoch 69/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1443 - accuracy: 0.9312 - val_loss: 0.5902 - val_accuracy: 0.8900\n",
            "Epoch 70/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1384 - accuracy: 0.9346 - val_loss: 0.5492 - val_accuracy: 0.8888\n",
            "Epoch 71/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1366 - accuracy: 0.9359 - val_loss: 0.5935 - val_accuracy: 0.8917\n",
            "Epoch 72/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1377 - accuracy: 0.9345 - val_loss: 0.6274 - val_accuracy: 0.8872\n",
            "Epoch 73/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1407 - accuracy: 0.9317 - val_loss: 0.6270 - val_accuracy: 0.8859\n",
            "Epoch 74/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1321 - accuracy: 0.9361 - val_loss: 0.5220 - val_accuracy: 0.8909\n",
            "Epoch 75/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1363 - accuracy: 0.9328 - val_loss: 0.4782 - val_accuracy: 0.8893\n",
            "Epoch 76/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1390 - accuracy: 0.9334 - val_loss: 0.6064 - val_accuracy: 0.8952\n",
            "Epoch 77/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1328 - accuracy: 0.9371 - val_loss: 0.5439 - val_accuracy: 0.8865\n",
            "Epoch 78/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1345 - accuracy: 0.9351 - val_loss: 0.5448 - val_accuracy: 0.8865\n",
            "Epoch 79/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1337 - accuracy: 0.9366 - val_loss: 0.5384 - val_accuracy: 0.8920\n",
            "Epoch 80/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1330 - accuracy: 0.9367 - val_loss: 0.5619 - val_accuracy: 0.8899\n",
            "Epoch 81/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1293 - accuracy: 0.9378 - val_loss: 0.5417 - val_accuracy: 0.8865\n",
            "Epoch 82/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1311 - accuracy: 0.9368 - val_loss: 0.5596 - val_accuracy: 0.8821\n",
            "Epoch 83/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1327 - accuracy: 0.9357 - val_loss: 0.5662 - val_accuracy: 0.8876\n",
            "Epoch 84/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1309 - accuracy: 0.9383 - val_loss: 0.6163 - val_accuracy: 0.8897\n",
            "Epoch 85/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1333 - accuracy: 0.9362 - val_loss: 0.6306 - val_accuracy: 0.8885\n",
            "Epoch 86/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1273 - accuracy: 0.9392 - val_loss: 0.5605 - val_accuracy: 0.8956\n",
            "Epoch 87/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1269 - accuracy: 0.9381 - val_loss: 0.6244 - val_accuracy: 0.9007\n",
            "Epoch 88/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1263 - accuracy: 0.9391 - val_loss: 0.6187 - val_accuracy: 0.8964\n",
            "Epoch 89/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1254 - accuracy: 0.9388 - val_loss: 0.5889 - val_accuracy: 0.8984\n",
            "Epoch 90/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1263 - accuracy: 0.9380 - val_loss: 0.5290 - val_accuracy: 0.8887\n",
            "Epoch 91/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1293 - accuracy: 0.9372 - val_loss: 0.5988 - val_accuracy: 0.8873\n",
            "Epoch 92/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1272 - accuracy: 0.9381 - val_loss: 0.5954 - val_accuracy: 0.9012\n",
            "Epoch 93/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1247 - accuracy: 0.9402 - val_loss: 0.5987 - val_accuracy: 0.8907\n",
            "Epoch 94/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1267 - accuracy: 0.9367 - val_loss: 0.6029 - val_accuracy: 0.8967\n",
            "Epoch 95/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1271 - accuracy: 0.9374 - val_loss: 0.6267 - val_accuracy: 0.8931\n",
            "Epoch 96/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1254 - accuracy: 0.9377 - val_loss: 0.5602 - val_accuracy: 0.8876\n",
            "Epoch 97/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1266 - accuracy: 0.9368 - val_loss: 0.6081 - val_accuracy: 0.8972\n",
            "Epoch 98/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1192 - accuracy: 0.9407 - val_loss: 0.4998 - val_accuracy: 0.8969\n",
            "Epoch 99/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1259 - accuracy: 0.9383 - val_loss: 0.5701 - val_accuracy: 0.8959\n",
            "Epoch 100/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1166 - accuracy: 0.9414 - val_loss: 0.5820 - val_accuracy: 0.8949\n",
            "Epoch 101/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1166 - accuracy: 0.9429 - val_loss: 0.5859 - val_accuracy: 0.8924\n",
            "Epoch 102/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1280 - accuracy: 0.9383 - val_loss: 0.5894 - val_accuracy: 0.8913\n",
            "Epoch 103/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1224 - accuracy: 0.9414 - val_loss: 0.5777 - val_accuracy: 0.8953\n",
            "Epoch 104/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1278 - accuracy: 0.9386 - val_loss: 0.5810 - val_accuracy: 0.8997\n",
            "Epoch 105/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1206 - accuracy: 0.9412 - val_loss: 0.6747 - val_accuracy: 0.8909\n",
            "Epoch 106/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1196 - accuracy: 0.9409 - val_loss: 0.6204 - val_accuracy: 0.8885\n",
            "Epoch 107/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1233 - accuracy: 0.9389 - val_loss: 0.6359 - val_accuracy: 0.9003\n",
            "Epoch 108/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1171 - accuracy: 0.9422 - val_loss: 0.6454 - val_accuracy: 0.8833\n",
            "Epoch 109/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1194 - accuracy: 0.9413 - val_loss: 0.6703 - val_accuracy: 0.8988\n",
            "Epoch 110/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1203 - accuracy: 0.9390 - val_loss: 0.6212 - val_accuracy: 0.9028\n",
            "Epoch 111/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1255 - accuracy: 0.9376 - val_loss: 0.6714 - val_accuracy: 0.9021\n",
            "Epoch 112/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1122 - accuracy: 0.9452 - val_loss: 0.5966 - val_accuracy: 0.8953\n",
            "Epoch 113/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1153 - accuracy: 0.9430 - val_loss: 0.6010 - val_accuracy: 0.8969\n",
            "Epoch 114/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1111 - accuracy: 0.9453 - val_loss: 0.6222 - val_accuracy: 0.9009\n",
            "Epoch 115/200\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.1191 - accuracy: 0.9412 - val_loss: 0.5409 - val_accuracy: 0.8985\n",
            "Epoch 116/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1219 - accuracy: 0.9408 - val_loss: 0.6340 - val_accuracy: 0.8972\n",
            "Epoch 117/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1222 - accuracy: 0.9380 - val_loss: 0.6278 - val_accuracy: 0.9017\n",
            "Epoch 118/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1175 - accuracy: 0.9420 - val_loss: 0.6310 - val_accuracy: 0.8979\n",
            "Epoch 119/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1161 - accuracy: 0.9426 - val_loss: 0.6603 - val_accuracy: 0.8995\n",
            "Epoch 120/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1176 - accuracy: 0.9436 - val_loss: 0.6522 - val_accuracy: 0.9011\n",
            "Epoch 121/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1160 - accuracy: 0.9422 - val_loss: 0.6058 - val_accuracy: 0.8983\n",
            "Epoch 122/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1180 - accuracy: 0.9424 - val_loss: 0.6631 - val_accuracy: 0.8980\n",
            "Epoch 123/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1157 - accuracy: 0.9439 - val_loss: 0.6060 - val_accuracy: 0.9013\n",
            "Epoch 124/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1132 - accuracy: 0.9451 - val_loss: 0.5124 - val_accuracy: 0.8904\n",
            "Epoch 125/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1130 - accuracy: 0.9442 - val_loss: 0.5425 - val_accuracy: 0.9027\n",
            "Epoch 126/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1147 - accuracy: 0.9431 - val_loss: 0.6459 - val_accuracy: 0.8931\n",
            "Epoch 127/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1166 - accuracy: 0.9419 - val_loss: 0.6500 - val_accuracy: 0.8995\n",
            "Epoch 128/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1157 - accuracy: 0.9431 - val_loss: 0.6655 - val_accuracy: 0.8927\n",
            "Epoch 129/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1196 - accuracy: 0.9423 - val_loss: 0.6778 - val_accuracy: 0.8935\n",
            "Epoch 130/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1223 - accuracy: 0.9398 - val_loss: 0.7090 - val_accuracy: 0.8952\n",
            "Epoch 131/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1205 - accuracy: 0.9411 - val_loss: 0.6024 - val_accuracy: 0.9015\n",
            "Epoch 132/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1117 - accuracy: 0.9459 - val_loss: 0.6565 - val_accuracy: 0.8989\n",
            "Epoch 133/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1164 - accuracy: 0.9412 - val_loss: 0.6499 - val_accuracy: 0.9000\n",
            "Epoch 134/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1122 - accuracy: 0.9447 - val_loss: 0.6352 - val_accuracy: 0.8997\n",
            "Epoch 135/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1166 - accuracy: 0.9421 - val_loss: 0.6623 - val_accuracy: 0.9007\n",
            "Epoch 136/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1152 - accuracy: 0.9422 - val_loss: 0.6192 - val_accuracy: 0.9045\n",
            "Epoch 137/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1110 - accuracy: 0.9442 - val_loss: 0.6048 - val_accuracy: 0.8993\n",
            "Epoch 138/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1084 - accuracy: 0.9458 - val_loss: 0.6325 - val_accuracy: 0.8965\n",
            "Epoch 139/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1163 - accuracy: 0.9425 - val_loss: 0.6468 - val_accuracy: 0.9008\n",
            "Epoch 140/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1117 - accuracy: 0.9441 - val_loss: 0.6292 - val_accuracy: 0.9024\n",
            "Epoch 141/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1105 - accuracy: 0.9449 - val_loss: 0.6296 - val_accuracy: 0.9017\n",
            "Epoch 142/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1114 - accuracy: 0.9446 - val_loss: 0.5806 - val_accuracy: 0.9019\n",
            "Epoch 143/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1069 - accuracy: 0.9478 - val_loss: 0.5938 - val_accuracy: 0.9025\n",
            "Epoch 144/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1100 - accuracy: 0.9452 - val_loss: 0.5859 - val_accuracy: 0.9045\n",
            "Epoch 145/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1112 - accuracy: 0.9444 - val_loss: 0.5958 - val_accuracy: 0.8943\n",
            "Epoch 146/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1143 - accuracy: 0.9439 - val_loss: 0.5846 - val_accuracy: 0.9036\n",
            "Epoch 147/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1121 - accuracy: 0.9435 - val_loss: 0.5388 - val_accuracy: 0.9040\n",
            "Epoch 148/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1079 - accuracy: 0.9470 - val_loss: 0.5309 - val_accuracy: 0.9012\n",
            "Epoch 149/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1111 - accuracy: 0.9437 - val_loss: 0.6065 - val_accuracy: 0.8999\n",
            "Epoch 150/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1234 - accuracy: 0.9385 - val_loss: 0.5697 - val_accuracy: 0.8931\n",
            "Epoch 151/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1148 - accuracy: 0.9429 - val_loss: 0.6078 - val_accuracy: 0.9001\n",
            "Epoch 152/200\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.1150 - accuracy: 0.9429 - val_loss: 0.5265 - val_accuracy: 0.8993\n",
            "Epoch 153/200\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.1111 - accuracy: 0.9446 - val_loss: 0.6943 - val_accuracy: 0.9031\n",
            "Epoch 154/200\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.1113 - accuracy: 0.9463 - val_loss: 0.6708 - val_accuracy: 0.9015\n",
            "Epoch 155/200\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.1083 - accuracy: 0.9477 - val_loss: 0.5515 - val_accuracy: 0.8999\n",
            "Epoch 156/200\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.1117 - accuracy: 0.9436 - val_loss: 0.6629 - val_accuracy: 0.9001\n",
            "Epoch 157/200\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.1061 - accuracy: 0.9493 - val_loss: 0.6328 - val_accuracy: 0.8980\n",
            "Epoch 158/200\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.1046 - accuracy: 0.9487 - val_loss: 0.4745 - val_accuracy: 0.9019\n",
            "Epoch 159/200\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.1102 - accuracy: 0.9458 - val_loss: 0.6610 - val_accuracy: 0.9013\n",
            "Epoch 160/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1122 - accuracy: 0.9458 - val_loss: 0.6692 - val_accuracy: 0.8996\n",
            "Epoch 161/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1094 - accuracy: 0.9473 - val_loss: 0.6755 - val_accuracy: 0.8993\n",
            "Epoch 162/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1090 - accuracy: 0.9465 - val_loss: 0.5952 - val_accuracy: 0.8905\n",
            "Epoch 163/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1131 - accuracy: 0.9451 - val_loss: 0.6889 - val_accuracy: 0.8971\n",
            "Epoch 164/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1064 - accuracy: 0.9483 - val_loss: 0.6670 - val_accuracy: 0.9017\n",
            "Epoch 165/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1119 - accuracy: 0.9456 - val_loss: 0.5862 - val_accuracy: 0.9125\n",
            "Epoch 166/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1088 - accuracy: 0.9469 - val_loss: 0.6376 - val_accuracy: 0.9023\n",
            "Epoch 167/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1055 - accuracy: 0.9487 - val_loss: 0.6677 - val_accuracy: 0.8987\n",
            "Epoch 168/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1032 - accuracy: 0.9489 - val_loss: 0.6407 - val_accuracy: 0.9012\n",
            "Epoch 169/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1039 - accuracy: 0.9481 - val_loss: 0.5766 - val_accuracy: 0.8971\n",
            "Epoch 170/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1186 - accuracy: 0.9442 - val_loss: 0.6852 - val_accuracy: 0.8951\n",
            "Epoch 171/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1165 - accuracy: 0.9437 - val_loss: 0.6348 - val_accuracy: 0.9021\n",
            "Epoch 172/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1100 - accuracy: 0.9458 - val_loss: 0.7634 - val_accuracy: 0.9008\n",
            "Epoch 173/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1067 - accuracy: 0.9473 - val_loss: 0.6874 - val_accuracy: 0.8939\n",
            "Epoch 174/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1099 - accuracy: 0.9472 - val_loss: 0.7337 - val_accuracy: 0.9012\n",
            "Epoch 175/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1092 - accuracy: 0.9463 - val_loss: 0.6752 - val_accuracy: 0.9044\n",
            "Epoch 176/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1151 - accuracy: 0.9439 - val_loss: 0.5855 - val_accuracy: 0.8901\n",
            "Epoch 177/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1162 - accuracy: 0.9414 - val_loss: 0.7330 - val_accuracy: 0.8995\n",
            "Epoch 178/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1058 - accuracy: 0.9488 - val_loss: 0.6860 - val_accuracy: 0.9049\n",
            "Epoch 179/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1059 - accuracy: 0.9471 - val_loss: 0.7544 - val_accuracy: 0.8999\n",
            "Epoch 180/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1069 - accuracy: 0.9480 - val_loss: 0.6707 - val_accuracy: 0.8853\n",
            "Epoch 181/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1380 - accuracy: 0.9354 - val_loss: 0.6298 - val_accuracy: 0.8895\n",
            "Epoch 182/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1218 - accuracy: 0.9404 - val_loss: 0.6201 - val_accuracy: 0.8996\n",
            "Epoch 183/200\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.1071 - accuracy: 0.9485 - val_loss: 0.6385 - val_accuracy: 0.9003\n",
            "Epoch 184/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1053 - accuracy: 0.9480 - val_loss: 0.6560 - val_accuracy: 0.9019\n",
            "Epoch 185/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1069 - accuracy: 0.9452 - val_loss: 0.6894 - val_accuracy: 0.8909\n",
            "Epoch 186/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1120 - accuracy: 0.9440 - val_loss: 0.7669 - val_accuracy: 0.8995\n",
            "Epoch 187/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1035 - accuracy: 0.9484 - val_loss: 0.6241 - val_accuracy: 0.9033\n",
            "Epoch 188/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1019 - accuracy: 0.9489 - val_loss: 0.7547 - val_accuracy: 0.8840\n",
            "Epoch 189/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1364 - accuracy: 0.9358 - val_loss: 0.8135 - val_accuracy: 0.9025\n",
            "Epoch 190/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1126 - accuracy: 0.9453 - val_loss: 0.8095 - val_accuracy: 0.8909\n",
            "Epoch 191/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1377 - accuracy: 0.9347 - val_loss: 0.7809 - val_accuracy: 0.8980\n",
            "Epoch 192/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1328 - accuracy: 0.9367 - val_loss: 0.8039 - val_accuracy: 0.8911\n",
            "Epoch 193/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1347 - accuracy: 0.9366 - val_loss: 0.8092 - val_accuracy: 0.8887\n",
            "Epoch 194/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1307 - accuracy: 0.9382 - val_loss: 0.7718 - val_accuracy: 0.8873\n",
            "Epoch 195/200\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.1270 - accuracy: 0.9375 - val_loss: 0.7057 - val_accuracy: 0.8897\n",
            "Epoch 196/200\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.1314 - accuracy: 0.9380 - val_loss: 0.6029 - val_accuracy: 0.8949\n",
            "Epoch 197/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1269 - accuracy: 0.9408 - val_loss: 0.7041 - val_accuracy: 0.8941\n",
            "Epoch 198/200\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.1306 - accuracy: 0.9382 - val_loss: 0.7947 - val_accuracy: 0.8961\n",
            "Epoch 199/200\n",
            "235/235 [==============================] - 1s 6ms/step - loss: 0.1324 - accuracy: 0.9363 - val_loss: 0.7368 - val_accuracy: 0.8997\n",
            "Epoch 200/200\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.1215 - accuracy: 0.9401 - val_loss: 0.8005 - val_accuracy: 0.8993\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f62c845c550>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQg2TKLVXsmo"
      },
      "source": [
        "**Model with 5 Con1D layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhvdIfDwymA9",
        "outputId": "1fee46d1-9b60-4e35-a9f9-42b44564aa6d"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# input layer\n",
        "model.add(Conv1D(filters=64, kernel_size=23, padding='same', input_shape=(71,1)))\n",
        "model.add(BatchNormalization(axis=1))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "model.add(Conv1D(filters=128, kernel_size=13))\n",
        "model.add(BatchNormalization(axis=1))\n",
        "model.add(Activation('relu'))\n",
        "model.add(MaxPooling1D(pool_size=2))\n",
        "\n",
        "model.add(Conv1D(filters=256, kernel_size=7))\n",
        "model.add(BatchNormalization(axis=1))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv1D(filters=512, kernel_size=3))\n",
        "model.add(BatchNormalization(axis=1))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Conv1D(filters=256, kernel_size=3))\n",
        "model.add(BatchNormalization(axis=1))\n",
        "model.add(Activation('relu'))\n",
        "\n",
        "model.add(Flatten())\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(100, activation='relu'))\n",
        "model.add(Dense(num_class))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_3 (Conv1D)            (None, 71, 64)            1536      \n",
            "_________________________________________________________________\n",
            "batch_normalization (BatchNo (None, 71, 64)            284       \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 71, 64)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d (MaxPooling1D) (None, 35, 64)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_4 (Conv1D)            (None, 23, 128)           106624    \n",
            "_________________________________________________________________\n",
            "batch_normalization_1 (Batch (None, 23, 128)           92        \n",
            "_________________________________________________________________\n",
            "activation_5 (Activation)    (None, 23, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_1 (MaxPooling1 (None, 11, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_5 (Conv1D)            (None, 5, 256)            229632    \n",
            "_________________________________________________________________\n",
            "batch_normalization_2 (Batch (None, 5, 256)            20        \n",
            "_________________________________________________________________\n",
            "activation_6 (Activation)    (None, 5, 256)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_6 (Conv1D)            (None, 3, 512)            393728    \n",
            "_________________________________________________________________\n",
            "batch_normalization_3 (Batch (None, 3, 512)            12        \n",
            "_________________________________________________________________\n",
            "activation_7 (Activation)    (None, 3, 512)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_7 (Conv1D)            (None, 1, 256)            393472    \n",
            "_________________________________________________________________\n",
            "batch_normalization_4 (Batch (None, 1, 256)            4         \n",
            "_________________________________________________________________\n",
            "activation_8 (Activation)    (None, 1, 256)            0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 100)               25700     \n",
            "_________________________________________________________________\n",
            "dense_3 (Dense)              (None, 15)                1515      \n",
            "_________________________________________________________________\n",
            "activation_9 (Activation)    (None, 15)                0         \n",
            "=================================================================\n",
            "Total params: 1,152,619\n",
            "Trainable params: 1,152,413\n",
            "Non-trainable params: 206\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHcoSDdxymA-"
      },
      "source": [
        "learning_rates = 0.0001\r\n",
        "optim = tf.keras.optimizers.Adam(lr=learning_rates, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy']) "
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_dq7oCbymA_"
      },
      "source": [
        "## Step 4. Training the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ZuWGG2dNymBA",
        "outputId": "f25dfe99-f03d-4f46-9e8a-ecea826ec0aa"
      },
      "source": [
        "# fit network\n",
        "model.fit(X_train_r, y_train_v, epochs=200, batch_size=batch_size, validation_data=(X_val_r, y_val_v), verbose=1)"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1748 - accuracy: 0.9119 - val_loss: 0.8026 - val_accuracy: 0.8669\n",
            "Epoch 2/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1747 - accuracy: 0.9171 - val_loss: 0.7817 - val_accuracy: 0.8779\n",
            "Epoch 3/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1727 - accuracy: 0.9166 - val_loss: 0.8243 - val_accuracy: 0.8753\n",
            "Epoch 4/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1719 - accuracy: 0.9190 - val_loss: 0.7202 - val_accuracy: 0.8719\n",
            "Epoch 5/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1743 - accuracy: 0.9168 - val_loss: 0.7407 - val_accuracy: 0.8827\n",
            "Epoch 6/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1725 - accuracy: 0.9168 - val_loss: 0.6609 - val_accuracy: 0.8793\n",
            "Epoch 7/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1719 - accuracy: 0.9166 - val_loss: 0.6401 - val_accuracy: 0.8824\n",
            "Epoch 8/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1707 - accuracy: 0.9203 - val_loss: 0.7595 - val_accuracy: 0.8772\n",
            "Epoch 9/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1685 - accuracy: 0.9209 - val_loss: 0.8277 - val_accuracy: 0.8683\n",
            "Epoch 10/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1704 - accuracy: 0.9198 - val_loss: 0.8966 - val_accuracy: 0.8664\n",
            "Epoch 11/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1701 - accuracy: 0.9194 - val_loss: 0.8496 - val_accuracy: 0.8755\n",
            "Epoch 12/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1679 - accuracy: 0.9206 - val_loss: 0.8933 - val_accuracy: 0.8753\n",
            "Epoch 13/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1674 - accuracy: 0.9223 - val_loss: 0.9648 - val_accuracy: 0.8669\n",
            "Epoch 14/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1686 - accuracy: 0.9200 - val_loss: 0.7587 - val_accuracy: 0.8703\n",
            "Epoch 15/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1676 - accuracy: 0.9193 - val_loss: 0.9156 - val_accuracy: 0.8668\n",
            "Epoch 16/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1750 - accuracy: 0.9181 - val_loss: 0.8312 - val_accuracy: 0.8575\n",
            "Epoch 17/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1678 - accuracy: 0.9219 - val_loss: 0.7876 - val_accuracy: 0.8748\n",
            "Epoch 18/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1668 - accuracy: 0.9236 - val_loss: 0.8336 - val_accuracy: 0.8839\n",
            "Epoch 19/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1653 - accuracy: 0.9223 - val_loss: 0.8148 - val_accuracy: 0.8763\n",
            "Epoch 20/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1655 - accuracy: 0.9228 - val_loss: 0.9666 - val_accuracy: 0.8691\n",
            "Epoch 21/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1638 - accuracy: 0.9246 - val_loss: 0.8026 - val_accuracy: 0.8820\n",
            "Epoch 22/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1619 - accuracy: 0.9250 - val_loss: 0.7776 - val_accuracy: 0.8835\n",
            "Epoch 23/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1673 - accuracy: 0.9237 - val_loss: 0.7841 - val_accuracy: 0.8849\n",
            "Epoch 24/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1634 - accuracy: 0.9241 - val_loss: 0.7617 - val_accuracy: 0.8741\n",
            "Epoch 25/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1607 - accuracy: 0.9250 - val_loss: 0.8889 - val_accuracy: 0.8785\n",
            "Epoch 26/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1624 - accuracy: 0.9255 - val_loss: 0.8122 - val_accuracy: 0.8863\n",
            "Epoch 27/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1619 - accuracy: 0.9256 - val_loss: 0.8965 - val_accuracy: 0.8761\n",
            "Epoch 28/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1618 - accuracy: 0.9265 - val_loss: 0.7092 - val_accuracy: 0.8732\n",
            "Epoch 29/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1593 - accuracy: 0.9282 - val_loss: 0.5871 - val_accuracy: 0.8852\n",
            "Epoch 30/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1581 - accuracy: 0.9275 - val_loss: 0.7213 - val_accuracy: 0.8727\n",
            "Epoch 31/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1603 - accuracy: 0.9267 - val_loss: 0.7213 - val_accuracy: 0.8840\n",
            "Epoch 32/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1619 - accuracy: 0.9265 - val_loss: 0.7980 - val_accuracy: 0.8893\n",
            "Epoch 33/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1583 - accuracy: 0.9284 - val_loss: 0.8366 - val_accuracy: 0.8757\n",
            "Epoch 34/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1614 - accuracy: 0.9260 - val_loss: 0.7811 - val_accuracy: 0.8841\n",
            "Epoch 35/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1591 - accuracy: 0.9258 - val_loss: 0.8313 - val_accuracy: 0.8805\n",
            "Epoch 36/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1573 - accuracy: 0.9261 - val_loss: 0.8396 - val_accuracy: 0.8820\n",
            "Epoch 37/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1603 - accuracy: 0.9276 - val_loss: 0.7733 - val_accuracy: 0.8835\n",
            "Epoch 38/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1623 - accuracy: 0.9265 - val_loss: 0.7562 - val_accuracy: 0.8845\n",
            "Epoch 39/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1568 - accuracy: 0.9300 - val_loss: 0.7469 - val_accuracy: 0.8877\n",
            "Epoch 40/200\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1588 - accuracy: 0.9272 - val_loss: 0.8096 - val_accuracy: 0.8764\n",
            "Epoch 41/200\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1570 - accuracy: 0.9291 - val_loss: 0.9060 - val_accuracy: 0.8885\n",
            "Epoch 42/200\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1564 - accuracy: 0.9284 - val_loss: 0.8719 - val_accuracy: 0.8879\n",
            "Epoch 43/200\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1542 - accuracy: 0.9309 - val_loss: 0.7515 - val_accuracy: 0.8855\n",
            "Epoch 44/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1570 - accuracy: 0.9296 - val_loss: 0.7637 - val_accuracy: 0.8749\n",
            "Epoch 45/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1533 - accuracy: 0.9293 - val_loss: 0.8459 - val_accuracy: 0.8671\n",
            "Epoch 46/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1547 - accuracy: 0.9299 - val_loss: 0.9326 - val_accuracy: 0.8681\n",
            "Epoch 47/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1565 - accuracy: 0.9281 - val_loss: 0.9510 - val_accuracy: 0.8831\n",
            "Epoch 48/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1575 - accuracy: 0.9277 - val_loss: 0.7726 - val_accuracy: 0.8863\n",
            "Epoch 49/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1570 - accuracy: 0.9299 - val_loss: 0.7938 - val_accuracy: 0.8851\n",
            "Epoch 50/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1547 - accuracy: 0.9286 - val_loss: 0.8117 - val_accuracy: 0.8833\n",
            "Epoch 51/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1554 - accuracy: 0.9300 - val_loss: 0.8385 - val_accuracy: 0.8772\n",
            "Epoch 52/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1536 - accuracy: 0.9310 - val_loss: 0.8878 - val_accuracy: 0.8819\n",
            "Epoch 53/200\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1533 - accuracy: 0.9308 - val_loss: 0.8353 - val_accuracy: 0.8757\n",
            "Epoch 54/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1536 - accuracy: 0.9301 - val_loss: 0.8678 - val_accuracy: 0.8857\n",
            "Epoch 55/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1586 - accuracy: 0.9247 - val_loss: 0.7474 - val_accuracy: 0.8845\n",
            "Epoch 56/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1530 - accuracy: 0.9311 - val_loss: 0.9304 - val_accuracy: 0.8797\n",
            "Epoch 57/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1528 - accuracy: 0.9296 - val_loss: 0.9404 - val_accuracy: 0.8820\n",
            "Epoch 58/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1524 - accuracy: 0.9305 - val_loss: 0.7930 - val_accuracy: 0.8879\n",
            "Epoch 59/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1587 - accuracy: 0.9288 - val_loss: 0.7552 - val_accuracy: 0.8847\n",
            "Epoch 60/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1515 - accuracy: 0.9330 - val_loss: 0.8968 - val_accuracy: 0.8879\n",
            "Epoch 61/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1545 - accuracy: 0.9274 - val_loss: 0.8302 - val_accuracy: 0.8760\n",
            "Epoch 62/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1519 - accuracy: 0.9317 - val_loss: 0.9052 - val_accuracy: 0.8897\n",
            "Epoch 63/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1558 - accuracy: 0.9294 - val_loss: 0.8730 - val_accuracy: 0.8821\n",
            "Epoch 64/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1505 - accuracy: 0.9321 - val_loss: 0.6931 - val_accuracy: 0.8847\n",
            "Epoch 65/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1513 - accuracy: 0.9321 - val_loss: 0.7938 - val_accuracy: 0.8860\n",
            "Epoch 66/200\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1493 - accuracy: 0.9320 - val_loss: 0.9313 - val_accuracy: 0.8835\n",
            "Epoch 67/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1484 - accuracy: 0.9340 - val_loss: 0.9661 - val_accuracy: 0.8807\n",
            "Epoch 68/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1536 - accuracy: 0.9301 - val_loss: 0.9399 - val_accuracy: 0.8748\n",
            "Epoch 69/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1529 - accuracy: 0.9302 - val_loss: 0.8273 - val_accuracy: 0.8800\n",
            "Epoch 70/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1531 - accuracy: 0.9315 - val_loss: 0.7738 - val_accuracy: 0.8825\n",
            "Epoch 71/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1509 - accuracy: 0.9315 - val_loss: 0.9773 - val_accuracy: 0.8813\n",
            "Epoch 72/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1528 - accuracy: 0.9313 - val_loss: 0.8979 - val_accuracy: 0.8720\n",
            "Epoch 73/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1524 - accuracy: 0.9315 - val_loss: 0.8890 - val_accuracy: 0.8737\n",
            "Epoch 74/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1518 - accuracy: 0.9320 - val_loss: 0.9278 - val_accuracy: 0.8724\n",
            "Epoch 75/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1515 - accuracy: 0.9324 - val_loss: 1.0012 - val_accuracy: 0.8735\n",
            "Epoch 76/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1502 - accuracy: 0.9320 - val_loss: 0.9955 - val_accuracy: 0.8713\n",
            "Epoch 77/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1485 - accuracy: 0.9330 - val_loss: 1.1751 - val_accuracy: 0.8705\n",
            "Epoch 78/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1512 - accuracy: 0.9295 - val_loss: 1.0672 - val_accuracy: 0.8845\n",
            "Epoch 79/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1519 - accuracy: 0.9324 - val_loss: 0.9844 - val_accuracy: 0.8700\n",
            "Epoch 80/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1502 - accuracy: 0.9335 - val_loss: 1.0277 - val_accuracy: 0.8831\n",
            "Epoch 81/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1522 - accuracy: 0.9311 - val_loss: 1.0058 - val_accuracy: 0.8845\n",
            "Epoch 82/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1486 - accuracy: 0.9334 - val_loss: 1.0871 - val_accuracy: 0.8813\n",
            "Epoch 83/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1505 - accuracy: 0.9326 - val_loss: 0.7589 - val_accuracy: 0.8819\n",
            "Epoch 84/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1478 - accuracy: 0.9327 - val_loss: 0.9433 - val_accuracy: 0.8699\n",
            "Epoch 85/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1513 - accuracy: 0.9313 - val_loss: 0.9412 - val_accuracy: 0.8748\n",
            "Epoch 86/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1496 - accuracy: 0.9322 - val_loss: 0.9418 - val_accuracy: 0.8748\n",
            "Epoch 87/200\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1492 - accuracy: 0.9334 - val_loss: 1.0132 - val_accuracy: 0.8884\n",
            "Epoch 88/200\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1470 - accuracy: 0.9338 - val_loss: 1.0755 - val_accuracy: 0.8783\n",
            "Epoch 89/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1486 - accuracy: 0.9334 - val_loss: 1.0612 - val_accuracy: 0.8821\n",
            "Epoch 90/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1528 - accuracy: 0.9309 - val_loss: 0.9588 - val_accuracy: 0.8797\n",
            "Epoch 91/200\n",
            "235/235 [==============================] - 2s 10ms/step - loss: 0.1492 - accuracy: 0.9319 - val_loss: 0.9715 - val_accuracy: 0.8820\n",
            "Epoch 92/200\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1478 - accuracy: 0.9334 - val_loss: 1.0944 - val_accuracy: 0.8847\n",
            "Epoch 93/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1485 - accuracy: 0.9338 - val_loss: 1.0990 - val_accuracy: 0.8832\n",
            "Epoch 94/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1464 - accuracy: 0.9337 - val_loss: 1.1492 - val_accuracy: 0.8819\n",
            "Epoch 95/200\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1458 - accuracy: 0.9341 - val_loss: 1.0972 - val_accuracy: 0.8772\n",
            "Epoch 96/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1508 - accuracy: 0.9323 - val_loss: 0.9810 - val_accuracy: 0.8820\n",
            "Epoch 97/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1519 - accuracy: 0.9317 - val_loss: 1.0017 - val_accuracy: 0.8795\n",
            "Epoch 98/200\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1486 - accuracy: 0.9336 - val_loss: 0.8186 - val_accuracy: 0.8891\n",
            "Epoch 99/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1492 - accuracy: 0.9338 - val_loss: 0.7719 - val_accuracy: 0.8832\n",
            "Epoch 100/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1458 - accuracy: 0.9351 - val_loss: 0.8449 - val_accuracy: 0.8892\n",
            "Epoch 101/200\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1498 - accuracy: 0.9276 - val_loss: 0.9426 - val_accuracy: 0.8848\n",
            "Epoch 102/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1491 - accuracy: 0.9338 - val_loss: 0.9049 - val_accuracy: 0.8840\n",
            "Epoch 103/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1478 - accuracy: 0.9334 - val_loss: 1.0088 - val_accuracy: 0.8791\n",
            "Epoch 104/200\n",
            "235/235 [==============================] - 3s 11ms/step - loss: 0.1491 - accuracy: 0.9317 - val_loss: 0.9889 - val_accuracy: 0.8877\n",
            "Epoch 105/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1469 - accuracy: 0.9345 - val_loss: 0.9651 - val_accuracy: 0.8867\n",
            "Epoch 106/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1454 - accuracy: 0.9346 - val_loss: 0.9565 - val_accuracy: 0.8869\n",
            "Epoch 107/200\n",
            "235/235 [==============================] - 2s 11ms/step - loss: 0.1506 - accuracy: 0.9333 - val_loss: 0.9314 - val_accuracy: 0.8892\n",
            "Epoch 108/200\n",
            "234/235 [============================>.] - ETA: 0s - loss: 0.1495 - accuracy: 0.9334"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-532e08850dd9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m200\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1139\u001b[0m               \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1140\u001b[0m               \u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1141\u001b[0;31m               return_dict=True)\n\u001b[0m\u001b[1;32m   1142\u001b[0m           \u001b[0mval_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'val_'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1143\u001b[0m           \u001b[0mepoch_logs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(self, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, return_dict)\u001b[0m\n\u001b[1;32m   1347\u001b[0m       \u001b[0;31m# Use cached evaluation data only when it's called in `Model.fit`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1348\u001b[0m       if (getattr(self, '_fit_frame', None) is not None\n\u001b[0;32m-> 1349\u001b[0;31m           \u001b[0;32mand\u001b[0m \u001b[0mtf_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcurrentframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fit_frame\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1350\u001b[0m           and getattr(self, '_eval_data_handler', None) is not None):\n\u001b[1;32m   1351\u001b[0m         \u001b[0mdata_handler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_eval_data_handler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/utils/tf_inspect.py\u001b[0m in \u001b[0;36mcurrentframe\u001b[0;34m()\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mcurrentframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m   \u001b[0;34m\"\"\"TFDecorator-aware replacement for inspect.currentframe.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 95\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_inspect\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mstack\u001b[0;34m(context)\u001b[0m\n\u001b[1;32m   1499\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mstack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;34m\"\"\"Return a list of records for the stack above the caller's frame.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1501\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgetouterframes\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getframe\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetouterframes\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1476\u001b[0m     \u001b[0mframelist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1477\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1478\u001b[0;31m         \u001b[0mframeinfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgetframeinfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1479\u001b[0m         \u001b[0mframelist\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mFrameInfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mframeinfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1480\u001b[0m         \u001b[0mframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf_back\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetframeinfo\u001b[0;34m(frame, context)\u001b[0m\n\u001b[1;32m   1450\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlineno\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m//\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1451\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1452\u001b[0;31m             \u001b[0mlines\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlnum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfindsource\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframe\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1453\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1454\u001b[0m             \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mfindsource\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    766\u001b[0m     is raised if the source code cannot be retrieved.\"\"\"\n\u001b[1;32m    767\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 768\u001b[0;31m     \u001b[0mfile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetsourcefile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    769\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    770\u001b[0m         \u001b[0;31m# Invalidate cache if needed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/inspect.py\u001b[0m in \u001b[0;36mgetsourcefile\u001b[0;34m(object)\u001b[0m\n\u001b[1;32m    691\u001b[0m                  importlib.machinery.EXTENSION_SUFFIXES):\n\u001b[1;32m    692\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 693\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    694\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    695\u001b[0m     \u001b[0;31m# only return a non-existent filename if the module has a PEP 302 loader\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/genericpath.py\u001b[0m in \u001b[0;36mexists\u001b[0;34m(path)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;34m\"\"\"Test whether a path exists.  Returns False for broken symbolic links\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mOSError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Y7P8jYVymBA",
        "outputId": "eeb9ac6d-ef33-434a-a0d3-a285c7009ca6"
      },
      "source": [
        "# evaluate model\n",
        "accuracy = model.evaluate(X_test_r, y_test_v, batch_size=batch_size, verbose=1)"
      ],
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "59/59 [==============================] - 0s 6ms/step - loss: 3.6070 - accuracy: 0.5965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YxM82iKRl3Ga"
      },
      "source": [
        "## Step 5. Reorder the features and put the most important feature first"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YZ2T7EtVymBB"
      },
      "source": [
        "column_names = [\r\n",
        " 'Dst Port',\r\n",
        " 'Protocol',\r\n",
        " 'Flow Duration',\r\n",
        " 'Tot Fwd Pkts',\r\n",
        " 'Tot Bwd Pkts',\r\n",
        " 'TotLen Fwd Pkts',\r\n",
        " 'TotLen Bwd Pkts',\r\n",
        " 'Fwd Pkt Len Max',\r\n",
        " 'Fwd Pkt Len Min',\r\n",
        " 'Fwd Pkt Len Mean',\r\n",
        " 'Fwd Pkt Len Std',\r\n",
        " 'Bwd Pkt Len Max',\r\n",
        " 'Bwd Pkt Len Min',\r\n",
        " 'Bwd Pkt Len Mean',\r\n",
        " 'Bwd Pkt Len Std',\r\n",
        " 'Flow Byts/s',\r\n",
        " 'Flow Pkts/s',\r\n",
        " 'Flow IAT Mean',\r\n",
        " 'Flow IAT Std',\r\n",
        " 'Flow IAT Max',\r\n",
        " 'Flow IAT Min',\r\n",
        " 'Fwd IAT Tot',\r\n",
        " 'Fwd IAT Mean',\r\n",
        " 'Fwd IAT Std',\r\n",
        " 'Fwd IAT Max',\r\n",
        " 'Fwd IAT Min',\r\n",
        " 'Bwd IAT Tot',\r\n",
        " 'Bwd IAT Mean',\r\n",
        " 'Bwd IAT Std',\r\n",
        " 'Bwd IAT Max',\r\n",
        " 'Bwd IAT Min',\r\n",
        " 'Fwd PSH Flags',\r\n",
        " 'Bwd PSH Flags',\r\n",
        " 'Fwd URG Flags',\r\n",
        " 'Bwd URG Flags',\r\n",
        " 'Fwd Header Len',\r\n",
        " 'Bwd Header Len',\r\n",
        " 'Fwd Pkts/s',\r\n",
        " 'Bwd Pkts/s',\r\n",
        " 'Pkt Len Min',\r\n",
        " 'Pkt Len Max',\r\n",
        " 'Pkt Len Mean',\r\n",
        " 'Pkt Len Std',\r\n",
        " 'Pkt Len Var',\r\n",
        " 'FIN Flag Cnt',\r\n",
        " 'SYN Flag Cnt',\r\n",
        " 'RST Flag Cnt',\r\n",
        " 'PSH Flag Cnt',\r\n",
        " 'ACK Flag Cnt',\r\n",
        " 'URG Flag Cnt',\r\n",
        " 'CWE Flag Count',\r\n",
        " 'ECE Flag Cnt',\r\n",
        " 'Down/Up Ratio',\r\n",
        " 'Pkt Size Avg',\r\n",
        " 'Fwd Seg Size Avg',\r\n",
        " 'Bwd Seg Size Avg',\r\n",
        " 'Fwd Byts/b Avg',\r\n",
        " 'Fwd Pkts/b Avg',\r\n",
        " 'Fwd Blk Rate Avg',\r\n",
        " 'Bwd Byts/b Avg',\r\n",
        " 'Bwd Pkts/b Avg',\r\n",
        " 'Bwd Blk Rate Avg',\r\n",
        " 'Subflow Fwd Pkts',\r\n",
        " 'Subflow Fwd Byts',\r\n",
        " 'Subflow Bwd Pkts',\r\n",
        " 'Subflow Bwd Byts',\r\n",
        " 'Init Fwd Win Byts',\r\n",
        " 'Init Bwd Win Byts',\r\n",
        " 'Fwd Act Data Pkts',\r\n",
        " 'Fwd Seg Size Min',\r\n",
        " 'Active Mean',\r\n",
        " 'Active Std',\r\n",
        " 'Active Max',\r\n",
        " 'Active Min',\r\n",
        " 'Idle Mean',\r\n",
        " 'Idle Std',\r\n",
        " 'Idle Max',\r\n",
        " 'Idle Min']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ygOJLKDEsynh",
        "outputId": "90c830a0-c240-4447-9827-186e716b14a7"
      },
      "source": [
        "len(column_names)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "78"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O4nkShrKmneA"
      },
      "source": [
        "According to \"**Selection and Performance Analysis of CICIDS2017 Features Importance**\", the important features are: *Destination Port, Fwd IAT Min, Init_Win_bytes_Forward, Init_Win_bytes_backward* and *FlowIATMin*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVW6qCF2mqnf"
      },
      "source": [
        "important_column_names = ['Dst Port',\r\n",
        " 'Fwd IAT Min',\r\n",
        " 'Init Fwd Win Byts',\r\n",
        " 'Init Bwd Win Byts',\r\n",
        " 'Flow IAT Min',\r\n",
        " 'Flow Duration',\r\n",
        " 'Tot Fwd Pkts',\r\n",
        " 'Tot Bwd Pkts',\r\n",
        " 'TotLen Fwd Pkts',\r\n",
        " 'TotLen Bwd Pkts',\r\n",
        " 'Fwd Pkt Len Max',\r\n",
        " 'Fwd Pkt Len Min',\r\n",
        " 'Fwd Pkt Len Mean',\r\n",
        " 'Fwd Pkt Len Std',\r\n",
        " 'Bwd Pkt Len Max',\r\n",
        " 'Bwd Pkt Len Min',\r\n",
        " 'Bwd Pkt Len Mean',\r\n",
        " 'Bwd Pkt Len Std',\r\n",
        " 'Protocol',\r\n",
        " 'Flow Byts/s',\r\n",
        " 'Flow Pkts/s',\r\n",
        " 'Flow IAT Mean',\r\n",
        " 'Flow IAT Std',\r\n",
        " 'Flow IAT Max',\r\n",
        " 'Fwd IAT Tot',\r\n",
        " 'Fwd IAT Mean',\r\n",
        " 'Fwd IAT Std',\r\n",
        " 'Fwd IAT Max',\r\n",
        " 'Bwd IAT Tot',\r\n",
        " 'Bwd IAT Mean',\r\n",
        " 'Bwd IAT Std',\r\n",
        " 'Bwd IAT Max',\r\n",
        " 'Bwd IAT Min',\r\n",
        " 'Fwd PSH Flags',\r\n",
        " 'Bwd PSH Flags',\r\n",
        " 'Fwd URG Flags',\r\n",
        " 'Bwd URG Flags',\r\n",
        " 'Fwd Header Len',\r\n",
        " 'Bwd Header Len',\r\n",
        " 'Fwd Pkts/s',\r\n",
        " 'Bwd Pkts/s',\r\n",
        " 'Pkt Len Min',\r\n",
        " 'Pkt Len Max',\r\n",
        " 'Pkt Len Mean',\r\n",
        " 'Pkt Len Std',\r\n",
        " 'Pkt Len Var',\r\n",
        " 'FIN Flag Cnt',\r\n",
        " 'SYN Flag Cnt',\r\n",
        " 'RST Flag Cnt',\r\n",
        " 'PSH Flag Cnt',\r\n",
        " 'ACK Flag Cnt',\r\n",
        " 'URG Flag Cnt',\r\n",
        " 'CWE Flag Count',\r\n",
        " 'ECE Flag Cnt',\r\n",
        " 'Down/Up Ratio',\r\n",
        " 'Pkt Size Avg',\r\n",
        " 'Fwd Seg Size Avg',\r\n",
        " 'Bwd Seg Size Avg',\r\n",
        " 'Fwd Byts/b Avg',\r\n",
        " 'Fwd Pkts/b Avg',\r\n",
        " 'Fwd Blk Rate Avg',\r\n",
        " 'Bwd Byts/b Avg',\r\n",
        " 'Bwd Pkts/b Avg',\r\n",
        " 'Bwd Blk Rate Avg',\r\n",
        " 'Subflow Fwd Pkts',\r\n",
        " 'Subflow Fwd Byts',\r\n",
        " 'Subflow Bwd Pkts',\r\n",
        " 'Subflow Bwd Byts',\r\n",
        " 'Fwd Act Data Pkts',\r\n",
        " 'Fwd Seg Size Min',\r\n",
        " 'Active Mean',\r\n",
        " 'Active Std',\r\n",
        " 'Active Max',\r\n",
        " 'Active Min',\r\n",
        " 'Idle Mean',\r\n",
        " 'Idle Std',\r\n",
        " 'Idle Max',\r\n",
        " 'Idle Min']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWOF-bSbpQ3m"
      },
      "source": [
        "Read X_train and X_test datasets."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 216
        },
        "id": "y2VrfaFPphm3",
        "outputId": "83d8ca57-fbaa-4ce9-bfc8-d3f207bfeb78"
      },
      "source": [
        "X_train = pd.read_csv(cleanfile, skiprows=0,index_col=0) \r\n",
        "X_test = pd.read_csv(xtestsmall, skiprows=0, index_col=0)\r\n",
        "X_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>54</th>\n",
              "      <th>55</th>\n",
              "      <th>56</th>\n",
              "      <th>57</th>\n",
              "      <th>58</th>\n",
              "      <th>59</th>\n",
              "      <th>60</th>\n",
              "      <th>61</th>\n",
              "      <th>62</th>\n",
              "      <th>63</th>\n",
              "      <th>64</th>\n",
              "      <th>65</th>\n",
              "      <th>66</th>\n",
              "      <th>67</th>\n",
              "      <th>68</th>\n",
              "      <th>69</th>\n",
              "      <th>70</th>\n",
              "      <th>71</th>\n",
              "      <th>72</th>\n",
              "      <th>73</th>\n",
              "      <th>74</th>\n",
              "      <th>75</th>\n",
              "      <th>76</th>\n",
              "      <th>77</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>80.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>69475.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>43.181000</td>\n",
              "      <td>34737.500000</td>\n",
              "      <td>1.095238e+04</td>\n",
              "      <td>42482.0</td>\n",
              "      <td>26993.0</td>\n",
              "      <td>69475.0</td>\n",
              "      <td>3.473750e+04</td>\n",
              "      <td>1.095238e+04</td>\n",
              "      <td>42482.0</td>\n",
              "      <td>26993.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>43.181000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>225.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>53102.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>39215.686270</td>\n",
              "      <td>51.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>51.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>5.100000e+01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>51.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39215.686270</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>256.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8080.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>517.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3868.471954</td>\n",
              "      <td>517.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>517.0</td>\n",
              "      <td>517.0</td>\n",
              "      <td>517.0</td>\n",
              "      <td>5.170000e+02</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>517.0</td>\n",
              "      <td>517.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3868.471954</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2052.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>22.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>369481.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>2665.0</td>\n",
              "      <td>640.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>87.636364</td>\n",
              "      <td>137.780552</td>\n",
              "      <td>976.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>121.136364</td>\n",
              "      <td>258.641560</td>\n",
              "      <td>12430.950441</td>\n",
              "      <td>119.085961</td>\n",
              "      <td>8592.581395</td>\n",
              "      <td>2.276687e+04</td>\n",
              "      <td>97771.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>369439.0</td>\n",
              "      <td>1.759233e+04</td>\n",
              "      <td>3.575191e+04</td>\n",
              "      <td>133959.0</td>\n",
              "      <td>163.0</td>\n",
              "      <td>369470.0</td>\n",
              "      <td>17593.809524</td>\n",
              "      <td>38826.475848</td>\n",
              "      <td>129556.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>712.0</td>\n",
              "      <td>712.0</td>\n",
              "      <td>59.542981</td>\n",
              "      <td>59.542981</td>\n",
              "      <td>0.0</td>\n",
              "      <td>976.0</td>\n",
              "      <td>102.066667</td>\n",
              "      <td>203.740967</td>\n",
              "      <td>41510.381818</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>104.386364</td>\n",
              "      <td>87.636364</td>\n",
              "      <td>121.136364</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>2665.0</td>\n",
              "      <td>26883.0</td>\n",
              "      <td>230.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>80.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5007525.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>646.0</td>\n",
              "      <td>364.0</td>\n",
              "      <td>646.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>129.200000</td>\n",
              "      <td>288.899983</td>\n",
              "      <td>364.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>121.333333</td>\n",
              "      <td>210.155498</td>\n",
              "      <td>201.696447</td>\n",
              "      <td>1.597596</td>\n",
              "      <td>715360.714286</td>\n",
              "      <td>1.868948e+06</td>\n",
              "      <td>4953524.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>5007525.0</td>\n",
              "      <td>1.251881e+06</td>\n",
              "      <td>2.467885e+06</td>\n",
              "      <td>4953524.0</td>\n",
              "      <td>406.0</td>\n",
              "      <td>3255.0</td>\n",
              "      <td>1627.500000</td>\n",
              "      <td>634.274783</td>\n",
              "      <td>2076.0</td>\n",
              "      <td>1179.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>0.998497</td>\n",
              "      <td>0.599098</td>\n",
              "      <td>0.0</td>\n",
              "      <td>646.0</td>\n",
              "      <td>112.222222</td>\n",
              "      <td>233.577491</td>\n",
              "      <td>54558.444444</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>129.200000</td>\n",
              "      <td>121.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>646.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>364.0</td>\n",
              "      <td>8192.0</td>\n",
              "      <td>221.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         0    1          2     3     4       5  ...   72   73   74   75   76   77\n",
              "0     80.0  6.0    69475.0   3.0   0.0     0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "1  53102.0  6.0       51.0   2.0   0.0     0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "2   8080.0  6.0      517.0   2.0   0.0     0.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "3     22.0  6.0   369481.0  22.0  22.0  1928.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "4     80.0  6.0  5007525.0   5.0   3.0   646.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "\n",
              "[5 rows x 78 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zk7F7tZtwcO_"
      },
      "source": [
        "Add the column names to them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "2J9hpvRLrPTw",
        "outputId": "6bedfc63-3c20-47ac-d7b1-ed8535f54a09"
      },
      "source": [
        " X_train.columns =  column_names\r\n",
        " X_test.columns = column_names\r\n",
        " X_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dst Port</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Tot Fwd Pkts</th>\n",
              "      <th>Tot Bwd Pkts</th>\n",
              "      <th>TotLen Fwd Pkts</th>\n",
              "      <th>TotLen Bwd Pkts</th>\n",
              "      <th>Fwd Pkt Len Max</th>\n",
              "      <th>Fwd Pkt Len Min</th>\n",
              "      <th>Fwd Pkt Len Mean</th>\n",
              "      <th>Fwd Pkt Len Std</th>\n",
              "      <th>Bwd Pkt Len Max</th>\n",
              "      <th>Bwd Pkt Len Min</th>\n",
              "      <th>Bwd Pkt Len Mean</th>\n",
              "      <th>Bwd Pkt Len Std</th>\n",
              "      <th>Flow Byts/s</th>\n",
              "      <th>Flow Pkts/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Tot</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Tot</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Bwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Bwd URG Flags</th>\n",
              "      <th>Fwd Header Len</th>\n",
              "      <th>Bwd Header Len</th>\n",
              "      <th>Fwd Pkts/s</th>\n",
              "      <th>Bwd Pkts/s</th>\n",
              "      <th>Pkt Len Min</th>\n",
              "      <th>Pkt Len Max</th>\n",
              "      <th>Pkt Len Mean</th>\n",
              "      <th>Pkt Len Std</th>\n",
              "      <th>Pkt Len Var</th>\n",
              "      <th>FIN Flag Cnt</th>\n",
              "      <th>SYN Flag Cnt</th>\n",
              "      <th>RST Flag Cnt</th>\n",
              "      <th>PSH Flag Cnt</th>\n",
              "      <th>ACK Flag Cnt</th>\n",
              "      <th>URG Flag Cnt</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Cnt</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Pkt Size Avg</th>\n",
              "      <th>Fwd Seg Size Avg</th>\n",
              "      <th>Bwd Seg Size Avg</th>\n",
              "      <th>Fwd Byts/b Avg</th>\n",
              "      <th>Fwd Pkts/b Avg</th>\n",
              "      <th>Fwd Blk Rate Avg</th>\n",
              "      <th>Bwd Byts/b Avg</th>\n",
              "      <th>Bwd Pkts/b Avg</th>\n",
              "      <th>Bwd Blk Rate Avg</th>\n",
              "      <th>Subflow Fwd Pkts</th>\n",
              "      <th>Subflow Fwd Byts</th>\n",
              "      <th>Subflow Bwd Pkts</th>\n",
              "      <th>Subflow Bwd Byts</th>\n",
              "      <th>Init Fwd Win Byts</th>\n",
              "      <th>Init Bwd Win Byts</th>\n",
              "      <th>Fwd Act Data Pkts</th>\n",
              "      <th>Fwd Seg Size Min</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>80.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>69475.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>43.181000</td>\n",
              "      <td>34737.500000</td>\n",
              "      <td>1.095238e+04</td>\n",
              "      <td>42482.0</td>\n",
              "      <td>26993.0</td>\n",
              "      <td>69475.0</td>\n",
              "      <td>3.473750e+04</td>\n",
              "      <td>1.095238e+04</td>\n",
              "      <td>42482.0</td>\n",
              "      <td>26993.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>43.181000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>225.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>53102.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>39215.686270</td>\n",
              "      <td>51.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>51.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>5.100000e+01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>51.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39215.686270</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>256.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8080.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>517.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3868.471954</td>\n",
              "      <td>517.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>517.0</td>\n",
              "      <td>517.0</td>\n",
              "      <td>517.0</td>\n",
              "      <td>5.170000e+02</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>517.0</td>\n",
              "      <td>517.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3868.471954</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2052.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>22.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>369481.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>2665.0</td>\n",
              "      <td>640.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>87.636364</td>\n",
              "      <td>137.780552</td>\n",
              "      <td>976.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>121.136364</td>\n",
              "      <td>258.641560</td>\n",
              "      <td>12430.950441</td>\n",
              "      <td>119.085961</td>\n",
              "      <td>8592.581395</td>\n",
              "      <td>2.276687e+04</td>\n",
              "      <td>97771.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>369439.0</td>\n",
              "      <td>1.759233e+04</td>\n",
              "      <td>3.575191e+04</td>\n",
              "      <td>133959.0</td>\n",
              "      <td>163.0</td>\n",
              "      <td>369470.0</td>\n",
              "      <td>17593.809524</td>\n",
              "      <td>38826.475848</td>\n",
              "      <td>129556.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>712.0</td>\n",
              "      <td>712.0</td>\n",
              "      <td>59.542981</td>\n",
              "      <td>59.542981</td>\n",
              "      <td>0.0</td>\n",
              "      <td>976.0</td>\n",
              "      <td>102.066667</td>\n",
              "      <td>203.740967</td>\n",
              "      <td>41510.381818</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>104.386364</td>\n",
              "      <td>87.636364</td>\n",
              "      <td>121.136364</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>2665.0</td>\n",
              "      <td>26883.0</td>\n",
              "      <td>230.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>80.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>5007525.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>646.0</td>\n",
              "      <td>364.0</td>\n",
              "      <td>646.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>129.200000</td>\n",
              "      <td>288.899983</td>\n",
              "      <td>364.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>121.333333</td>\n",
              "      <td>210.155498</td>\n",
              "      <td>201.696447</td>\n",
              "      <td>1.597596</td>\n",
              "      <td>715360.714286</td>\n",
              "      <td>1.868948e+06</td>\n",
              "      <td>4953524.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>5007525.0</td>\n",
              "      <td>1.251881e+06</td>\n",
              "      <td>2.467885e+06</td>\n",
              "      <td>4953524.0</td>\n",
              "      <td>406.0</td>\n",
              "      <td>3255.0</td>\n",
              "      <td>1627.500000</td>\n",
              "      <td>634.274783</td>\n",
              "      <td>2076.0</td>\n",
              "      <td>1179.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>0.998497</td>\n",
              "      <td>0.599098</td>\n",
              "      <td>0.0</td>\n",
              "      <td>646.0</td>\n",
              "      <td>112.222222</td>\n",
              "      <td>233.577491</td>\n",
              "      <td>54558.444444</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>129.200000</td>\n",
              "      <td>121.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>646.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>364.0</td>\n",
              "      <td>8192.0</td>\n",
              "      <td>221.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Dst Port  Protocol  Flow Duration  ...  Idle Std  Idle Max  Idle Min\n",
              "0      80.0       6.0        69475.0  ...       0.0       0.0       0.0\n",
              "1   53102.0       6.0           51.0  ...       0.0       0.0       0.0\n",
              "2    8080.0       6.0          517.0  ...       0.0       0.0       0.0\n",
              "3      22.0       6.0       369481.0  ...       0.0       0.0       0.0\n",
              "4      80.0       6.0      5007525.0  ...       0.0       0.0       0.0\n",
              "\n",
              "[5 rows x 78 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6cwDqLzgwfwY"
      },
      "source": [
        "reorder the columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "pOsx7nPZwitY",
        "outputId": "c62f5a0b-0b49-4133-89fd-3acdeacba57c"
      },
      "source": [
        "X_train_important = X_train.reindex(important_column_names, axis=1)\r\n",
        "X_test_important = X_test.reindex(important_column_names, axis=1)\r\n",
        "X_train_important.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dst Port</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Init Fwd Win Byts</th>\n",
              "      <th>Init Bwd Win Byts</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Tot Fwd Pkts</th>\n",
              "      <th>Tot Bwd Pkts</th>\n",
              "      <th>TotLen Fwd Pkts</th>\n",
              "      <th>TotLen Bwd Pkts</th>\n",
              "      <th>Fwd Pkt Len Max</th>\n",
              "      <th>Fwd Pkt Len Min</th>\n",
              "      <th>Fwd Pkt Len Mean</th>\n",
              "      <th>Fwd Pkt Len Std</th>\n",
              "      <th>Bwd Pkt Len Max</th>\n",
              "      <th>Bwd Pkt Len Min</th>\n",
              "      <th>Bwd Pkt Len Mean</th>\n",
              "      <th>Bwd Pkt Len Std</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Flow Byts/s</th>\n",
              "      <th>Flow Pkts/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Fwd IAT Tot</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Bwd IAT Tot</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Bwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Bwd URG Flags</th>\n",
              "      <th>Fwd Header Len</th>\n",
              "      <th>Bwd Header Len</th>\n",
              "      <th>Fwd Pkts/s</th>\n",
              "      <th>Bwd Pkts/s</th>\n",
              "      <th>Pkt Len Min</th>\n",
              "      <th>Pkt Len Max</th>\n",
              "      <th>Pkt Len Mean</th>\n",
              "      <th>Pkt Len Std</th>\n",
              "      <th>Pkt Len Var</th>\n",
              "      <th>FIN Flag Cnt</th>\n",
              "      <th>SYN Flag Cnt</th>\n",
              "      <th>RST Flag Cnt</th>\n",
              "      <th>PSH Flag Cnt</th>\n",
              "      <th>ACK Flag Cnt</th>\n",
              "      <th>URG Flag Cnt</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Cnt</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Pkt Size Avg</th>\n",
              "      <th>Fwd Seg Size Avg</th>\n",
              "      <th>Bwd Seg Size Avg</th>\n",
              "      <th>Fwd Byts/b Avg</th>\n",
              "      <th>Fwd Pkts/b Avg</th>\n",
              "      <th>Fwd Blk Rate Avg</th>\n",
              "      <th>Bwd Byts/b Avg</th>\n",
              "      <th>Bwd Pkts/b Avg</th>\n",
              "      <th>Bwd Blk Rate Avg</th>\n",
              "      <th>Subflow Fwd Pkts</th>\n",
              "      <th>Subflow Fwd Byts</th>\n",
              "      <th>Subflow Bwd Pkts</th>\n",
              "      <th>Subflow Bwd Byts</th>\n",
              "      <th>Fwd Act Data Pkts</th>\n",
              "      <th>Fwd Seg Size Min</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>80.0</td>\n",
              "      <td>26993.0</td>\n",
              "      <td>225.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>26993.0</td>\n",
              "      <td>69475.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>43.181000</td>\n",
              "      <td>34737.500000</td>\n",
              "      <td>1.095238e+04</td>\n",
              "      <td>42482.0</td>\n",
              "      <td>69475.0</td>\n",
              "      <td>3.473750e+04</td>\n",
              "      <td>1.095238e+04</td>\n",
              "      <td>42482.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>96.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>43.181000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>53102.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>256.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>39215.686270</td>\n",
              "      <td>51.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>51.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>5.100000e+01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>51.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>39215.686270</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>8080.0</td>\n",
              "      <td>517.0</td>\n",
              "      <td>2052.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>517.0</td>\n",
              "      <td>517.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3868.471954</td>\n",
              "      <td>517.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>517.0</td>\n",
              "      <td>517.0</td>\n",
              "      <td>5.170000e+02</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>517.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3868.471954</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>22.0</td>\n",
              "      <td>163.0</td>\n",
              "      <td>26883.0</td>\n",
              "      <td>230.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>369481.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>2665.0</td>\n",
              "      <td>640.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>87.636364</td>\n",
              "      <td>137.780552</td>\n",
              "      <td>976.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>121.136364</td>\n",
              "      <td>258.641560</td>\n",
              "      <td>6.0</td>\n",
              "      <td>12430.950441</td>\n",
              "      <td>119.085961</td>\n",
              "      <td>8592.581395</td>\n",
              "      <td>2.276687e+04</td>\n",
              "      <td>97771.0</td>\n",
              "      <td>369439.0</td>\n",
              "      <td>1.759233e+04</td>\n",
              "      <td>3.575191e+04</td>\n",
              "      <td>133959.0</td>\n",
              "      <td>369470.0</td>\n",
              "      <td>17593.809524</td>\n",
              "      <td>38826.475848</td>\n",
              "      <td>129556.0</td>\n",
              "      <td>11.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>712.0</td>\n",
              "      <td>712.0</td>\n",
              "      <td>59.542981</td>\n",
              "      <td>59.542981</td>\n",
              "      <td>0.0</td>\n",
              "      <td>976.0</td>\n",
              "      <td>102.066667</td>\n",
              "      <td>203.740967</td>\n",
              "      <td>41510.381818</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>104.386364</td>\n",
              "      <td>87.636364</td>\n",
              "      <td>121.136364</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>1928.0</td>\n",
              "      <td>22.0</td>\n",
              "      <td>2665.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>80.0</td>\n",
              "      <td>406.0</td>\n",
              "      <td>8192.0</td>\n",
              "      <td>221.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>5007525.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>646.0</td>\n",
              "      <td>364.0</td>\n",
              "      <td>646.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>129.200000</td>\n",
              "      <td>288.899983</td>\n",
              "      <td>364.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>121.333333</td>\n",
              "      <td>210.155498</td>\n",
              "      <td>6.0</td>\n",
              "      <td>201.696447</td>\n",
              "      <td>1.597596</td>\n",
              "      <td>715360.714286</td>\n",
              "      <td>1.868948e+06</td>\n",
              "      <td>4953524.0</td>\n",
              "      <td>5007525.0</td>\n",
              "      <td>1.251881e+06</td>\n",
              "      <td>2.467885e+06</td>\n",
              "      <td>4953524.0</td>\n",
              "      <td>3255.0</td>\n",
              "      <td>1627.500000</td>\n",
              "      <td>634.274783</td>\n",
              "      <td>2076.0</td>\n",
              "      <td>1179.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>112.0</td>\n",
              "      <td>72.0</td>\n",
              "      <td>0.998497</td>\n",
              "      <td>0.599098</td>\n",
              "      <td>0.0</td>\n",
              "      <td>646.0</td>\n",
              "      <td>112.222222</td>\n",
              "      <td>233.577491</td>\n",
              "      <td>54558.444444</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>126.250000</td>\n",
              "      <td>129.200000</td>\n",
              "      <td>121.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>646.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>364.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Dst Port  Fwd IAT Min  Init Fwd Win Byts  ...  Idle Std  Idle Max  Idle Min\n",
              "0      80.0      26993.0              225.0  ...       0.0       0.0       0.0\n",
              "1   53102.0         51.0              256.0  ...       0.0       0.0       0.0\n",
              "2    8080.0        517.0             2052.0  ...       0.0       0.0       0.0\n",
              "3      22.0        163.0            26883.0  ...       0.0       0.0       0.0\n",
              "4      80.0        406.0             8192.0  ...       0.0       0.0       0.0\n",
              "\n",
              "[5 rows x 78 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "En0usKzvxFyC",
        "outputId": "45a1cf65-51fe-44ba-ead1-ded338d641fc"
      },
      "source": [
        "X_test_important.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Dst Port</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Init Fwd Win Byts</th>\n",
              "      <th>Init Bwd Win Byts</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Tot Fwd Pkts</th>\n",
              "      <th>Tot Bwd Pkts</th>\n",
              "      <th>TotLen Fwd Pkts</th>\n",
              "      <th>TotLen Bwd Pkts</th>\n",
              "      <th>Fwd Pkt Len Max</th>\n",
              "      <th>Fwd Pkt Len Min</th>\n",
              "      <th>Fwd Pkt Len Mean</th>\n",
              "      <th>Fwd Pkt Len Std</th>\n",
              "      <th>Bwd Pkt Len Max</th>\n",
              "      <th>Bwd Pkt Len Min</th>\n",
              "      <th>Bwd Pkt Len Mean</th>\n",
              "      <th>Bwd Pkt Len Std</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Flow Byts/s</th>\n",
              "      <th>Flow Pkts/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Fwd IAT Tot</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Bwd IAT Tot</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Bwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Bwd URG Flags</th>\n",
              "      <th>Fwd Header Len</th>\n",
              "      <th>Bwd Header Len</th>\n",
              "      <th>Fwd Pkts/s</th>\n",
              "      <th>Bwd Pkts/s</th>\n",
              "      <th>Pkt Len Min</th>\n",
              "      <th>Pkt Len Max</th>\n",
              "      <th>Pkt Len Mean</th>\n",
              "      <th>Pkt Len Std</th>\n",
              "      <th>Pkt Len Var</th>\n",
              "      <th>FIN Flag Cnt</th>\n",
              "      <th>SYN Flag Cnt</th>\n",
              "      <th>RST Flag Cnt</th>\n",
              "      <th>PSH Flag Cnt</th>\n",
              "      <th>ACK Flag Cnt</th>\n",
              "      <th>URG Flag Cnt</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Cnt</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Pkt Size Avg</th>\n",
              "      <th>Fwd Seg Size Avg</th>\n",
              "      <th>Bwd Seg Size Avg</th>\n",
              "      <th>Fwd Byts/b Avg</th>\n",
              "      <th>Fwd Pkts/b Avg</th>\n",
              "      <th>Fwd Blk Rate Avg</th>\n",
              "      <th>Bwd Byts/b Avg</th>\n",
              "      <th>Bwd Pkts/b Avg</th>\n",
              "      <th>Bwd Blk Rate Avg</th>\n",
              "      <th>Subflow Fwd Pkts</th>\n",
              "      <th>Subflow Fwd Byts</th>\n",
              "      <th>Subflow Bwd Pkts</th>\n",
              "      <th>Subflow Bwd Byts</th>\n",
              "      <th>Fwd Act Data Pkts</th>\n",
              "      <th>Fwd Seg Size Min</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>80.0</td>\n",
              "      <td>53422.0</td>\n",
              "      <td>225.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>53422.0</td>\n",
              "      <td>53422.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>37.437760</td>\n",
              "      <td>5.342200e+04</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>53422.0</td>\n",
              "      <td>53422.0</td>\n",
              "      <td>53422.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>53422.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>37.437760</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>51863.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>946.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.333333</td>\n",
              "      <td>17.897858</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.030000e+07</td>\n",
              "      <td>1000000.000000</td>\n",
              "      <td>1.500000e+00</td>\n",
              "      <td>7.071068e-01</td>\n",
              "      <td>2.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.707107</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1000000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>7.75</td>\n",
              "      <td>15.500000</td>\n",
              "      <td>240.250000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>10.333333</td>\n",
              "      <td>10.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>31.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>80.0</td>\n",
              "      <td>81829392.0</td>\n",
              "      <td>211.0</td>\n",
              "      <td>219.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>81829392.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>5.656854</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>9.776438e-02</td>\n",
              "      <td>0.036662</td>\n",
              "      <td>4.091470e+07</td>\n",
              "      <td>5.786212e+07</td>\n",
              "      <td>81829390.0</td>\n",
              "      <td>81829392.0</td>\n",
              "      <td>81829392.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>81829392.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.024441</td>\n",
              "      <td>0.012221</td>\n",
              "      <td>0.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>4.00</td>\n",
              "      <td>4.618802</td>\n",
              "      <td>21.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>5.333333</td>\n",
              "      <td>4.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>53.0</td>\n",
              "      <td>18491.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>101932.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>46.0</td>\n",
              "      <td>46.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>148.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>3.806459e+03</td>\n",
              "      <td>39.241848</td>\n",
              "      <td>3.397733e+04</td>\n",
              "      <td>4.379175e+04</td>\n",
              "      <td>83408.0</td>\n",
              "      <td>18491.0</td>\n",
              "      <td>18491.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>18491.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>33.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>16.0</td>\n",
              "      <td>19.620924</td>\n",
              "      <td>19.620924</td>\n",
              "      <td>46.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>86.80</td>\n",
              "      <td>55.867701</td>\n",
              "      <td>3121.200000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>108.500000</td>\n",
              "      <td>46.000000</td>\n",
              "      <td>148.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>296.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>8.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>80.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>2047.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>20000.000000</td>\n",
              "      <td>1.000000e+02</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>100.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20000.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Dst Port  Fwd IAT Min  Init Fwd Win Byts  ...  Idle Std  Idle Max  Idle Min\n",
              "0      80.0      53422.0              225.0  ...       0.0       0.0       0.0\n",
              "1   51863.0          1.0              946.0  ...       0.0       0.0       0.0\n",
              "2      80.0   81829392.0              211.0  ...       0.0       0.0       0.0\n",
              "3      53.0      18491.0               -1.0  ...       0.0       0.0       0.0\n",
              "4      80.0        100.0             2047.0  ...       0.0       0.0       0.0\n",
              "\n",
              "[5 rows x 78 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFUMzqOxxbUp"
      },
      "source": [
        "## Step 6. Normalization Again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-cNL8zlxirR"
      },
      "source": [
        "scaler = MinMaxScaler()\r\n",
        "X_train = scaler.fit_transform(X_train_important)\r\n",
        "X_test = scaler.fit_transform(X_test_important)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pXpwnZcgyGez"
      },
      "source": [
        "X_train_r = np.zeros((len(X_train), input_dim, 1))\r\n",
        "X_train_r[:, :, 0] = X_train[:, :input_dim]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rbjAXJNayJjD"
      },
      "source": [
        "X_test_r = np.zeros((len(X_test), input_dim, 1))\r\n",
        "X_test_r[:, :, 0] = X_test[:, :input_dim]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZRFaDcvDyYnS"
      },
      "source": [
        "## Step 7. Reuse the Model and train it again"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EOi89ZXY0YmS",
        "outputId": "57e12328-26f9-4e7c-d7ba-92473cd00972"
      },
      "source": [
        "model = Sequential()\r\n",
        "\r\n",
        "# input layer\r\n",
        "model.add(Conv1D(filters=32, kernel_size=17, padding='same', input_shape=(78,1)))\r\n",
        "model.add(BatchNormalization(axis=1))\r\n",
        "model.add(Activation('relu'))\r\n",
        "model.add(MaxPooling1D(pool_size=2))\r\n",
        "\r\n",
        "model.add(Conv1D(filters=128, kernel_size=7))\r\n",
        "model.add(BatchNormalization(axis=1))\r\n",
        "model.add(Activation('relu'))\r\n",
        "model.add(MaxPooling1D(pool_size=2))\r\n",
        "\r\n",
        "model.add(Conv1D(filters=256, kernel_size=5))\r\n",
        "model.add(BatchNormalization(axis=1))\r\n",
        "model.add(Activation('relu'))\r\n",
        "\r\n",
        "model.add(Conv1D(filters=512, kernel_size=3))\r\n",
        "model.add(BatchNormalization(axis=1))\r\n",
        "model.add(Activation('relu'))\r\n",
        "\r\n",
        "model.add(Conv1D(filters=256, kernel_size=3))\r\n",
        "model.add(BatchNormalization(axis=1))\r\n",
        "model.add(Activation('relu'))\r\n",
        "\r\n",
        "model.add(Flatten())\r\n",
        "model.add(Dropout(0.5))\r\n",
        "model.add(Dense(100, activation='relu'))\r\n",
        "model.add(Dense(num_class))\r\n",
        "model.add(Activation('softmax'))\r\n",
        "\r\n",
        "\r\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_15 (Conv1D)           (None, 78, 32)            576       \n",
            "_________________________________________________________________\n",
            "batch_normalization_15 (Batc (None, 78, 32)            312       \n",
            "_________________________________________________________________\n",
            "activation_18 (Activation)   (None, 78, 32)            0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_6 (MaxPooling1 (None, 39, 32)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_16 (Conv1D)           (None, 33, 128)           28800     \n",
            "_________________________________________________________________\n",
            "batch_normalization_16 (Batc (None, 33, 128)           132       \n",
            "_________________________________________________________________\n",
            "activation_19 (Activation)   (None, 33, 128)           0         \n",
            "_________________________________________________________________\n",
            "max_pooling1d_7 (MaxPooling1 (None, 16, 128)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_17 (Conv1D)           (None, 12, 256)           164096    \n",
            "_________________________________________________________________\n",
            "batch_normalization_17 (Batc (None, 12, 256)           48        \n",
            "_________________________________________________________________\n",
            "activation_20 (Activation)   (None, 12, 256)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_18 (Conv1D)           (None, 10, 512)           393728    \n",
            "_________________________________________________________________\n",
            "batch_normalization_18 (Batc (None, 10, 512)           40        \n",
            "_________________________________________________________________\n",
            "activation_21 (Activation)   (None, 10, 512)           0         \n",
            "_________________________________________________________________\n",
            "conv1d_19 (Conv1D)           (None, 8, 256)            393472    \n",
            "_________________________________________________________________\n",
            "batch_normalization_19 (Batc (None, 8, 256)            32        \n",
            "_________________________________________________________________\n",
            "activation_22 (Activation)   (None, 8, 256)            0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 2048)              0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 100)               204900    \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 15)                1515      \n",
            "_________________________________________________________________\n",
            "activation_23 (Activation)   (None, 15)                0         \n",
            "=================================================================\n",
            "Total params: 1,187,651\n",
            "Trainable params: 1,187,369\n",
            "Non-trainable params: 282\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nt4Svw_5yc4p"
      },
      "source": [
        "#num_epochs = 35\r\n",
        "\r\n",
        "model2.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwWdtA17yd-U",
        "outputId": "012dd561-9f9c-4168-ef7f-e8b61a8ed4a2"
      },
      "source": [
        "model2.fit(X_train_r, y_train, epochs=num_epochs, batch_size=batch_size, validation_data=(X_test_r, y_test), verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/90\n",
            "235/235 [==============================] - 2s 5ms/step - loss: 1.1727 - accuracy: 0.6958 - val_loss: 0.3491 - val_accuracy: 0.8605\n",
            "Epoch 2/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.3360 - accuracy: 0.8449 - val_loss: 0.3285 - val_accuracy: 0.8611\n",
            "Epoch 3/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.3000 - accuracy: 0.8583 - val_loss: 0.3139 - val_accuracy: 0.8661\n",
            "Epoch 4/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2900 - accuracy: 0.8577 - val_loss: 0.3088 - val_accuracy: 0.8672\n",
            "Epoch 5/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2725 - accuracy: 0.8687 - val_loss: 0.3158 - val_accuracy: 0.8571\n",
            "Epoch 6/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2715 - accuracy: 0.8662 - val_loss: 0.2957 - val_accuracy: 0.8693\n",
            "Epoch 7/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2669 - accuracy: 0.8676 - val_loss: 0.2915 - val_accuracy: 0.8681\n",
            "Epoch 8/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2596 - accuracy: 0.8714 - val_loss: 0.2847 - val_accuracy: 0.8739\n",
            "Epoch 9/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2600 - accuracy: 0.8701 - val_loss: 0.2828 - val_accuracy: 0.8721\n",
            "Epoch 10/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2453 - accuracy: 0.8775 - val_loss: 0.2997 - val_accuracy: 0.8633\n",
            "Epoch 11/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2575 - accuracy: 0.8711 - val_loss: 0.2918 - val_accuracy: 0.8661\n",
            "Epoch 12/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2528 - accuracy: 0.8755 - val_loss: 0.2889 - val_accuracy: 0.8696\n",
            "Epoch 13/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2487 - accuracy: 0.8740 - val_loss: 0.2832 - val_accuracy: 0.8709\n",
            "Epoch 14/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2444 - accuracy: 0.8791 - val_loss: 0.2920 - val_accuracy: 0.8644\n",
            "Epoch 15/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2444 - accuracy: 0.8757 - val_loss: 0.2850 - val_accuracy: 0.8645\n",
            "Epoch 16/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2380 - accuracy: 0.8769 - val_loss: 0.2929 - val_accuracy: 0.8772\n",
            "Epoch 17/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2435 - accuracy: 0.8784 - val_loss: 0.2763 - val_accuracy: 0.8787\n",
            "Epoch 18/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2382 - accuracy: 0.8817 - val_loss: 0.2752 - val_accuracy: 0.8813\n",
            "Epoch 19/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2422 - accuracy: 0.8776 - val_loss: 0.2827 - val_accuracy: 0.8796\n",
            "Epoch 20/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2434 - accuracy: 0.8770 - val_loss: 0.2824 - val_accuracy: 0.8649\n",
            "Epoch 21/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2387 - accuracy: 0.8759 - val_loss: 0.2735 - val_accuracy: 0.8793\n",
            "Epoch 22/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2370 - accuracy: 0.8802 - val_loss: 0.2779 - val_accuracy: 0.8777\n",
            "Epoch 23/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2392 - accuracy: 0.8774 - val_loss: 0.2819 - val_accuracy: 0.8805\n",
            "Epoch 24/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2302 - accuracy: 0.8797 - val_loss: 0.2757 - val_accuracy: 0.8704\n",
            "Epoch 25/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2373 - accuracy: 0.8786 - val_loss: 0.2777 - val_accuracy: 0.8784\n",
            "Epoch 26/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2332 - accuracy: 0.8793 - val_loss: 0.2739 - val_accuracy: 0.8784\n",
            "Epoch 27/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2345 - accuracy: 0.8807 - val_loss: 0.2767 - val_accuracy: 0.8703\n",
            "Epoch 28/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2346 - accuracy: 0.8782 - val_loss: 0.2800 - val_accuracy: 0.8671\n",
            "Epoch 29/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2288 - accuracy: 0.8801 - val_loss: 0.2714 - val_accuracy: 0.8811\n",
            "Epoch 30/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2317 - accuracy: 0.8799 - val_loss: 0.2813 - val_accuracy: 0.8741\n",
            "Epoch 31/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2271 - accuracy: 0.8818 - val_loss: 0.2814 - val_accuracy: 0.8785\n",
            "Epoch 32/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2308 - accuracy: 0.8817 - val_loss: 0.2757 - val_accuracy: 0.8751\n",
            "Epoch 33/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2325 - accuracy: 0.8801 - val_loss: 0.2738 - val_accuracy: 0.8779\n",
            "Epoch 34/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2295 - accuracy: 0.8822 - val_loss: 0.2749 - val_accuracy: 0.8789\n",
            "Epoch 35/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2304 - accuracy: 0.8796 - val_loss: 0.2767 - val_accuracy: 0.8707\n",
            "Epoch 36/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2275 - accuracy: 0.8830 - val_loss: 0.2754 - val_accuracy: 0.8776\n",
            "Epoch 37/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2288 - accuracy: 0.8785 - val_loss: 0.2813 - val_accuracy: 0.8617\n",
            "Epoch 38/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2294 - accuracy: 0.8830 - val_loss: 0.2787 - val_accuracy: 0.8639\n",
            "Epoch 39/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2302 - accuracy: 0.8784 - val_loss: 0.2792 - val_accuracy: 0.8665\n",
            "Epoch 40/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2247 - accuracy: 0.8808 - val_loss: 0.2878 - val_accuracy: 0.8563\n",
            "Epoch 41/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2260 - accuracy: 0.8816 - val_loss: 0.2782 - val_accuracy: 0.8764\n",
            "Epoch 42/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2307 - accuracy: 0.8781 - val_loss: 0.2876 - val_accuracy: 0.8632\n",
            "Epoch 43/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2285 - accuracy: 0.8813 - val_loss: 0.2766 - val_accuracy: 0.8795\n",
            "Epoch 44/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2266 - accuracy: 0.8793 - val_loss: 0.2836 - val_accuracy: 0.8631\n",
            "Epoch 45/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2242 - accuracy: 0.8816 - val_loss: 0.2833 - val_accuracy: 0.8609\n",
            "Epoch 46/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2285 - accuracy: 0.8816 - val_loss: 0.2777 - val_accuracy: 0.8671\n",
            "Epoch 47/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2263 - accuracy: 0.8808 - val_loss: 0.2823 - val_accuracy: 0.8784\n",
            "Epoch 48/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2282 - accuracy: 0.8818 - val_loss: 0.2762 - val_accuracy: 0.8793\n",
            "Epoch 49/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2179 - accuracy: 0.8883 - val_loss: 0.2887 - val_accuracy: 0.8677\n",
            "Epoch 50/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2273 - accuracy: 0.8835 - val_loss: 0.2806 - val_accuracy: 0.8784\n",
            "Epoch 51/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2237 - accuracy: 0.8833 - val_loss: 0.2729 - val_accuracy: 0.8677\n",
            "Epoch 52/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2287 - accuracy: 0.8778 - val_loss: 0.2807 - val_accuracy: 0.8772\n",
            "Epoch 53/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2272 - accuracy: 0.8832 - val_loss: 0.2878 - val_accuracy: 0.8737\n",
            "Epoch 54/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2267 - accuracy: 0.8851 - val_loss: 0.2792 - val_accuracy: 0.8787\n",
            "Epoch 55/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2218 - accuracy: 0.8840 - val_loss: 0.2781 - val_accuracy: 0.8804\n",
            "Epoch 56/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2270 - accuracy: 0.8840 - val_loss: 0.2867 - val_accuracy: 0.8528\n",
            "Epoch 57/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2251 - accuracy: 0.8825 - val_loss: 0.2869 - val_accuracy: 0.8772\n",
            "Epoch 58/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2221 - accuracy: 0.8882 - val_loss: 0.2828 - val_accuracy: 0.8617\n",
            "Epoch 59/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2227 - accuracy: 0.8836 - val_loss: 0.2845 - val_accuracy: 0.8779\n",
            "Epoch 60/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2253 - accuracy: 0.8827 - val_loss: 0.2807 - val_accuracy: 0.8788\n",
            "Epoch 61/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2190 - accuracy: 0.8864 - val_loss: 0.2802 - val_accuracy: 0.8737\n",
            "Epoch 62/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2234 - accuracy: 0.8846 - val_loss: 0.2844 - val_accuracy: 0.8803\n",
            "Epoch 63/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2217 - accuracy: 0.8873 - val_loss: 0.2816 - val_accuracy: 0.8625\n",
            "Epoch 64/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2176 - accuracy: 0.8847 - val_loss: 0.2908 - val_accuracy: 0.8625\n",
            "Epoch 65/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2208 - accuracy: 0.8852 - val_loss: 0.2829 - val_accuracy: 0.8793\n",
            "Epoch 66/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2237 - accuracy: 0.8825 - val_loss: 0.2825 - val_accuracy: 0.8756\n",
            "Epoch 67/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2227 - accuracy: 0.8850 - val_loss: 0.2834 - val_accuracy: 0.8697\n",
            "Epoch 68/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2209 - accuracy: 0.8868 - val_loss: 0.2862 - val_accuracy: 0.8801\n",
            "Epoch 69/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2146 - accuracy: 0.8900 - val_loss: 0.2850 - val_accuracy: 0.8648\n",
            "Epoch 70/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2212 - accuracy: 0.8861 - val_loss: 0.2867 - val_accuracy: 0.8763\n",
            "Epoch 71/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2208 - accuracy: 0.8845 - val_loss: 0.2864 - val_accuracy: 0.8581\n",
            "Epoch 72/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2204 - accuracy: 0.8862 - val_loss: 0.2880 - val_accuracy: 0.8753\n",
            "Epoch 73/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2240 - accuracy: 0.8811 - val_loss: 0.2915 - val_accuracy: 0.8707\n",
            "Epoch 74/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2203 - accuracy: 0.8868 - val_loss: 0.2897 - val_accuracy: 0.8624\n",
            "Epoch 75/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2194 - accuracy: 0.8856 - val_loss: 0.2839 - val_accuracy: 0.8701\n",
            "Epoch 76/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2200 - accuracy: 0.8889 - val_loss: 0.2900 - val_accuracy: 0.8603\n",
            "Epoch 77/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2218 - accuracy: 0.8821 - val_loss: 0.2869 - val_accuracy: 0.8739\n",
            "Epoch 78/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2164 - accuracy: 0.8879 - val_loss: 0.2816 - val_accuracy: 0.8784\n",
            "Epoch 79/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2165 - accuracy: 0.8865 - val_loss: 0.2800 - val_accuracy: 0.8836\n",
            "Epoch 80/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2181 - accuracy: 0.8855 - val_loss: 0.2886 - val_accuracy: 0.8645\n",
            "Epoch 81/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2198 - accuracy: 0.8854 - val_loss: 0.2881 - val_accuracy: 0.8685\n",
            "Epoch 82/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2164 - accuracy: 0.8843 - val_loss: 0.2874 - val_accuracy: 0.8659\n",
            "Epoch 83/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2196 - accuracy: 0.8863 - val_loss: 0.2865 - val_accuracy: 0.8797\n",
            "Epoch 84/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2172 - accuracy: 0.8853 - val_loss: 0.2785 - val_accuracy: 0.8801\n",
            "Epoch 85/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2194 - accuracy: 0.8866 - val_loss: 0.2938 - val_accuracy: 0.8684\n",
            "Epoch 86/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2212 - accuracy: 0.8860 - val_loss: 0.2906 - val_accuracy: 0.8643\n",
            "Epoch 87/90\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2190 - accuracy: 0.8856 - val_loss: 0.2849 - val_accuracy: 0.8651\n",
            "Epoch 88/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2197 - accuracy: 0.8830 - val_loss: 0.2906 - val_accuracy: 0.8591\n",
            "Epoch 89/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2214 - accuracy: 0.8858 - val_loss: 0.2955 - val_accuracy: 0.8612\n",
            "Epoch 90/90\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2234 - accuracy: 0.8830 - val_loss: 0.2888 - val_accuracy: 0.8757\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f59cb6677f0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5db_PWUa3w9i"
      },
      "source": [
        "learning_rates = 1e-6\r\n",
        "\r\n",
        "optim = tf.keras.optimizers.Adam(lr=learning_rates, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\r\n",
        "model2.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lq0xPYef35Pq",
        "outputId": "a37cefcf-fc70-4089-f8e4-ef565758bd4a"
      },
      "source": [
        "model2.fit(X_train_r, y_train, epochs=60, batch_size=batch_size, validation_data=(X_test_r, y_test), verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/60\n",
            "235/235 [==============================] - 2s 5ms/step - loss: 0.2195 - accuracy: 0.8857 - val_loss: 0.2878 - val_accuracy: 0.8759\n",
            "Epoch 2/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2178 - accuracy: 0.8849 - val_loss: 0.2871 - val_accuracy: 0.8757\n",
            "Epoch 3/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2164 - accuracy: 0.8884 - val_loss: 0.2866 - val_accuracy: 0.8799\n",
            "Epoch 4/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2176 - accuracy: 0.8870 - val_loss: 0.2867 - val_accuracy: 0.8797\n",
            "Epoch 5/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2148 - accuracy: 0.8883 - val_loss: 0.2864 - val_accuracy: 0.8799\n",
            "Epoch 6/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2167 - accuracy: 0.8874 - val_loss: 0.2866 - val_accuracy: 0.8800\n",
            "Epoch 7/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2165 - accuracy: 0.8877 - val_loss: 0.2863 - val_accuracy: 0.8800\n",
            "Epoch 8/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2139 - accuracy: 0.8876 - val_loss: 0.2869 - val_accuracy: 0.8801\n",
            "Epoch 9/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2157 - accuracy: 0.8890 - val_loss: 0.2864 - val_accuracy: 0.8801\n",
            "Epoch 10/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2145 - accuracy: 0.8892 - val_loss: 0.2867 - val_accuracy: 0.8748\n",
            "Epoch 11/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2096 - accuracy: 0.8907 - val_loss: 0.2863 - val_accuracy: 0.8729\n",
            "Epoch 12/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2136 - accuracy: 0.8868 - val_loss: 0.2868 - val_accuracy: 0.8720\n",
            "Epoch 13/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2173 - accuracy: 0.8849 - val_loss: 0.2870 - val_accuracy: 0.8732\n",
            "Epoch 14/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2162 - accuracy: 0.8877 - val_loss: 0.2864 - val_accuracy: 0.8732\n",
            "Epoch 15/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2127 - accuracy: 0.8875 - val_loss: 0.2867 - val_accuracy: 0.8739\n",
            "Epoch 16/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2152 - accuracy: 0.8866 - val_loss: 0.2870 - val_accuracy: 0.8739\n",
            "Epoch 17/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2096 - accuracy: 0.8887 - val_loss: 0.2866 - val_accuracy: 0.8740\n",
            "Epoch 18/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2124 - accuracy: 0.8910 - val_loss: 0.2862 - val_accuracy: 0.8739\n",
            "Epoch 19/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2124 - accuracy: 0.8897 - val_loss: 0.2867 - val_accuracy: 0.8739\n",
            "Epoch 20/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2085 - accuracy: 0.8855 - val_loss: 0.2867 - val_accuracy: 0.8740\n",
            "Epoch 21/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2093 - accuracy: 0.8902 - val_loss: 0.2868 - val_accuracy: 0.8740\n",
            "Epoch 22/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2101 - accuracy: 0.8876 - val_loss: 0.2862 - val_accuracy: 0.8739\n",
            "Epoch 23/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2067 - accuracy: 0.8896 - val_loss: 0.2861 - val_accuracy: 0.8739\n",
            "Epoch 24/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2123 - accuracy: 0.8849 - val_loss: 0.2864 - val_accuracy: 0.8740\n",
            "Epoch 25/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2147 - accuracy: 0.8878 - val_loss: 0.2868 - val_accuracy: 0.8728\n",
            "Epoch 26/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2130 - accuracy: 0.8888 - val_loss: 0.2873 - val_accuracy: 0.8725\n",
            "Epoch 27/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2144 - accuracy: 0.8904 - val_loss: 0.2873 - val_accuracy: 0.8736\n",
            "Epoch 28/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2121 - accuracy: 0.8898 - val_loss: 0.2863 - val_accuracy: 0.8737\n",
            "Epoch 29/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2149 - accuracy: 0.8876 - val_loss: 0.2869 - val_accuracy: 0.8737\n",
            "Epoch 30/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2125 - accuracy: 0.8910 - val_loss: 0.2867 - val_accuracy: 0.8737\n",
            "Epoch 31/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2100 - accuracy: 0.8915 - val_loss: 0.2865 - val_accuracy: 0.8727\n",
            "Epoch 32/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2119 - accuracy: 0.8894 - val_loss: 0.2866 - val_accuracy: 0.8737\n",
            "Epoch 33/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2099 - accuracy: 0.8927 - val_loss: 0.2876 - val_accuracy: 0.8737\n",
            "Epoch 34/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2099 - accuracy: 0.8903 - val_loss: 0.2869 - val_accuracy: 0.8739\n",
            "Epoch 35/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2143 - accuracy: 0.8867 - val_loss: 0.2865 - val_accuracy: 0.8739\n",
            "Epoch 36/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2093 - accuracy: 0.8904 - val_loss: 0.2872 - val_accuracy: 0.8727\n",
            "Epoch 37/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2163 - accuracy: 0.8887 - val_loss: 0.2874 - val_accuracy: 0.8755\n",
            "Epoch 38/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2120 - accuracy: 0.8903 - val_loss: 0.2866 - val_accuracy: 0.8757\n",
            "Epoch 39/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2134 - accuracy: 0.8877 - val_loss: 0.2871 - val_accuracy: 0.8739\n",
            "Epoch 40/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2146 - accuracy: 0.8864 - val_loss: 0.2870 - val_accuracy: 0.8740\n",
            "Epoch 41/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2135 - accuracy: 0.8882 - val_loss: 0.2870 - val_accuracy: 0.8739\n",
            "Epoch 42/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2149 - accuracy: 0.8883 - val_loss: 0.2868 - val_accuracy: 0.8743\n",
            "Epoch 43/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2154 - accuracy: 0.8861 - val_loss: 0.2862 - val_accuracy: 0.8732\n",
            "Epoch 44/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2126 - accuracy: 0.8891 - val_loss: 0.2873 - val_accuracy: 0.8757\n",
            "Epoch 45/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2165 - accuracy: 0.8891 - val_loss: 0.2869 - val_accuracy: 0.8740\n",
            "Epoch 46/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2143 - accuracy: 0.8878 - val_loss: 0.2868 - val_accuracy: 0.8740\n",
            "Epoch 47/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2153 - accuracy: 0.8877 - val_loss: 0.2870 - val_accuracy: 0.8740\n",
            "Epoch 48/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2095 - accuracy: 0.8896 - val_loss: 0.2867 - val_accuracy: 0.8740\n",
            "Epoch 49/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2144 - accuracy: 0.8876 - val_loss: 0.2871 - val_accuracy: 0.8768\n",
            "Epoch 50/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2120 - accuracy: 0.8913 - val_loss: 0.2865 - val_accuracy: 0.8741\n",
            "Epoch 51/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2152 - accuracy: 0.8865 - val_loss: 0.2870 - val_accuracy: 0.8768\n",
            "Epoch 52/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2099 - accuracy: 0.8907 - val_loss: 0.2870 - val_accuracy: 0.8756\n",
            "Epoch 53/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2128 - accuracy: 0.8881 - val_loss: 0.2864 - val_accuracy: 0.8768\n",
            "Epoch 54/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2158 - accuracy: 0.8868 - val_loss: 0.2866 - val_accuracy: 0.8767\n",
            "Epoch 55/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2177 - accuracy: 0.8861 - val_loss: 0.2870 - val_accuracy: 0.8729\n",
            "Epoch 56/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2080 - accuracy: 0.8922 - val_loss: 0.2865 - val_accuracy: 0.8768\n",
            "Epoch 57/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2166 - accuracy: 0.8872 - val_loss: 0.2864 - val_accuracy: 0.8765\n",
            "Epoch 58/60\n",
            "235/235 [==============================] - 1s 5ms/step - loss: 0.2117 - accuracy: 0.8897 - val_loss: 0.2865 - val_accuracy: 0.8768\n",
            "Epoch 59/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2100 - accuracy: 0.8913 - val_loss: 0.2868 - val_accuracy: 0.8768\n",
            "Epoch 60/60\n",
            "235/235 [==============================] - 1s 4ms/step - loss: 0.2107 - accuracy: 0.8915 - val_loss: 0.2872 - val_accuracy: 0.8741\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7f59c23c59b0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    }
  ]
}