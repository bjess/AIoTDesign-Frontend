{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "cnn_small_cicids2017_colab.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fwangliberty/AIoTDesign-Frontend/blob/master/cnn_small_cicids2017_colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HakdgCXBymAw"
      },
      "source": [
        "# Intrusion Detection by using small CICIDS 2017 DataSet with 2000 examples each type"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yqdB1_F0ymAz"
      },
      "source": [
        "import os\n",
        "from os.path import join\n",
        "import glob\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l7LnjNAcvll6"
      },
      "source": [
        "def make_value2index(attacks):\r\n",
        "    #make dictionary\r\n",
        "    attacks = sorted(attacks)\r\n",
        "    d = {}\r\n",
        "    counter=0\r\n",
        "    for attack in attacks:\r\n",
        "        d[attack] = counter\r\n",
        "        counter+=1\r\n",
        "    return d"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bJxEKODvmH5"
      },
      "source": [
        "# chganges label from string to integer/index\r\n",
        "def encode_label(Y_str):\r\n",
        "    labels_d = make_value2index(np.unique(Y_str))\r\n",
        "    Y = [labels_d[y_str] for y_str  in Y_str]\r\n",
        "    Y = np.array(Y)\r\n",
        "    return np.array(Y)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lW4i5wbIrCfT"
      },
      "source": [
        "def get_dataframe_ofType(df, traffic_type):\r\n",
        "    \"\"\"\r\n",
        "    Analyze traffic distribution of pandas data frame containing IDS2017 CSV\r\n",
        "    file with labelled traffic\r\n",
        "\r\n",
        "    Parameter\r\n",
        "    ---------\r\n",
        "    df: DataFrame\r\n",
        "        Pandas DataFrame corresponding to the content of a CSV file\r\n",
        "    traffic_type: string\r\n",
        "        name corresponding to traffic type\r\n",
        "\r\n",
        "    Return\r\n",
        "    ------\r\n",
        "    req_df: DataFrame\r\n",
        "        Pandas DataFrame containing only the requested traffic type\r\n",
        "    \"\"\"\r\n",
        "    req_df = df.loc[df['Label'] == traffic_type]\r\n",
        "    # don't keep original indexes\r\n",
        "    #req_df = req_df.reset_index()\r\n",
        "    return req_df"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "crMkU4GxphLu"
      },
      "source": [
        "def get_typelist(df):\r\n",
        "    \"\"\"\r\n",
        "    Extract traffic type from a pandas data frame containing IDS2017 CSV\r\n",
        "    file with labelled traffic\r\n",
        "\r\n",
        "    Parameter\r\n",
        "    ---------\r\n",
        "    df: DataFrame\r\n",
        "        Pandas DataFrame corresponding to the content of a CSV file\r\n",
        "\r\n",
        "    Return\r\n",
        "    ------\r\n",
        "    traffic_type_list: list\r\n",
        "        List of traffic types contained in the DataFrame\r\n",
        "    \"\"\"\r\n",
        "    traffic_type_list = df['Label'].value_counts().index.tolist()\r\n",
        "    return traffic_type_list"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yyxQXnv_uJok"
      },
      "source": [
        "#We balance data as follows:\r\n",
        "#1) oversample small classes so that their population/count is equal to mean_number_of_samples_per_class\r\n",
        "#2) undersample large classes so that their count is equal to mean_number_of_samples_per_class\r\n",
        "def balance_data(X,y,seed, mean_samples):\r\n",
        "    np.random.seed(seed)\r\n",
        "    unique,counts = np.unique(y,return_counts=True)\r\n",
        "    mean_samples_per_class = mean_samples # int(round(np.mean(counts)))\r\n",
        "    N,D = X.shape #(number of examples, number of features)\r\n",
        "    new_X = np.empty((0,D)) \r\n",
        "    new_y = np.empty((0),dtype=int)\r\n",
        "    for i,c in enumerate(unique):\r\n",
        "        temp_x = X[y==c]\r\n",
        "        indices = np.random.choice(temp_x.shape[0],mean_samples_per_class) # gets `mean_samples_per_class` indices of class `c`\r\n",
        "        new_X = np.concatenate((new_X,temp_x[indices]),axis=0) # now we put new data into new_X \r\n",
        "        temp_y = np.ones(mean_samples_per_class,dtype=int)*c\r\n",
        "        new_y = np.concatenate((new_y,temp_y),axis=0)\r\n",
        "        \r\n",
        "    # in order to break class order in data we need shuffling\r\n",
        "    indices = np.arange(new_y.shape[0])\r\n",
        "    np.random.shuffle(indices)\r\n",
        "    new_X =  new_X[indices,:]\r\n",
        "    new_y = new_y[indices]\r\n",
        "    return (new_X,new_y)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T-kNtwdKymAz"
      },
      "source": [
        "## Step 1. Read cleaned CICIDS2017 dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Is2az0Giy0iC"
      },
      "source": [
        "Connect to Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p9zJ-0oQy1Gy",
        "outputId": "293264b6-3691-45a2-c203-2c4aef618700"
      },
      "source": [
        "from google.colab import drive\r\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aL5u5kynL_B"
      },
      "source": [
        "# All columns\r\n",
        "col_names = np.array(['Source Port', 'Destination Port',\r\n",
        "                      'Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets',\r\n",
        "                      'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean',\r\n",
        "                      'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\r\n",
        "                      'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total',\r\n",
        "                      'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\r\n",
        "                      'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Length', 'Bwd Header Length',\r\n",
        "                      'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std',\r\n",
        "                      'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\r\n",
        "                      'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size',\r\n",
        "                      'Avg Bwd Segment Size','Subflow Fwd Packets', 'Subflow Fwd Bytes',\r\n",
        "                      'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward',\r\n",
        "                      'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\r\n",
        "                      'Idle Std', 'Idle Max', 'Idle Min', 'Label'])"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rC6XqERIc9c4"
      },
      "source": [
        "According to \"**Selection and Performance Analysis of CICIDS2017 Features Importance**\", the important features are: *Destination Port, Fwd IAT Min, Init_Win_bytes_Forward, Init_Win_bytes_backward* and *FlowIATMin*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSfAZhiPCwpu"
      },
      "source": [
        "col_important = np.array(['Source Port', 'Destination Port', 'Fwd IAT Min', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward', 'Flow IAT Min',\r\n",
        "                      'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets',\r\n",
        "                      'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Protocol', 'Fwd Packet Length Min', 'Fwd Packet Length Mean',\r\n",
        "                      'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\r\n",
        "                      'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Fwd IAT Total',\r\n",
        "                      'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\r\n",
        "                      'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Length', 'Bwd Header Length',\r\n",
        "                      'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std',\r\n",
        "                      'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\r\n",
        "                      'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size',\r\n",
        "                      'Avg Bwd Segment Size','Subflow Fwd Packets', 'Subflow Fwd Bytes',\r\n",
        "                      'Subflow Bwd Packets', 'Subflow Bwd Bytes', \r\n",
        "                      'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\r\n",
        "                      'Idle Std', 'Idle Max', 'Idle Min', 'Label'])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E17GpxaNymAz",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 434
        },
        "outputId": "60c9cca4-6c80-429c-e031-a88f845a050b"
      },
      "source": [
        "# load train data\n",
        "df_train = pd.read_csv('/content/drive/My Drive/CICIDS2017/train_set.csv',names=col_names, skiprows=1)  \n",
        "#df_train = pd.read_csv('/content/drive/My Drive/CICIDS2017/train_set.csv',names=col_important, skiprows=1) \n",
        "df_train.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th>Source Port</th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>5141</th>\n",
              "      <th>172.16.0.1-192.168.10.50-50294-80-6</th>\n",
              "      <th>172.16.0.1</th>\n",
              "      <th>50294.0</th>\n",
              "      <th>192.168.10.50</th>\n",
              "      <th>80.0</th>\n",
              "      <th>6.0</th>\n",
              "      <th>5/7/2017 10:33</th>\n",
              "      <th>63101744.0</th>\n",
              "      <th>7.0</th>\n",
              "      <th>0.0</th>\n",
              "      <th>0.0</th>\n",
              "      <th>0.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.110932</td>\n",
              "      <td>10500000.0</td>\n",
              "      <td>1.190000e+07</td>\n",
              "      <td>32100000.0</td>\n",
              "      <td>998158.0</td>\n",
              "      <td>63100000.0</td>\n",
              "      <td>1.050000e+07</td>\n",
              "      <td>1.190000e+07</td>\n",
              "      <td>32100000.0</td>\n",
              "      <td>998158.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>280.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.110932</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>280.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>29200.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>40.0</td>\n",
              "      <td>7006133.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7006133.0</td>\n",
              "      <td>7006133.0</td>\n",
              "      <td>18700000.0</td>\n",
              "      <td>12200000.0</td>\n",
              "      <td>32100000.0</td>\n",
              "      <td>8015895.0</td>\n",
              "      <td>DoS Slowhttptest</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40963</th>\n",
              "      <th>172.16.0.1-192.168.10.50-37796-1199-6</th>\n",
              "      <th>172.16.0.1</th>\n",
              "      <th>37796.0</th>\n",
              "      <th>192.168.10.50</th>\n",
              "      <th>1199.0</th>\n",
              "      <th>6.0</th>\n",
              "      <th>7/7/2017 2:52</th>\n",
              "      <th>62.0</th>\n",
              "      <th>1.0</th>\n",
              "      <th>1.0</th>\n",
              "      <th>2.0</th>\n",
              "      <th>6.0</th>\n",
              "      <th>2.0</th>\n",
              "      <td>2.0</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.290323e+05</td>\n",
              "      <td>32258.064520</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>62.0</td>\n",
              "      <td>62.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>16129.032260</td>\n",
              "      <td>16129.032260</td>\n",
              "      <td>2.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>3.333333</td>\n",
              "      <td>2.309401</td>\n",
              "      <td>5.333333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>5.000000</td>\n",
              "      <td>2.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1024.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>24.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>PortScan</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27718</th>\n",
              "      <th>172.217.6.200-192.168.10.12-443-42634-6</th>\n",
              "      <th>172.217.6.200</th>\n",
              "      <th>443.0</th>\n",
              "      <th>192.168.10.12</th>\n",
              "      <th>42634.0</th>\n",
              "      <th>6.0</th>\n",
              "      <th>03/07/2017 09:49:12</th>\n",
              "      <th>3.0</th>\n",
              "      <th>2.0</th>\n",
              "      <th>0.0</th>\n",
              "      <th>0.0</th>\n",
              "      <th>0.0</th>\n",
              "      <th>0.0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>666666.666667</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>3.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>666666.666667</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>64.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>357.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>32.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>106492</th>\n",
              "      <th>192.168.10.8-23.208.79.206-52235-443-6</th>\n",
              "      <th>192.168.10.8</th>\n",
              "      <th>52235.0</th>\n",
              "      <th>23.208.79.206</th>\n",
              "      <th>443.0</th>\n",
              "      <th>6.0</th>\n",
              "      <th>4/7/2017 11:46</th>\n",
              "      <th>5007496.0</th>\n",
              "      <th>7.0</th>\n",
              "      <th>4.0</th>\n",
              "      <th>1679.0</th>\n",
              "      <th>152.0</th>\n",
              "      <th>1080.0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>239.857143</td>\n",
              "      <td>415.237052</td>\n",
              "      <td>152.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>38.0</td>\n",
              "      <td>76.0</td>\n",
              "      <td>3.656518e+02</td>\n",
              "      <td>2.196707</td>\n",
              "      <td>500749.6</td>\n",
              "      <td>1.543257e+06</td>\n",
              "      <td>4892570.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>5007496.0</td>\n",
              "      <td>8.345827e+05</td>\n",
              "      <td>2.018795e+06</td>\n",
              "      <td>4955369.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>87090.0</td>\n",
              "      <td>29030.0</td>\n",
              "      <td>31709.63089</td>\n",
              "      <td>63179.0</td>\n",
              "      <td>515.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>92.0</td>\n",
              "      <td>1.397904</td>\n",
              "      <td>0.798802</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1080.0</td>\n",
              "      <td>152.583333</td>\n",
              "      <td>327.660428</td>\n",
              "      <td>107361.356100</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>166.454545</td>\n",
              "      <td>239.857143</td>\n",
              "      <td>38.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>1679.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>8192.0</td>\n",
              "      <td>946.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>63559</th>\n",
              "      <th>192.168.10.14-52.84.30.203-59835-80-6</th>\n",
              "      <th>52.84.30.203</th>\n",
              "      <th>80.0</th>\n",
              "      <th>192.168.10.14</th>\n",
              "      <th>59835.0</th>\n",
              "      <th>6.0</th>\n",
              "      <th>6/7/2017 10:04</th>\n",
              "      <th>4.0</th>\n",
              "      <th>1.0</th>\n",
              "      <th>1.0</th>\n",
              "      <th>6.0</th>\n",
              "      <th>6.0</th>\n",
              "      <th>6.0</th>\n",
              "      <td>6.0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>3.000000e+06</td>\n",
              "      <td>500000.000000</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>4.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>250000.000000</td>\n",
              "      <td>250000.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>9.000000</td>\n",
              "      <td>6.000000</td>\n",
              "      <td>6.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>6.0</td>\n",
              "      <td>115.0</td>\n",
              "      <td>256.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>BENIGN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                                                                                                                           Source Port  ...             Label\n",
              "5141   172.16.0.1-192.168.10.50-50294-80-6     172.16.0.1    50294.0 192.168.10.50 80.0    6.0 5/7/2017 10:33      63101744.0 7.0 0.0 0.0    0.0   0.0             0.0  ...  DoS Slowhttptest\n",
              "40963  172.16.0.1-192.168.10.50-37796-1199-6   172.16.0.1    37796.0 192.168.10.50 1199.0  6.0 7/7/2017 2:52       62.0       1.0 1.0 2.0    6.0   2.0             2.0  ...          PortScan\n",
              "27718  172.217.6.200-192.168.10.12-443-42634-6 172.217.6.200 443.0   192.168.10.12 42634.0 6.0 03/07/2017 09:49:12 3.0        2.0 0.0 0.0    0.0   0.0             0.0  ...            BENIGN\n",
              "106492 192.168.10.8-23.208.79.206-52235-443-6  192.168.10.8  52235.0 23.208.79.206 443.0   6.0 4/7/2017 11:46      5007496.0  7.0 4.0 1679.0 152.0 1080.0          0.0  ...            BENIGN\n",
              "63559  192.168.10.14-52.84.30.203-59835-80-6   52.84.30.203  80.0    192.168.10.14 59835.0 6.0 6/7/2017 10:04      4.0        1.0 1.0 6.0    6.0   6.0             6.0  ...            BENIGN\n",
              "\n",
              "[5 rows x 72 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXMkWoEUymA0",
        "outputId": "15f5010c-7123-400a-ddd5-492cd7955121"
      },
      "source": [
        "df_test = pd.read_csv('/content/drive/My Drive/CICIDS2017/test_set.csv',names=col_names, skiprows=1)  \n",
        "#df_test = pd.read_csv('/content/drive/My Drive/CICIDS2017/test_set.csv',names=col_important, skiprows=1) \n",
        "print('Test set size: ', df_test.shape)\n",
        "\n",
        "df_val = pd.read_csv('/content/drive/My Drive/CICIDS2017/crossval_set.csv',names=col_names, skiprows=1)  \n",
        "#df_val = pd.read_csv('/content/drive/My Drive/CICIDS2017/crossval_set.csv',names=col_important, skiprows=1) \n",
        "print('Validation set size: ', df_val.shape)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set size:  (278270, 72)\n",
            "Validation set size:  (278270, 72)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cPLtWtJ8ymA1",
        "outputId": "4d715c6f-cf31-4407-e76f-699eaa9606ad"
      },
      "source": [
        "# Here we can see the number of rows and columns for each table.\n",
        "print(df_train.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(556548, 72)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VVGdQOpFnhMT"
      },
      "source": [
        "Count the number of attacks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZK5kP4X3ngMA",
        "outputId": "80e3233f-11de-4d35-aeae-4a6242fe496e"
      },
      "source": [
        "df_train['Label'].value_counts()"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        278274\n",
              "DoS Hulk                      115062\n",
              "PortScan                       79402\n",
              "DDoS                           64012\n",
              "DoS GoldenEye                   5146\n",
              "FTP-Patator                     3967\n",
              "SSH-Patator                     2948\n",
              "DoS slowloris                   2898\n",
              "DoS Slowhttptest                2749\n",
              "Bot                              978\n",
              "Web Attack  Brute Force         753\n",
              "Web Attack  XSS                 326\n",
              "Infiltration                      18\n",
              "Web Attack  Sql Injection        10\n",
              "Heartbleed                         5\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kCjcgUmlnr0T",
        "outputId": "6abe84b1-9ea7-4705-eb3f-36a09236cb20"
      },
      "source": [
        "print('Test set: ')\r\n",
        "df_test['Label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test set: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        139135\n",
              "DoS Hulk                       57531\n",
              "PortScan                       39701\n",
              "DDoS                           32006\n",
              "DoS GoldenEye                   2573\n",
              "FTP-Patator                     1983\n",
              "SSH-Patator                     1474\n",
              "DoS slowloris                   1449\n",
              "DoS Slowhttptest                1374\n",
              "Bot                              489\n",
              "Web Attack  Brute Force         376\n",
              "Web Attack  XSS                 163\n",
              "Infiltration                       9\n",
              "Web Attack  Sql Injection         5\n",
              "Heartbleed                         2\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1sIo9YzUnuY5",
        "outputId": "a0e4c697-ce43-4f85-d253-2bdb3fa87d2f"
      },
      "source": [
        "print('Validation set: ')\r\n",
        "df_val['Label'].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Validation set: \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BENIGN                        139135\n",
              "DoS Hulk                       57531\n",
              "PortScan                       39701\n",
              "DDoS                           32006\n",
              "DoS GoldenEye                   2573\n",
              "FTP-Patator                     1983\n",
              "SSH-Patator                     1474\n",
              "DoS slowloris                   1449\n",
              "DoS Slowhttptest                1374\n",
              "Bot                              489\n",
              "Web Attack  Brute Force         376\n",
              "Web Attack  XSS                 163\n",
              "Infiltration                       9\n",
              "Web Attack  Sql Injection         5\n",
              "Heartbleed                         2\n",
              "Name: Label, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzHvvoMDotTj"
      },
      "source": [
        "## Step 2. Randomly Selecting 2000 examples from each type"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "asjXEVY7yrXa"
      },
      "source": [
        "First, selecting 4000 examples for each type in train dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nW88xIvao0GO"
      },
      "source": [
        "df_label = df_train['Label']\r\n",
        "data = df_train.drop(columns=['Label'])\r\n",
        "X = data.values\r\n",
        "y = encode_label(df_label.values)"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m32yYQ-prx45",
        "outputId": "cc583d87-33f0-4ead-e989-f73c0178ccd1"
      },
      "source": [
        "print(X.shape)\r\n",
        "print(y.shape)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(556548, 71)\n",
            "(556548,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jo9psJWmx7y7",
        "outputId": "b67fab28-cdab-4e14-d988-4db5932b8f3d"
      },
      "source": [
        "unique, counts = np.unique(y, return_counts=True)\r\n",
        "print(unique, counts)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14] [278274    978  64012   5146 115062   2749   2898   3967      5     18\n",
            "  79402   2948    753     10    326]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RDTCXiY-qtFZ"
      },
      "source": [
        "SEED = 2\r\n",
        "X_train,y_train = balance_data(X,y,seed=SEED, mean_samples=4000)"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks1hpGiFyRi_",
        "outputId": "06a8cdda-be4d-4166-b740-f76208075e4e"
      },
      "source": [
        "print(X_train.shape)\r\n",
        "print(y_train.shape)\r\n",
        "unique, counts = np.unique(y_train, return_counts=True)\r\n",
        "print(unique, counts)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 71)\n",
            "(60000,)\n",
            "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14] [4000 4000 4000 4000 4000 4000 4000 4000 4000 4000 4000 4000 4000 4000\n",
            " 4000]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gua98tTqypEx"
      },
      "source": [
        "Next, selecting 1000 examples from validation datesets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DbW1x3UzCzT"
      },
      "source": [
        "df_label = df_test['Label']\r\n",
        "data = df_test.drop(columns=['Label'])\r\n",
        "X = data.values\r\n",
        "y = encode_label(df_label.values)\r\n",
        "\r\n",
        "SEED = 2\r\n",
        "X_test,y_test = balance_data(X,y,seed=SEED, mean_samples=1000)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SLLES1n7zjAC"
      },
      "source": [
        "Next, selecting 500 examples from test datesets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v7VxrFq3zQ8M"
      },
      "source": [
        "df_label = df_val['Label']\r\n",
        "data = df_val.drop(columns=['Label'])\r\n",
        "X = data.values\r\n",
        "y = encode_label(df_label.values)\r\n",
        "\r\n",
        "SEED = 2\r\n",
        "X_val,y_val = balance_data(X,y,seed=SEED, mean_samples=500)"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BtjNVDHfymA1"
      },
      "source": [
        "## Step 3. Normalization\n",
        "\n",
        "The continuous feature values are normalized into the same feature space. This is important when using features that have different measurements, and is a general requirement of many machine learning algorithms. Therefore, the values for this dataset are also normalized using the Min-Max scaling technique, bringing them all within a range of [0,1]."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "euTfYdWHymA2"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S5JH_i4EymA2",
        "outputId": "c2b3960e-1784-431b-ff3a-e88646613511"
      },
      "source": [
        "scaler = MinMaxScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_train"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.00109412, 0.00130398, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.0077991 , 0.0197178 , ..., 0.        , 0.05733978,\n",
              "        0.05733978]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_PuA6ri3ymA2",
        "outputId": "cbe84d19-cfc6-4c71-f407-d46deb22aaec"
      },
      "source": [
        "X_train.shape"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(60000, 71)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ry0R9AFFymA3"
      },
      "source": [
        "X_test = scaler.fit_transform(X_test)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeFIseICymA3",
        "outputId": "974646e5-711c-4999-bb82-f357810a0fc9"
      },
      "source": [
        "X_test.shape"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(15000, 71)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1mN1ZAI0gcL",
        "outputId": "7d0da227-696e-4d72-e558-aad4b79122f0"
      },
      "source": [
        "X_val = scaler.fit_transform(X_val)\r\n",
        "X_val"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0.        , 0.        , 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.00095839, 0.02132314, ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.00100857, 0.00039029, 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       ...,\n",
              "       [0.        , 0.01865954, 0.0270547 , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.00302572, 0.00117087, 0.        , ..., 0.        , 0.        ,\n",
              "        0.        ],\n",
              "       [0.        , 0.02337832, 0.05175073, ..., 0.        , 0.        ,\n",
              "        0.        ]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PGN8Mww10uvV"
      },
      "source": [
        "## Step 4. One-hot encoding for labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XayluHqdymA3"
      },
      "source": [
        "y_train and y_test have to be one-hot-encoded. That means they must have dimension (number_of_samples, 15), where 15 denotes number of classes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1pMxFRGCymA4"
      },
      "source": [
        "from tensorflow.keras.utils import to_categorical"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Na5t3wpmymA4",
        "outputId": "ae9a956b-c333-49ca-ce5a-6750be37fd4f"
      },
      "source": [
        "y_train_v = to_categorical(y_train, 15)\n",
        "y_test_v = to_categorical(y_test, 15)\n",
        "y_val_v = to_categorical(y_val, 15)\n",
        "print(y_train_v.shape)\n",
        "print(y_test_v.shape)\n",
        "print(y_val_v.shape)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 15)\n",
            "(15000, 15)\n",
            "(7500, 15)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79Q2ezxEymA6"
      },
      "source": [
        "## Step 5. Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYdmYYyWymA6"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Conv1D, BatchNormalization, MaxPooling1D, Flatten, Dense, Activation,Dropout\n",
        "from tensorflow.keras.constraints import max_norm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VinchNXyymA7",
        "outputId": "e8330276-212e-423a-a83c-2acca4851fbf"
      },
      "source": [
        "#hyper-params\n",
        "batch_size = 100 # increasing batch size with more gpu added\n",
        "\n",
        "input_dim = X_train.shape[1]\n",
        "num_class = 15                   # 15 intrusion classes, including benign traffic class\n",
        "num_epochs = 90\n",
        "\n",
        "print(input_dim)\n",
        "print(num_class)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "71\n",
            "15\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SeAQGIoymA8",
        "outputId": "606356e3-a50a-4066-c2e4-f646da8aab90"
      },
      "source": [
        "X_train_r = np.zeros((len(X_train), input_dim, 1))\n",
        "X_train_r[:, :, 0] = X_train[:, :input_dim]\n",
        "print(X_train_r.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 71, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mlcgM-4311Ee",
        "outputId": "060993ac-8d09-4413-c430-6eb1a79268e9"
      },
      "source": [
        "X_val_r = np.zeros((len(X_val), input_dim, 1))\r\n",
        "X_val_r[:, :, 0] = X_val[:, :input_dim]\r\n",
        "print(X_val_r.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7500, 71, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B96MRlI_ymA8",
        "outputId": "4ee1bc49-72af-4fd8-ef47-bc57d03306ef"
      },
      "source": [
        "X_test_r = np.zeros((len(X_test), input_dim, 1))\n",
        "X_test_r[:, :, 0] = X_test[:, :input_dim]\n",
        "print(X_test_r.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(15000, 71, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKMiVwhCXx2I"
      },
      "source": [
        "**Model with 2 Con1D layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LJqaGS9SX01m",
        "outputId": "ab5a98dd-ff10-4fef-c48e-9b05f971be0f"
      },
      "source": [
        "model2 = Sequential()\r\n",
        "\r\n",
        "# input layer\r\n",
        "model2.add(Conv1D(filters=60, kernel_size=11,  input_shape=(71,1)))\r\n",
        "#model2.add(BatchNormalization(axis=1))\r\n",
        "model2.add(Activation('relu'))\r\n",
        "model2.add(Dropout(0.1))\r\n",
        "\r\n",
        "model2.add(Conv1D(filters=60, kernel_size=3))\r\n",
        "#model2.add(BatchNormalization(axis=1))\r\n",
        "model2.add(Activation('relu'))\r\n",
        "model2.add(Dropout(0.1))\r\n",
        "\r\n",
        "model2.add(Conv1D(filters=60, kernel_size=7))\r\n",
        "#model2.add(BatchNormalization(axis=1))\r\n",
        "model2.add(Activation('relu'))\r\n",
        "model2.add(Dropout(0.1))\r\n",
        "\r\n",
        "model2.add(Flatten())\r\n",
        "#model2.add(Dropout(0.1))\r\n",
        "model2.add(Dense(128, activation='relu'))\r\n",
        "model2.add(Dense(num_class))\r\n",
        "model2.add(Activation('softmax'))\r\n",
        "\r\n",
        "model2.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d (Conv1D)              (None, 61, 60)            720       \n",
            "_________________________________________________________________\n",
            "activation (Activation)      (None, 61, 60)            0         \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 61, 60)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_1 (Conv1D)            (None, 59, 60)            10860     \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 59, 60)            0         \n",
            "_________________________________________________________________\n",
            "dropout_1 (Dropout)          (None, 59, 60)            0         \n",
            "_________________________________________________________________\n",
            "conv1d_2 (Conv1D)            (None, 53, 60)            25260     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 53, 60)            0         \n",
            "_________________________________________________________________\n",
            "dropout_2 (Dropout)          (None, 53, 60)            0         \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3180)              0         \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 128)               407168    \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 15)                1935      \n",
            "_________________________________________________________________\n",
            "activation_3 (Activation)    (None, 15)                0         \n",
            "=================================================================\n",
            "Total params: 445,943\n",
            "Trainable params: 445,943\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2u3i2MkUYNo3"
      },
      "source": [
        "learning_rates = 1e-3\r\n",
        "optim = tf.keras.optimizers.Adam(lr=learning_rates, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\r\n",
        "model2.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5NGP29aXYPyw",
        "outputId": "7858f36e-9314-4174-d5a5-5d194f312ddc"
      },
      "source": [
        "model2.fit(X_train_r, y_train_v, epochs=100, batch_size=batch_size, validation_data=(X_val_r, y_val_v), verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "600/600 [==============================] - 10s 4ms/step - loss: 0.9566 - accuracy: 0.6844 - val_loss: 0.3925 - val_accuracy: 0.8560\n",
            "Epoch 2/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.2993 - accuracy: 0.8746 - val_loss: 0.3710 - val_accuracy: 0.8488\n",
            "Epoch 3/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.2609 - accuracy: 0.8799 - val_loss: 0.3645 - val_accuracy: 0.8527\n",
            "Epoch 4/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.2427 - accuracy: 0.8864 - val_loss: 0.3273 - val_accuracy: 0.8615\n",
            "Epoch 5/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.2304 - accuracy: 0.8865 - val_loss: 0.3215 - val_accuracy: 0.8669\n",
            "Epoch 6/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.2244 - accuracy: 0.8906 - val_loss: 0.3465 - val_accuracy: 0.8791\n",
            "Epoch 7/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.2147 - accuracy: 0.8942 - val_loss: 0.3312 - val_accuracy: 0.8579\n",
            "Epoch 8/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.2043 - accuracy: 0.8978 - val_loss: 0.3430 - val_accuracy: 0.8565\n",
            "Epoch 9/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1987 - accuracy: 0.9003 - val_loss: 0.3313 - val_accuracy: 0.8781\n",
            "Epoch 10/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1930 - accuracy: 0.8967 - val_loss: 0.3100 - val_accuracy: 0.8604\n",
            "Epoch 11/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1920 - accuracy: 0.9000 - val_loss: 0.3725 - val_accuracy: 0.8601\n",
            "Epoch 12/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1903 - accuracy: 0.9000 - val_loss: 0.3704 - val_accuracy: 0.8720\n",
            "Epoch 13/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1905 - accuracy: 0.8999 - val_loss: 0.4024 - val_accuracy: 0.8699\n",
            "Epoch 14/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1847 - accuracy: 0.9021 - val_loss: 0.4170 - val_accuracy: 0.8625\n",
            "Epoch 15/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1838 - accuracy: 0.9015 - val_loss: 0.3662 - val_accuracy: 0.8812\n",
            "Epoch 16/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1839 - accuracy: 0.9002 - val_loss: 0.4223 - val_accuracy: 0.8795\n",
            "Epoch 17/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1817 - accuracy: 0.9038 - val_loss: 0.4045 - val_accuracy: 0.8701\n",
            "Epoch 18/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1811 - accuracy: 0.9023 - val_loss: 0.3927 - val_accuracy: 0.8700\n",
            "Epoch 19/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1750 - accuracy: 0.9066 - val_loss: 0.4338 - val_accuracy: 0.8735\n",
            "Epoch 20/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1746 - accuracy: 0.9065 - val_loss: 0.4291 - val_accuracy: 0.8703\n",
            "Epoch 21/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1765 - accuracy: 0.9055 - val_loss: 0.3819 - val_accuracy: 0.8777\n",
            "Epoch 22/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1726 - accuracy: 0.9073 - val_loss: 0.4700 - val_accuracy: 0.8805\n",
            "Epoch 23/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1755 - accuracy: 0.9048 - val_loss: 0.4620 - val_accuracy: 0.8707\n",
            "Epoch 24/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1750 - accuracy: 0.9054 - val_loss: 0.4337 - val_accuracy: 0.8624\n",
            "Epoch 25/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1701 - accuracy: 0.9105 - val_loss: 0.3926 - val_accuracy: 0.8735\n",
            "Epoch 26/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1738 - accuracy: 0.9076 - val_loss: 0.4228 - val_accuracy: 0.8725\n",
            "Epoch 27/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1717 - accuracy: 0.9074 - val_loss: 0.4564 - val_accuracy: 0.8640\n",
            "Epoch 28/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1694 - accuracy: 0.9134 - val_loss: 0.3712 - val_accuracy: 0.8739\n",
            "Epoch 29/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1698 - accuracy: 0.9072 - val_loss: 0.4643 - val_accuracy: 0.8747\n",
            "Epoch 30/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1666 - accuracy: 0.9122 - val_loss: 0.4803 - val_accuracy: 0.8683\n",
            "Epoch 31/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1630 - accuracy: 0.9126 - val_loss: 0.4179 - val_accuracy: 0.8637\n",
            "Epoch 32/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1671 - accuracy: 0.9130 - val_loss: 0.5520 - val_accuracy: 0.8731\n",
            "Epoch 33/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1636 - accuracy: 0.9141 - val_loss: 0.4773 - val_accuracy: 0.8797\n",
            "Epoch 34/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1636 - accuracy: 0.9145 - val_loss: 0.5031 - val_accuracy: 0.8589\n",
            "Epoch 35/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1608 - accuracy: 0.9140 - val_loss: 0.4917 - val_accuracy: 0.8741\n",
            "Epoch 36/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1608 - accuracy: 0.9153 - val_loss: 0.5210 - val_accuracy: 0.8736\n",
            "Epoch 37/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1608 - accuracy: 0.9165 - val_loss: 0.4987 - val_accuracy: 0.8871\n",
            "Epoch 38/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1565 - accuracy: 0.9151 - val_loss: 0.4952 - val_accuracy: 0.8828\n",
            "Epoch 39/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1592 - accuracy: 0.9159 - val_loss: 0.4449 - val_accuracy: 0.8729\n",
            "Epoch 40/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1573 - accuracy: 0.9168 - val_loss: 0.4256 - val_accuracy: 0.8848\n",
            "Epoch 41/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1491 - accuracy: 0.9210 - val_loss: 0.4525 - val_accuracy: 0.8852\n",
            "Epoch 42/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1525 - accuracy: 0.9169 - val_loss: 0.4501 - val_accuracy: 0.8833\n",
            "Epoch 43/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1501 - accuracy: 0.9194 - val_loss: 0.4541 - val_accuracy: 0.8916\n",
            "Epoch 44/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1468 - accuracy: 0.9207 - val_loss: 0.4526 - val_accuracy: 0.8820\n",
            "Epoch 45/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1502 - accuracy: 0.9206 - val_loss: 0.4726 - val_accuracy: 0.8760\n",
            "Epoch 46/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1485 - accuracy: 0.9214 - val_loss: 0.5251 - val_accuracy: 0.8800\n",
            "Epoch 47/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1459 - accuracy: 0.9205 - val_loss: 0.5104 - val_accuracy: 0.8823\n",
            "Epoch 48/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1453 - accuracy: 0.9221 - val_loss: 0.4918 - val_accuracy: 0.8668\n",
            "Epoch 49/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1447 - accuracy: 0.9203 - val_loss: 0.4660 - val_accuracy: 0.8796\n",
            "Epoch 50/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1425 - accuracy: 0.9239 - val_loss: 0.4907 - val_accuracy: 0.8728\n",
            "Epoch 51/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1502 - accuracy: 0.9204 - val_loss: 0.4611 - val_accuracy: 0.8773\n",
            "Epoch 52/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1415 - accuracy: 0.9230 - val_loss: 0.5041 - val_accuracy: 0.8799\n",
            "Epoch 53/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1422 - accuracy: 0.9259 - val_loss: 0.4665 - val_accuracy: 0.8783\n",
            "Epoch 54/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1409 - accuracy: 0.9248 - val_loss: 0.4802 - val_accuracy: 0.8780\n",
            "Epoch 55/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1349 - accuracy: 0.9261 - val_loss: 0.5225 - val_accuracy: 0.8785\n",
            "Epoch 56/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1356 - accuracy: 0.9263 - val_loss: 0.4566 - val_accuracy: 0.9007\n",
            "Epoch 57/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1345 - accuracy: 0.9277 - val_loss: 0.4714 - val_accuracy: 0.8768\n",
            "Epoch 58/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1364 - accuracy: 0.9288 - val_loss: 0.4861 - val_accuracy: 0.9071\n",
            "Epoch 59/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1342 - accuracy: 0.9284 - val_loss: 0.4777 - val_accuracy: 0.8799\n",
            "Epoch 60/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1342 - accuracy: 0.9283 - val_loss: 0.4863 - val_accuracy: 0.8832\n",
            "Epoch 61/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1358 - accuracy: 0.9246 - val_loss: 0.4600 - val_accuracy: 0.8780\n",
            "Epoch 62/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1367 - accuracy: 0.9267 - val_loss: 0.4577 - val_accuracy: 0.8783\n",
            "Epoch 63/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1393 - accuracy: 0.9255 - val_loss: 0.4951 - val_accuracy: 0.8887\n",
            "Epoch 64/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1337 - accuracy: 0.9282 - val_loss: 0.4588 - val_accuracy: 0.8809\n",
            "Epoch 65/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1359 - accuracy: 0.9278 - val_loss: 0.4753 - val_accuracy: 0.9036\n",
            "Epoch 66/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1286 - accuracy: 0.9298 - val_loss: 0.4248 - val_accuracy: 0.8783\n",
            "Epoch 67/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1286 - accuracy: 0.9305 - val_loss: 0.4939 - val_accuracy: 0.9063\n",
            "Epoch 68/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1329 - accuracy: 0.9279 - val_loss: 0.4496 - val_accuracy: 0.8912\n",
            "Epoch 69/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1295 - accuracy: 0.9300 - val_loss: 0.4340 - val_accuracy: 0.8861\n",
            "Epoch 70/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1352 - accuracy: 0.9294 - val_loss: 0.4760 - val_accuracy: 0.8988\n",
            "Epoch 71/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1305 - accuracy: 0.9308 - val_loss: 0.4442 - val_accuracy: 0.9016\n",
            "Epoch 72/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1279 - accuracy: 0.9313 - val_loss: 0.4928 - val_accuracy: 0.8865\n",
            "Epoch 73/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1277 - accuracy: 0.9329 - val_loss: 0.4374 - val_accuracy: 0.8885\n",
            "Epoch 74/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1322 - accuracy: 0.9286 - val_loss: 0.4499 - val_accuracy: 0.8873\n",
            "Epoch 75/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1309 - accuracy: 0.9308 - val_loss: 0.4088 - val_accuracy: 0.8941\n",
            "Epoch 76/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1265 - accuracy: 0.9328 - val_loss: 0.4202 - val_accuracy: 0.9060\n",
            "Epoch 77/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1287 - accuracy: 0.9325 - val_loss: 0.5266 - val_accuracy: 0.8896\n",
            "Epoch 78/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1319 - accuracy: 0.9282 - val_loss: 0.4856 - val_accuracy: 0.8944\n",
            "Epoch 79/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1316 - accuracy: 0.9315 - val_loss: 0.4803 - val_accuracy: 0.8952\n",
            "Epoch 80/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1291 - accuracy: 0.9315 - val_loss: 0.5469 - val_accuracy: 0.9101\n",
            "Epoch 81/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1248 - accuracy: 0.9336 - val_loss: 0.4762 - val_accuracy: 0.8877\n",
            "Epoch 82/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1254 - accuracy: 0.9340 - val_loss: 0.5144 - val_accuracy: 0.8887\n",
            "Epoch 83/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1283 - accuracy: 0.9298 - val_loss: 0.5689 - val_accuracy: 0.8972\n",
            "Epoch 84/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1270 - accuracy: 0.9324 - val_loss: 0.5581 - val_accuracy: 0.8936\n",
            "Epoch 85/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1235 - accuracy: 0.9338 - val_loss: 0.5211 - val_accuracy: 0.9104\n",
            "Epoch 86/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1260 - accuracy: 0.9352 - val_loss: 0.4814 - val_accuracy: 0.8971\n",
            "Epoch 87/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1246 - accuracy: 0.9349 - val_loss: 0.5600 - val_accuracy: 0.8971\n",
            "Epoch 88/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1256 - accuracy: 0.9344 - val_loss: 0.5110 - val_accuracy: 0.9076\n",
            "Epoch 89/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1268 - accuracy: 0.9340 - val_loss: 0.4904 - val_accuracy: 0.8959\n",
            "Epoch 90/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1287 - accuracy: 0.9332 - val_loss: 0.5131 - val_accuracy: 0.9076\n",
            "Epoch 91/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1191 - accuracy: 0.9372 - val_loss: 0.4261 - val_accuracy: 0.8931\n",
            "Epoch 92/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1238 - accuracy: 0.9358 - val_loss: 0.4982 - val_accuracy: 0.8885\n",
            "Epoch 93/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1274 - accuracy: 0.9333 - val_loss: 0.5430 - val_accuracy: 0.9005\n",
            "Epoch 94/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1224 - accuracy: 0.9353 - val_loss: 0.5088 - val_accuracy: 0.9069\n",
            "Epoch 95/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1241 - accuracy: 0.9344 - val_loss: 0.5843 - val_accuracy: 0.8952\n",
            "Epoch 96/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1225 - accuracy: 0.9371 - val_loss: 0.5395 - val_accuracy: 0.9095\n",
            "Epoch 97/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1213 - accuracy: 0.9365 - val_loss: 0.4639 - val_accuracy: 0.9071\n",
            "Epoch 98/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1243 - accuracy: 0.9370 - val_loss: 0.6056 - val_accuracy: 0.8965\n",
            "Epoch 99/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1179 - accuracy: 0.9401 - val_loss: 0.4929 - val_accuracy: 0.9008\n",
            "Epoch 100/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1185 - accuracy: 0.9384 - val_loss: 0.5345 - val_accuracy: 0.9089\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fd2508b1c88>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IQg2TKLVXsmo"
      },
      "source": [
        "## Step 6. **Model with DNN layers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dhvdIfDwymA9",
        "outputId": "9448bc48-77c9-48e1-decf-578c2879cec8"
      },
      "source": [
        "model = Sequential()\n",
        "\n",
        "# input layer\n",
        "model.add(Dense(128,  activation='relu', input_shape = (71,)))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "# hidden layer\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "# hidden layer\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dropout(0.1))\n",
        "\n",
        "model.add(Flatten()) \n",
        "model.add(Dense(num_class))\n",
        "model.add(Activation('softmax'))\n",
        "\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "dense_4 (Dense)              (None, 128)               9216      \n",
            "_________________________________________________________________\n",
            "dropout_3 (Dropout)          (None, 128)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 256)               33024     \n",
            "_________________________________________________________________\n",
            "dropout_4 (Dropout)          (None, 256)               0         \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 64)                16448     \n",
            "_________________________________________________________________\n",
            "dropout_5 (Dropout)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          (None, 64)                0         \n",
            "_________________________________________________________________\n",
            "dense_7 (Dense)              (None, 15)                975       \n",
            "_________________________________________________________________\n",
            "activation_1 (Activation)    (None, 15)                0         \n",
            "=================================================================\n",
            "Total params: 59,663\n",
            "Trainable params: 59,663\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UHcoSDdxymA-"
      },
      "source": [
        "learning_rates = 1e-4\r\n",
        "optim = tf.keras.optimizers.Adam(lr=learning_rates, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\r\n",
        "model.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7_dq7oCbymA_"
      },
      "source": [
        "## Step 7. Training the DNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZuWGG2dNymBA",
        "outputId": "c718ea73-1a72-4717-ad51-d3b90c5e3b98"
      },
      "source": [
        "# fit network\n",
        "model.fit(X_train, y_train_v, epochs=100, batch_size=batch_size, validation_data=(X_val, y_val_v), verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1337 - accuracy: 0.9391 - val_loss: 1.0509 - val_accuracy: 0.8891\n",
            "Epoch 2/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1341 - accuracy: 0.9387 - val_loss: 1.0689 - val_accuracy: 0.8905\n",
            "Epoch 3/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1357 - accuracy: 0.9385 - val_loss: 1.0054 - val_accuracy: 0.8895\n",
            "Epoch 4/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1346 - accuracy: 0.9396 - val_loss: 0.9986 - val_accuracy: 0.8893\n",
            "Epoch 5/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1344 - accuracy: 0.9389 - val_loss: 1.0265 - val_accuracy: 0.8912\n",
            "Epoch 6/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1344 - accuracy: 0.9390 - val_loss: 0.9785 - val_accuracy: 0.8892\n",
            "Epoch 7/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1349 - accuracy: 0.9390 - val_loss: 0.9900 - val_accuracy: 0.8903\n",
            "Epoch 8/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1339 - accuracy: 0.9392 - val_loss: 1.0127 - val_accuracy: 0.8897\n",
            "Epoch 9/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1328 - accuracy: 0.9396 - val_loss: 1.0093 - val_accuracy: 0.8895\n",
            "Epoch 10/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1337 - accuracy: 0.9392 - val_loss: 1.0354 - val_accuracy: 0.8907\n",
            "Epoch 11/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1334 - accuracy: 0.9397 - val_loss: 1.0179 - val_accuracy: 0.8901\n",
            "Epoch 12/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1345 - accuracy: 0.9391 - val_loss: 0.9966 - val_accuracy: 0.8911\n",
            "Epoch 13/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1341 - accuracy: 0.9393 - val_loss: 1.0156 - val_accuracy: 0.8892\n",
            "Epoch 14/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1339 - accuracy: 0.9392 - val_loss: 1.0039 - val_accuracy: 0.8899\n",
            "Epoch 15/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1334 - accuracy: 0.9391 - val_loss: 1.0699 - val_accuracy: 0.8897\n",
            "Epoch 16/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1335 - accuracy: 0.9394 - val_loss: 1.0719 - val_accuracy: 0.8903\n",
            "Epoch 17/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1338 - accuracy: 0.9395 - val_loss: 1.0420 - val_accuracy: 0.8884\n",
            "Epoch 18/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1339 - accuracy: 0.9396 - val_loss: 1.0489 - val_accuracy: 0.8899\n",
            "Epoch 19/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1327 - accuracy: 0.9394 - val_loss: 1.0560 - val_accuracy: 0.8900\n",
            "Epoch 20/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1332 - accuracy: 0.9396 - val_loss: 1.0339 - val_accuracy: 0.8904\n",
            "Epoch 21/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1337 - accuracy: 0.9392 - val_loss: 1.0560 - val_accuracy: 0.8896\n",
            "Epoch 22/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1326 - accuracy: 0.9402 - val_loss: 1.0573 - val_accuracy: 0.8897\n",
            "Epoch 23/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1328 - accuracy: 0.9396 - val_loss: 1.0762 - val_accuracy: 0.8900\n",
            "Epoch 24/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1335 - accuracy: 0.9395 - val_loss: 1.1100 - val_accuracy: 0.8913\n",
            "Epoch 25/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1322 - accuracy: 0.9407 - val_loss: 1.0763 - val_accuracy: 0.8908\n",
            "Epoch 26/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1322 - accuracy: 0.9398 - val_loss: 1.1124 - val_accuracy: 0.8896\n",
            "Epoch 27/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1329 - accuracy: 0.9396 - val_loss: 1.1115 - val_accuracy: 0.8903\n",
            "Epoch 28/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1332 - accuracy: 0.9395 - val_loss: 1.1014 - val_accuracy: 0.8913\n",
            "Epoch 29/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1330 - accuracy: 0.9398 - val_loss: 1.0930 - val_accuracy: 0.8927\n",
            "Epoch 30/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1334 - accuracy: 0.9400 - val_loss: 1.1330 - val_accuracy: 0.8904\n",
            "Epoch 31/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1324 - accuracy: 0.9402 - val_loss: 1.1404 - val_accuracy: 0.8912\n",
            "Epoch 32/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1326 - accuracy: 0.9395 - val_loss: 1.1305 - val_accuracy: 0.8897\n",
            "Epoch 33/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1333 - accuracy: 0.9390 - val_loss: 1.0882 - val_accuracy: 0.8909\n",
            "Epoch 34/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1325 - accuracy: 0.9394 - val_loss: 1.0978 - val_accuracy: 0.8897\n",
            "Epoch 35/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1323 - accuracy: 0.9398 - val_loss: 1.1773 - val_accuracy: 0.8904\n",
            "Epoch 36/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1325 - accuracy: 0.9399 - val_loss: 1.1464 - val_accuracy: 0.8900\n",
            "Epoch 37/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1323 - accuracy: 0.9396 - val_loss: 1.1378 - val_accuracy: 0.8908\n",
            "Epoch 38/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1314 - accuracy: 0.9397 - val_loss: 1.1255 - val_accuracy: 0.8903\n",
            "Epoch 39/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1317 - accuracy: 0.9396 - val_loss: 1.1841 - val_accuracy: 0.8901\n",
            "Epoch 40/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1326 - accuracy: 0.9403 - val_loss: 1.1580 - val_accuracy: 0.8908\n",
            "Epoch 41/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1317 - accuracy: 0.9401 - val_loss: 1.1714 - val_accuracy: 0.8912\n",
            "Epoch 42/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1322 - accuracy: 0.9400 - val_loss: 1.1641 - val_accuracy: 0.8912\n",
            "Epoch 43/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1323 - accuracy: 0.9397 - val_loss: 1.1888 - val_accuracy: 0.8900\n",
            "Epoch 44/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1317 - accuracy: 0.9398 - val_loss: 1.1905 - val_accuracy: 0.8908\n",
            "Epoch 45/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1319 - accuracy: 0.9397 - val_loss: 1.2020 - val_accuracy: 0.8924\n",
            "Epoch 46/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1317 - accuracy: 0.9396 - val_loss: 1.2137 - val_accuracy: 0.8909\n",
            "Epoch 47/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1316 - accuracy: 0.9398 - val_loss: 1.1874 - val_accuracy: 0.8915\n",
            "Epoch 48/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1311 - accuracy: 0.9402 - val_loss: 1.1906 - val_accuracy: 0.8907\n",
            "Epoch 49/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1323 - accuracy: 0.9400 - val_loss: 1.2311 - val_accuracy: 0.8931\n",
            "Epoch 50/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1310 - accuracy: 0.9402 - val_loss: 1.1986 - val_accuracy: 0.8908\n",
            "Epoch 51/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1315 - accuracy: 0.9398 - val_loss: 1.1562 - val_accuracy: 0.8884\n",
            "Epoch 52/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1309 - accuracy: 0.9400 - val_loss: 1.1801 - val_accuracy: 0.8901\n",
            "Epoch 53/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1312 - accuracy: 0.9406 - val_loss: 1.1212 - val_accuracy: 0.8888\n",
            "Epoch 54/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1309 - accuracy: 0.9400 - val_loss: 1.1540 - val_accuracy: 0.8912\n",
            "Epoch 55/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1313 - accuracy: 0.9398 - val_loss: 1.1331 - val_accuracy: 0.8899\n",
            "Epoch 56/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1306 - accuracy: 0.9408 - val_loss: 1.1648 - val_accuracy: 0.8916\n",
            "Epoch 57/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1313 - accuracy: 0.9400 - val_loss: 1.0945 - val_accuracy: 0.8917\n",
            "Epoch 58/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1302 - accuracy: 0.9408 - val_loss: 1.1812 - val_accuracy: 0.8913\n",
            "Epoch 59/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1305 - accuracy: 0.9400 - val_loss: 1.1810 - val_accuracy: 0.8907\n",
            "Epoch 60/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1309 - accuracy: 0.9398 - val_loss: 1.1808 - val_accuracy: 0.8912\n",
            "Epoch 61/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1308 - accuracy: 0.9402 - val_loss: 1.1652 - val_accuracy: 0.8911\n",
            "Epoch 62/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1306 - accuracy: 0.9401 - val_loss: 1.1921 - val_accuracy: 0.8908\n",
            "Epoch 63/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1305 - accuracy: 0.9403 - val_loss: 1.1630 - val_accuracy: 0.8915\n",
            "Epoch 64/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1300 - accuracy: 0.9402 - val_loss: 1.1854 - val_accuracy: 0.8909\n",
            "Epoch 65/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1304 - accuracy: 0.9403 - val_loss: 1.1964 - val_accuracy: 0.8925\n",
            "Epoch 66/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1307 - accuracy: 0.9399 - val_loss: 1.1605 - val_accuracy: 0.8911\n",
            "Epoch 67/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1300 - accuracy: 0.9406 - val_loss: 1.1846 - val_accuracy: 0.8901\n",
            "Epoch 68/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1293 - accuracy: 0.9409 - val_loss: 1.2301 - val_accuracy: 0.8909\n",
            "Epoch 69/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1299 - accuracy: 0.9405 - val_loss: 1.2432 - val_accuracy: 0.8908\n",
            "Epoch 70/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1306 - accuracy: 0.9399 - val_loss: 1.1842 - val_accuracy: 0.8904\n",
            "Epoch 71/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1308 - accuracy: 0.9397 - val_loss: 1.1823 - val_accuracy: 0.8912\n",
            "Epoch 72/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1299 - accuracy: 0.9402 - val_loss: 1.1564 - val_accuracy: 0.8921\n",
            "Epoch 73/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1301 - accuracy: 0.9402 - val_loss: 1.1765 - val_accuracy: 0.8900\n",
            "Epoch 74/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1292 - accuracy: 0.9409 - val_loss: 1.1398 - val_accuracy: 0.8920\n",
            "Epoch 75/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1299 - accuracy: 0.9405 - val_loss: 1.1292 - val_accuracy: 0.8931\n",
            "Epoch 76/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1298 - accuracy: 0.9410 - val_loss: 1.1961 - val_accuracy: 0.8912\n",
            "Epoch 77/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1290 - accuracy: 0.9406 - val_loss: 1.1998 - val_accuracy: 0.8903\n",
            "Epoch 78/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1304 - accuracy: 0.9401 - val_loss: 1.1982 - val_accuracy: 0.8913\n",
            "Epoch 79/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1289 - accuracy: 0.9403 - val_loss: 1.1815 - val_accuracy: 0.8911\n",
            "Epoch 80/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1285 - accuracy: 0.9409 - val_loss: 1.2132 - val_accuracy: 0.8904\n",
            "Epoch 81/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1302 - accuracy: 0.9408 - val_loss: 1.2063 - val_accuracy: 0.8912\n",
            "Epoch 82/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1290 - accuracy: 0.9406 - val_loss: 1.1912 - val_accuracy: 0.8905\n",
            "Epoch 83/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1291 - accuracy: 0.9409 - val_loss: 1.1580 - val_accuracy: 0.8891\n",
            "Epoch 84/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1286 - accuracy: 0.9410 - val_loss: 1.2023 - val_accuracy: 0.8912\n",
            "Epoch 85/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1296 - accuracy: 0.9404 - val_loss: 1.1728 - val_accuracy: 0.8919\n",
            "Epoch 86/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1288 - accuracy: 0.9408 - val_loss: 1.1958 - val_accuracy: 0.8901\n",
            "Epoch 87/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1283 - accuracy: 0.9408 - val_loss: 1.1717 - val_accuracy: 0.8915\n",
            "Epoch 88/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1273 - accuracy: 0.9413 - val_loss: 1.2114 - val_accuracy: 0.8917\n",
            "Epoch 89/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1278 - accuracy: 0.9406 - val_loss: 1.1738 - val_accuracy: 0.8911\n",
            "Epoch 90/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1292 - accuracy: 0.9411 - val_loss: 1.1554 - val_accuracy: 0.8896\n",
            "Epoch 91/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1277 - accuracy: 0.9409 - val_loss: 1.1595 - val_accuracy: 0.8924\n",
            "Epoch 92/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1284 - accuracy: 0.9409 - val_loss: 1.1654 - val_accuracy: 0.8912\n",
            "Epoch 93/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1279 - accuracy: 0.9407 - val_loss: 1.1632 - val_accuracy: 0.8909\n",
            "Epoch 94/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1274 - accuracy: 0.9412 - val_loss: 1.1755 - val_accuracy: 0.8901\n",
            "Epoch 95/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1278 - accuracy: 0.9413 - val_loss: 1.1518 - val_accuracy: 0.8901\n",
            "Epoch 96/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1275 - accuracy: 0.9410 - val_loss: 1.1777 - val_accuracy: 0.8923\n",
            "Epoch 97/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1273 - accuracy: 0.9409 - val_loss: 1.1618 - val_accuracy: 0.8923\n",
            "Epoch 98/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1268 - accuracy: 0.9414 - val_loss: 1.1095 - val_accuracy: 0.8913\n",
            "Epoch 99/100\n",
            "600/600 [==============================] - 1s 2ms/step - loss: 0.1273 - accuracy: 0.9410 - val_loss: 1.1392 - val_accuracy: 0.8919\n",
            "Epoch 100/100\n",
            "600/600 [==============================] - 2s 3ms/step - loss: 0.1271 - accuracy: 0.9413 - val_loss: 1.1595 - val_accuracy: 0.8908\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ff93819bba8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "id": "8Y7P8jYVymBA",
        "outputId": "75175ac6-7b5a-4644-af78-65f3e7e115a9"
      },
      "source": [
        "# evaluate model\n",
        "accuracy = model.evaluate(X_test_r, y_test_v, batch_size=batch_size, verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-1e739510ce27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# evaluate model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0maccuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UhXpjKmxaigy"
      },
      "source": [
        "## Step 8. KNN Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DbrS4mGFagn7"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\r\n",
        "from sklearn.metrics import accuracy_score "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdS-VHrqIvQH"
      },
      "source": [
        "features_order = ['Source Port', 'Destination Port',\r\n",
        "                      'Protocol', 'Flow Duration', 'Total Fwd Packets', 'Total Backward Packets', 'Total Length of Fwd Packets',\r\n",
        "                      'Total Length of Bwd Packets', 'Fwd Packet Length Max', 'Fwd Packet Length Min', 'Fwd Packet Length Mean',\r\n",
        "                      'Fwd Packet Length Std', 'Bwd Packet Length Max', 'Bwd Packet Length Min', 'Bwd Packet Length Mean', 'Bwd Packet Length Std',\r\n",
        "                      'Flow Bytes/s', 'Flow Packets/s', 'Flow IAT Mean', 'Flow IAT Std', 'Flow IAT Max', 'Flow IAT Min', 'Fwd IAT Total',\r\n",
        "                      'Fwd IAT Mean', 'Fwd IAT Std', 'Fwd IAT Max', 'Fwd IAT Min', 'Bwd IAT Total', 'Bwd IAT Mean', 'Bwd IAT Std', 'Bwd IAT Max',\r\n",
        "                      'Bwd IAT Min', 'Fwd PSH Flags', 'Fwd URG Flags', 'Fwd Header Length', 'Bwd Header Length',\r\n",
        "                      'Fwd Packets/s', 'Bwd Packets/s', 'Min Packet Length', 'Max Packet Length', 'Packet Length Mean', 'Packet Length Std',\r\n",
        "                      'Packet Length Variance', 'FIN Flag Count', 'SYN Flag Count', 'RST Flag Count', 'PSH Flag Count', 'ACK Flag Count',\r\n",
        "                      'URG Flag Count', 'CWE Flag Count', 'ECE Flag Count', 'Down/Up Ratio', 'Average Packet Size', 'Avg Fwd Segment Size',\r\n",
        "                      'Avg Bwd Segment Size','Subflow Fwd Packets', 'Subflow Fwd Bytes',\r\n",
        "                      'Subflow Bwd Packets', 'Subflow Bwd Bytes', 'Init_Win_bytes_forward', 'Init_Win_bytes_backward',\r\n",
        "                      'act_data_pkt_fwd', 'min_seg_size_forward', 'Active Mean', 'Active Std', 'Active Max', 'Active Min', 'Idle Mean',\r\n",
        "                      'Idle Std', 'Idle Max', 'Idle Min']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1vbmR-lexwT"
      },
      "source": [
        "features=[\"Fwd Packet Length Max\",\"Flow IAT Std\",\"Fwd Packet Length Std\" ,\"Fwd IAT Total\",'Flow Packets/s', \"Fwd Packet Length Mean\",  \"Flow Bytes/s\",  \"Flow IAT Mean\", \"Bwd Packet Length Mean\",  \"Flow IAT Max\", \"Bwd Packet Length Std\", ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGiMP4URCr_1"
      },
      "source": [
        "### Import module for KNN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FOSoDSnLISlh"
      },
      "source": [
        "First, convert numpy array to dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "YKhW22bNHOjH",
        "outputId": "9891dd8a-b9d8-46a5-cedf-abe3154da0bb"
      },
      "source": [
        "df_train = pd.DataFrame(X_train, columns = features_order)\r\n",
        "df_train.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Source Port</th>\n",
              "      <th>Destination Port</th>\n",
              "      <th>Protocol</th>\n",
              "      <th>Flow Duration</th>\n",
              "      <th>Total Fwd Packets</th>\n",
              "      <th>Total Backward Packets</th>\n",
              "      <th>Total Length of Fwd Packets</th>\n",
              "      <th>Total Length of Bwd Packets</th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Fwd Packet Length Min</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Bwd Packet Length Max</th>\n",
              "      <th>Bwd Packet Length Min</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Flow IAT Min</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Fwd IAT Mean</th>\n",
              "      <th>Fwd IAT Std</th>\n",
              "      <th>Fwd IAT Max</th>\n",
              "      <th>Fwd IAT Min</th>\n",
              "      <th>Bwd IAT Total</th>\n",
              "      <th>Bwd IAT Mean</th>\n",
              "      <th>Bwd IAT Std</th>\n",
              "      <th>Bwd IAT Max</th>\n",
              "      <th>Bwd IAT Min</th>\n",
              "      <th>Fwd PSH Flags</th>\n",
              "      <th>Fwd URG Flags</th>\n",
              "      <th>Fwd Header Length</th>\n",
              "      <th>Bwd Header Length</th>\n",
              "      <th>Fwd Packets/s</th>\n",
              "      <th>Bwd Packets/s</th>\n",
              "      <th>Min Packet Length</th>\n",
              "      <th>Max Packet Length</th>\n",
              "      <th>Packet Length Mean</th>\n",
              "      <th>Packet Length Std</th>\n",
              "      <th>Packet Length Variance</th>\n",
              "      <th>FIN Flag Count</th>\n",
              "      <th>SYN Flag Count</th>\n",
              "      <th>RST Flag Count</th>\n",
              "      <th>PSH Flag Count</th>\n",
              "      <th>ACK Flag Count</th>\n",
              "      <th>URG Flag Count</th>\n",
              "      <th>CWE Flag Count</th>\n",
              "      <th>ECE Flag Count</th>\n",
              "      <th>Down/Up Ratio</th>\n",
              "      <th>Average Packet Size</th>\n",
              "      <th>Avg Fwd Segment Size</th>\n",
              "      <th>Avg Bwd Segment Size</th>\n",
              "      <th>Subflow Fwd Packets</th>\n",
              "      <th>Subflow Fwd Bytes</th>\n",
              "      <th>Subflow Bwd Packets</th>\n",
              "      <th>Subflow Bwd Bytes</th>\n",
              "      <th>Init_Win_bytes_forward</th>\n",
              "      <th>Init_Win_bytes_backward</th>\n",
              "      <th>act_data_pkt_fwd</th>\n",
              "      <th>min_seg_size_forward</th>\n",
              "      <th>Active Mean</th>\n",
              "      <th>Active Std</th>\n",
              "      <th>Active Max</th>\n",
              "      <th>Active Min</th>\n",
              "      <th>Idle Mean</th>\n",
              "      <th>Idle Std</th>\n",
              "      <th>Idle Max</th>\n",
              "      <th>Idle Min</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.004161</td>\n",
              "      <td>0.001528</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.750368e-09</td>\n",
              "      <td>2.006996e-06</td>\n",
              "      <td>1.670159e-03</td>\n",
              "      <td>0.003210</td>\n",
              "      <td>4.163675e-03</td>\n",
              "      <td>3.294118e-06</td>\n",
              "      <td>0.008277</td>\n",
              "      <td>0.004139</td>\n",
              "      <td>0.000067</td>\n",
              "      <td>0.004167</td>\n",
              "      <td>4.110400e-03</td>\n",
              "      <td>0.008277</td>\n",
              "      <td>0.004170</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.004199</td>\n",
              "      <td>4.140332e-03</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000833</td>\n",
              "      <td>0.000543</td>\n",
              "      <td>1.501095e-06</td>\n",
              "      <td>3.018877e-06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.001355</td>\n",
              "      <td>0.000681</td>\n",
              "      <td>4.633205e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.001451</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001528</td>\n",
              "      <td>0.000833</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000362</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000543</td>\n",
              "      <td>2.283559e-06</td>\n",
              "      <td>0.125015</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.636364</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>8.658003e-03</td>\n",
              "      <td>6.428571e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.333333e-07</td>\n",
              "      <td>6.554622e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.333333e-09</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000290</td>\n",
              "      <td>0.000290</td>\n",
              "      <td>6.493498e-03</td>\n",
              "      <td>1.298701e-02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000290</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000181</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.003967</td>\n",
              "      <td>0.003784</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001094</td>\n",
              "      <td>0.001304</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.991039e-10</td>\n",
              "      <td>4.203819e-08</td>\n",
              "      <td>8.823529e-02</td>\n",
              "      <td>0.174941</td>\n",
              "      <td>1.750000e-01</td>\n",
              "      <td>5.882353e-07</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>1.750000e-01</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000579</td>\n",
              "      <td>0.000290</td>\n",
              "      <td>3.928256e-08</td>\n",
              "      <td>4.762654e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000524</td>\n",
              "      <td>0.003424</td>\n",
              "      <td>0.001594</td>\n",
              "      <td>2.537538e-06</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004191</td>\n",
              "      <td>0.001094</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000579</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000181</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000181</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.003510</td>\n",
              "      <td>0.003601</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.727273</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000345</td>\n",
              "      <td>0.004161</td>\n",
              "      <td>0.001528</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.528986e-05</td>\n",
              "      <td>1.041666e-02</td>\n",
              "      <td>5.336134e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.250000e-07</td>\n",
              "      <td>5.462185e-07</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>8.333333e-09</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000362</td>\n",
              "      <td>0.000181</td>\n",
              "      <td>7.812492e-03</td>\n",
              "      <td>1.562500e-02</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000242</td>\n",
              "      <td>0.001054</td>\n",
              "      <td>0.000736</td>\n",
              "      <td>5.405405e-07</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.001451</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001528</td>\n",
              "      <td>0.000362</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000181</td>\n",
              "      <td>7.611864e-07</td>\n",
              "      <td>0.445572</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.909091</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.015444</td>\n",
              "      <td>0.025466</td>\n",
              "      <td>0.417300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.590488</td>\n",
              "      <td>0.496983</td>\n",
              "      <td>5.882509e-08</td>\n",
              "      <td>2.495034e-08</td>\n",
              "      <td>1.033613e-01</td>\n",
              "      <td>0.410165</td>\n",
              "      <td>8.183333e-01</td>\n",
              "      <td>4.201681e-08</td>\n",
              "      <td>0.818333</td>\n",
              "      <td>0.272500</td>\n",
              "      <td>0.791899</td>\n",
              "      <td>0.818333</td>\n",
              "      <td>4.166667e-08</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>3.358349e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001122</td>\n",
              "      <td>0.001520</td>\n",
              "      <td>1.201514e-08</td>\n",
              "      <td>5.089781e-08</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.292143</td>\n",
              "      <td>0.630426</td>\n",
              "      <td>0.535618</td>\n",
              "      <td>2.865587e-01</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.142857</td>\n",
              "      <td>0.643026</td>\n",
              "      <td>0.015444</td>\n",
              "      <td>0.590488</td>\n",
              "      <td>0.001122</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000543</td>\n",
              "      <td>0.000128</td>\n",
              "      <td>0.000905</td>\n",
              "      <td>1.470993e-03</td>\n",
              "      <td>0.000015</td>\n",
              "      <td>0.003601</td>\n",
              "      <td>0.000181</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.818333</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.818333</td>\n",
              "      <td>0.818333</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Source Port  Destination Port  Protocol  ...  Idle Std  Idle Max  Idle Min\n",
              "0          0.0          0.000000  0.000000  ...       0.0  0.000000  0.000000\n",
              "1          0.0          0.000000  0.000000  ...       0.0  0.000000  0.000000\n",
              "2          0.0          0.001094  0.001304  ...       0.0  0.000000  0.000000\n",
              "3          0.0          0.000000  0.000000  ...       0.0  0.000000  0.000000\n",
              "4          0.0          0.015444  0.025466  ...       0.0  0.818333  0.818333\n",
              "\n",
              "[5 rows x 71 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tMdcfBB5MF2E"
      },
      "source": [
        "df_val = pd.DataFrame(X_val, columns = features_order)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mc065YWMNPDZ"
      },
      "source": [
        "Select a subset of features from the dataframe"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "CQcKmtHlKL3W",
        "outputId": "c578465b-ead9-46f3-9c19-a5c303a93d6c"
      },
      "source": [
        "df_sub=df_train.loc[:, features]\r\n",
        "df_sub.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.006996e-06</td>\n",
              "      <td>0.004170</td>\n",
              "      <td>4.163675e-03</td>\n",
              "      <td>4.140332e-03</td>\n",
              "      <td>4.110400e-03</td>\n",
              "      <td>0.003210</td>\n",
              "      <td>0.004167</td>\n",
              "      <td>0.008277</td>\n",
              "      <td>0.004139</td>\n",
              "      <td>0.000063</td>\n",
              "      <td>0.000067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>8.658003e-03</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6.333333e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>8.333333e-09</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.203819e-08</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.750000e-01</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>1.750000e-01</td>\n",
              "      <td>0.174941</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.175000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.041666e-02</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.250000e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>8.333333e-09</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2.495034e-08</td>\n",
              "      <td>0.000006</td>\n",
              "      <td>8.183333e-01</td>\n",
              "      <td>3.358349e-08</td>\n",
              "      <td>4.166667e-08</td>\n",
              "      <td>0.410165</td>\n",
              "      <td>0.818333</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.272500</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>0.791899</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Fwd Packet Length Max  Flow IAT Std  ...  Flow IAT Max  Bwd Packet Length Std\n",
              "0           2.006996e-06      0.004170  ...      0.000063               0.000067\n",
              "1           8.658003e-03      0.000000  ...      0.000000               0.000000\n",
              "2           4.203819e-08      0.000000  ...      0.000000               0.000000\n",
              "3           1.041666e-02      0.000000  ...      0.000000               0.000000\n",
              "4           2.495034e-08      0.000006  ...      0.000011               0.791899\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 213
        },
        "id": "NfzSBQvYMKIu",
        "outputId": "034109f5-fdf4-4a51-83c7-3a64450bc70b"
      },
      "source": [
        "df_val_sub=df_val.loc[:, features]\r\n",
        "df_val_sub.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Fwd Packet Length Max</th>\n",
              "      <th>Flow IAT Std</th>\n",
              "      <th>Fwd Packet Length Std</th>\n",
              "      <th>Fwd IAT Total</th>\n",
              "      <th>Flow Packets/s</th>\n",
              "      <th>Fwd Packet Length Mean</th>\n",
              "      <th>Flow Bytes/s</th>\n",
              "      <th>Flow IAT Mean</th>\n",
              "      <th>Bwd Packet Length Mean</th>\n",
              "      <th>Flow IAT Max</th>\n",
              "      <th>Bwd Packet Length Std</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.007246</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.629147e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000474</td>\n",
              "      <td>8.344686e-03</td>\n",
              "      <td>8.382044e-09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001867</td>\n",
              "      <td>0.008343</td>\n",
              "      <td>0.991667</td>\n",
              "      <td>0.000358</td>\n",
              "      <td>0.002946</td>\n",
              "      <td>0.002530</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.007663</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7.209963e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.000014</td>\n",
              "      <td>0.000484</td>\n",
              "      <td>1.672833e-02</td>\n",
              "      <td>8.382044e-09</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001878</td>\n",
              "      <td>0.016715</td>\n",
              "      <td>0.991667</td>\n",
              "      <td>0.000357</td>\n",
              "      <td>0.002999</td>\n",
              "      <td>0.002537</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.009804</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5.617064e-07</td>\n",
              "      <td>0.000000e+00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Fwd Packet Length Max  Flow IAT Std  ...  Flow IAT Max  Bwd Packet Length Std\n",
              "0               0.007246      0.000000  ...      0.000000               0.000000\n",
              "1               0.000014      0.000474  ...      0.002946               0.002530\n",
              "2               0.007663      0.000000  ...      0.000000               0.000000\n",
              "3               0.000014      0.000484  ...      0.002999               0.002537\n",
              "4               0.009804      0.000000  ...      0.000000               0.000000\n",
              "\n",
              "[5 rows x 11 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T1LdcE2INV4z"
      },
      "source": [
        "Convert dataframes to numpy array"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZVfCiyLJkwo"
      },
      "source": [
        "X_train = df_sub.to_numpy()\r\n",
        "X_val = df_val_sub.to_numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q_Z_PAi7zntJ",
        "outputId": "149750b0-761c-4fe8-dcb7-341a27f0e2ed"
      },
      "source": [
        "for i in range(1,X_train.shape[1]+1):\r\n",
        "    knn=KNeighborsClassifier(n_neighbors=i)\r\n",
        "    model_knn=knn.fit(X_train,y_train)\r\n",
        "    yhat=model_knn.predict(X_val)\r\n",
        "    print(\"for \" , i,  \" as K, accuracy is : \", accuracy_score(y_val, yhat))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "for  1  as K, accuracy is :  0.6404\n",
            "for  2  as K, accuracy is :  0.6353333333333333\n",
            "for  3  as K, accuracy is :  0.6524\n",
            "for  4  as K, accuracy is :  0.6562666666666667\n",
            "for  5  as K, accuracy is :  0.6664\n",
            "for  6  as K, accuracy is :  0.6624\n",
            "for  7  as K, accuracy is :  0.6705333333333333\n",
            "for  8  as K, accuracy is :  0.6737333333333333\n",
            "for  9  as K, accuracy is :  0.6776\n",
            "for  10  as K, accuracy is :  0.6786666666666666\n",
            "for  11  as K, accuracy is :  0.6785333333333333\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kSGgIaknOZ34"
      },
      "source": [
        "## Step 9. Random Foresty with DecisionTree "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I7jbJl0fOaMB"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "from sklearn import metrics"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jn17DazJOjsv",
        "outputId": "939129dd-67b5-4e0f-c560-cf31a387d5e8"
      },
      "source": [
        "clf = RandomForestClassifier(n_estimators=10, random_state=SEED)\r\n",
        "clf.fit(X_train,y_train)\r\n",
        "    \r\n",
        "pred = clf.predict(X_val)\r\n",
        "val_acc = metrics.accuracy_score(y_val,pred)*100        \r\n",
        "print('val acc:',val_acc)\r\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "val acc: 72.72\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IpXbErJHfbf3"
      },
      "source": [
        "## Step 10. CONV2D model\r\n",
        "Convert each example to a 6x4 color image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u1SKdA7zkyA8"
      },
      "source": [
        "X_train_enlarge = np.append(X_train, np.zeros([len(X_train),1]),1)\r\n",
        "X_test_enlarge = np.append(X_test, np.zeros([len(X_test),1]),1)"
      ],
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYmyATCFfptg"
      },
      "source": [
        "img_row = 6\r\n",
        "img_col = 4\r\n",
        "\r\n",
        "X_train_img = np.array([x.reshape(img_row, img_col, 3) for x in X_train_enlarge])\r\n",
        "X_test_img = np.array([x.reshape(img_row, img_col, 3) for x in X_test_enlarge])"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "4R8X8qztlaB7",
        "outputId": "bf6da646-edf9-4c2e-a4b6-111c2ba4a78f"
      },
      "source": [
        "#ploting images for data\r\n",
        "%matplotlib inline\r\n",
        "n = 40  # how many digits we will display\r\n",
        "plt.figure(figsize=(80, 30))\r\n",
        "for i in range(6,11):\r\n",
        "    # display original\r\n",
        "    ax = plt.subplot(1, n, i + 1)\r\n",
        "    plt.imshow(X_train_img[i])\r\n",
        "    plt.gray()\r\n",
        "    ax.get_xaxis().set_visible(False)\r\n",
        "    ax.get_yaxis().set_visible(False)\r\n",
        "plt.show()\r\n",
        "plt.close()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 5760x2160 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "8Jzhzsq0xkXH",
        "outputId": "df18fbac-7e7c-4535-c572-e3f39fae2f8f"
      },
      "source": [
        "n = 40  # how many digits we will display\r\n",
        "plt.figure(figsize=(80, 30))\r\n",
        "for i in range(6,11):\r\n",
        "    # display original\r\n",
        "    ax = plt.subplot(1, n, i + 1)\r\n",
        "    plt.imshow(X_test_img[i])\r\n",
        "    plt.gray()\r\n",
        "    ax.get_xaxis().set_visible(False)\r\n",
        "    ax.get_yaxis().set_visible(False)\r\n",
        "plt.show()\r\n",
        "plt.close()"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAi8AAACdCAYAAAB8SZgTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAErElEQVR4nO3bQWqkVRSG4XNTZdOR1oxE6IGO3JQLcO4S3IGbcFdOBCfiqCVNol1Vx4E0CJ1IBf5j8aWfZ/zz8dO3bvJSoVd3FwBAiqtLvwAAwFOIFwAgingBAKKIFwAgingBAKLsn/LwWst/TXrIGmzAPlV3ry2mnN9lbHt+m0x9uD2y+u/9uY/eyf3LdrWb2z6dqvvk/j3D+/ekeOFhu5fXY9vH+7ux7U1Nf5IOgz/g6ji4vaVVtX85svziMPt7dVf3Y9t3Q/8mVVV9mHtv/rG7fjW2fby73XDN/XvIpe6fPxsBAFHECwAQRbwAAFHECwAQRbwAAFHECwAQRbwAAFHECwAQRbwAAFHECwAQRbwAAFHECwAQRbwAAFHECwAQRbwAAFHECwAQRbwAAFHECwAQRbwAAFHECwAQZXX3+Q/vrrqu9yMvclPvRnbfe/N2dH5Ud68tdtZa5x82m3F+VZ/urufGj3dj0/dVddzq/Par69Vui6kPfF7Hkd33/ngzOj/K/Xue9883LwBAFPECAEQRLwBAFPECAEQRLwBAFPECAEQRLwBAFPECAEQRLwBAFPECAEQRLwBAFPECAEQRLwBAFPECAEQRLwBAFPECAEQRLwBAFPECAEQRLwBAFPECAEQRLwBAFPECAEQRLwBAlNXd5z+81vkPs5nuXlvszJ7fbAd/sj+Nbb87jE1XVcr5TXsxtnzz2V9j27dvqw5H51f7wW3373/w/O6fb14AgCjiBQCIIl4AgCjiBQCIIl4AgCjiBQCIIl4AgCjiBQCIIl4AgCjiBQCIIl4AgCjiBQCIIl4AgCjiBQCIIl4AgCjiBQCIIl4AgCjiBQCIIl4AgCjiBQCIIl4AgCjiBQCIsr/0C7zXw/treP+j18fR+Xfrm7HtVT+PbU9/rrcyf/++mhu/+3Vu+/Tn3PaW/ACN5v494j/un29eAIAo4gUAiCJeAIAo4gUAiCJeAIAo4gUAiCJeAIAo4gUAiCJeAIAo4gUAiCJeAIAo4gUAiCJeAIAo4gUAiCJeAIAo4gUAiCJeAIAo4gUAiCJeAIAo4gUAiCJeAIAo4gUAiCJeAIAoq7vPf3it8x/+qHw/uP1Tdf+2tlhyfpfR3c5vlPvH49y/aZe5f755AQCiiBcAIIp4AQCiiBcAIIp4AQCiiBcAIIp4AQCiiBcAIIp4AQCiiBcAIIp4AQCiiBcAIIp4AQCiiBcAIIp4AQCiiBcAIIp4AQCiiBcAIIp4AQCiiBcAIIp4AQCiiBcAIMrq7rMf/nK97m/ru5EX+bF+GNl9Drp7bbGz1jr/sNnMduf3umvo/pX79yj3r6puIqfr9rbqcHD/kj12/3zzAgBEES8AQBTxAgBEES8AQBTxAgBEES8AQBTxAgBEES8AQBTxAgBEES8AQBTxAgBEES8AQBTxAgBEES8AQBTxAgBEES8AQBTxAgBEES8AQBTxAgBEES8AQBTxAgBEES8AQBTxAgBEWd19/sNr/V5Vv8y9Dg/4uru/2GLI+V2E88vm/LI5v2yPnt+T4gUA4NL82QgAiCJeAIAo4gUAiCJeAIAo4gUAiCJeAIAo4gUAiCJeAIAo4gUAiPI3tLrk/DuI3PsAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 5760x2160 with 5 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HnHm0-NankcX"
      },
      "source": [
        "Build a simple CNN model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HqVz8tQIpfn1"
      },
      "source": [
        "import tensorflow as tf\r\n",
        "from tensorflow.keras.models import Sequential\r\n",
        "from tensorflow.keras.layers import Conv2D, BatchNormalization, MaxPooling2D, Flatten, Dense, Activation,Dropout\r\n",
        "from tensorflow.keras.constraints import max_norm"
      ],
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vqn8AWBor4d-"
      },
      "source": [
        "batch_size = 100 # increasing batch size with more gpu added\r\n",
        " \r\n",
        "num_class = 15                   # 15 intrusion classes, including benign traffic class\r\n",
        "epochs = 90"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RFPWWw2AnnUa",
        "outputId": "e372e5e4-4281-452e-a0f6-14ac35f8f878"
      },
      "source": [
        "model2d = Sequential()\r\n",
        "model2d.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(6,4,3)))\r\n",
        "model2d.add(MaxPooling2D(pool_size=(1, 1)))\r\n",
        "model2d.add(Conv2D(64, kernel_size=(1,1), activation='relu'))\r\n",
        "model2d.add(MaxPooling2D(pool_size=(1, 1)))\r\n",
        "model2d.add(Flatten())\r\n",
        "model2d.add(Dense(1024))\r\n",
        "model2d.add(Dense(num_class, activation='softmax'))\r\n",
        " \r\n",
        "model2d.summary()"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_8\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv2d_14 (Conv2D)           (None, 4, 2, 32)          896       \n",
            "_________________________________________________________________\n",
            "max_pooling2d_9 (MaxPooling2 (None, 4, 2, 32)          0         \n",
            "_________________________________________________________________\n",
            "conv2d_15 (Conv2D)           (None, 4, 2, 64)          2112      \n",
            "_________________________________________________________________\n",
            "max_pooling2d_10 (MaxPooling (None, 4, 2, 64)          0         \n",
            "_________________________________________________________________\n",
            "flatten_3 (Flatten)          (None, 512)               0         \n",
            "_________________________________________________________________\n",
            "dense_5 (Dense)              (None, 1024)              525312    \n",
            "_________________________________________________________________\n",
            "dense_6 (Dense)              (None, 15)                15375     \n",
            "=================================================================\n",
            "Total params: 543,695\n",
            "Trainable params: 543,695\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2pTciRO_r6g3",
        "outputId": "e95a4128-306e-4e4f-a5eb-c527de42aaf2"
      },
      "source": [
        "# training\r\n",
        "model2d.compile(loss=tf.keras.losses.categorical_crossentropy,\r\n",
        "              optimizer=tf.keras.optimizers.SGD(lr=0.01),\r\n",
        "              metrics=['accuracy'])\r\n",
        "model2d.fit(X_train_img, y_train_v,\r\n",
        "          batch_size=batch_size,\r\n",
        "          epochs=epochs,\r\n",
        "          verbose=1,\r\n",
        "          validation_data=(X_test_img, y_test_v))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 2.5952 - accuracy: 0.1990 - val_loss: 2.0756 - val_accuracy: 0.4707\n",
            "Epoch 2/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 1.8769 - accuracy: 0.4955 - val_loss: 1.5641 - val_accuracy: 0.5488\n",
            "Epoch 3/90\n",
            "600/600 [==============================] - 9s 16ms/step - loss: 1.4231 - accuracy: 0.5838 - val_loss: 1.3400 - val_accuracy: 0.5923\n",
            "Epoch 4/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 1.1852 - accuracy: 0.6279 - val_loss: 1.1925 - val_accuracy: 0.6857\n",
            "Epoch 5/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 1.0250 - accuracy: 0.6877 - val_loss: 1.1128 - val_accuracy: 0.6514\n",
            "Epoch 6/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.8841 - accuracy: 0.7396 - val_loss: 1.1063 - val_accuracy: 0.6769\n",
            "Epoch 7/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.8001 - accuracy: 0.7502 - val_loss: 1.1154 - val_accuracy: 0.6847\n",
            "Epoch 8/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.7308 - accuracy: 0.7581 - val_loss: 1.1377 - val_accuracy: 0.7017\n",
            "Epoch 9/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.6783 - accuracy: 0.7695 - val_loss: 1.1906 - val_accuracy: 0.6975\n",
            "Epoch 10/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.6500 - accuracy: 0.7805 - val_loss: 1.2579 - val_accuracy: 0.6908\n",
            "Epoch 11/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.6168 - accuracy: 0.7907 - val_loss: 1.2575 - val_accuracy: 0.6892\n",
            "Epoch 12/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.5962 - accuracy: 0.7951 - val_loss: 1.2864 - val_accuracy: 0.6920\n",
            "Epoch 13/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.5659 - accuracy: 0.7994 - val_loss: 1.3443 - val_accuracy: 0.6705\n",
            "Epoch 14/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.5469 - accuracy: 0.7991 - val_loss: 1.4198 - val_accuracy: 0.6340\n",
            "Epoch 15/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.5287 - accuracy: 0.8069 - val_loss: 1.4398 - val_accuracy: 0.6222\n",
            "Epoch 16/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.5188 - accuracy: 0.8058 - val_loss: 1.4044 - val_accuracy: 0.6485\n",
            "Epoch 17/90\n",
            "600/600 [==============================] - 9s 16ms/step - loss: 0.5078 - accuracy: 0.8141 - val_loss: 1.4647 - val_accuracy: 0.6051\n",
            "Epoch 18/90\n",
            "600/600 [==============================] - 9s 16ms/step - loss: 0.4816 - accuracy: 0.8209 - val_loss: 1.6550 - val_accuracy: 0.5369\n",
            "Epoch 19/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.4719 - accuracy: 0.8202 - val_loss: 1.5656 - val_accuracy: 0.5543\n",
            "Epoch 20/90\n",
            "600/600 [==============================] - 11s 18ms/step - loss: 0.4684 - accuracy: 0.8215 - val_loss: 1.6422 - val_accuracy: 0.5326\n",
            "Epoch 21/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.4632 - accuracy: 0.8254 - val_loss: 1.6599 - val_accuracy: 0.5376\n",
            "Epoch 22/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.4615 - accuracy: 0.8236 - val_loss: 1.7776 - val_accuracy: 0.5337\n",
            "Epoch 23/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.4381 - accuracy: 0.8309 - val_loss: 1.5183 - val_accuracy: 0.5537\n",
            "Epoch 24/90\n",
            "600/600 [==============================] - 10s 17ms/step - loss: 0.4396 - accuracy: 0.8283 - val_loss: 1.6374 - val_accuracy: 0.5387\n",
            "Epoch 25/90\n",
            "600/600 [==============================] - 10s 17ms/step - loss: 0.4294 - accuracy: 0.8299 - val_loss: 1.8052 - val_accuracy: 0.5329\n",
            "Epoch 26/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.4189 - accuracy: 0.8381 - val_loss: 1.7657 - val_accuracy: 0.5275\n",
            "Epoch 27/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.4211 - accuracy: 0.8394 - val_loss: 1.9165 - val_accuracy: 0.5267\n",
            "Epoch 28/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.4071 - accuracy: 0.8393 - val_loss: 1.7649 - val_accuracy: 0.5371\n",
            "Epoch 29/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.4049 - accuracy: 0.8437 - val_loss: 1.9406 - val_accuracy: 0.5269\n",
            "Epoch 30/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.4003 - accuracy: 0.8461 - val_loss: 1.7100 - val_accuracy: 0.5330\n",
            "Epoch 31/90\n",
            "600/600 [==============================] - 9s 16ms/step - loss: 0.3830 - accuracy: 0.8497 - val_loss: 1.7483 - val_accuracy: 0.5347\n",
            "Epoch 32/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.3861 - accuracy: 0.8489 - val_loss: 1.7239 - val_accuracy: 0.5369\n",
            "Epoch 33/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.3847 - accuracy: 0.8512 - val_loss: 1.7785 - val_accuracy: 0.5379\n",
            "Epoch 34/90\n",
            "600/600 [==============================] - 9s 16ms/step - loss: 0.3702 - accuracy: 0.8556 - val_loss: 1.6264 - val_accuracy: 0.5442\n",
            "Epoch 35/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.3631 - accuracy: 0.8606 - val_loss: 1.8567 - val_accuracy: 0.5315\n",
            "Epoch 36/90\n",
            "600/600 [==============================] - 9s 16ms/step - loss: 0.3629 - accuracy: 0.8571 - val_loss: 1.9584 - val_accuracy: 0.5345\n",
            "Epoch 37/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.3629 - accuracy: 0.8621 - val_loss: 1.6743 - val_accuracy: 0.5615\n",
            "Epoch 38/90\n",
            "600/600 [==============================] - 9s 16ms/step - loss: 0.3552 - accuracy: 0.8605 - val_loss: 1.8099 - val_accuracy: 0.5380\n",
            "Epoch 39/90\n",
            "600/600 [==============================] - 9s 16ms/step - loss: 0.3614 - accuracy: 0.8609 - val_loss: 1.8217 - val_accuracy: 0.5358\n",
            "Epoch 40/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.3475 - accuracy: 0.8621 - val_loss: 2.0973 - val_accuracy: 0.5234\n",
            "Epoch 41/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.3417 - accuracy: 0.8665 - val_loss: 1.7911 - val_accuracy: 0.5457\n",
            "Epoch 42/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.3431 - accuracy: 0.8648 - val_loss: 2.1604 - val_accuracy: 0.5272\n",
            "Epoch 43/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.3519 - accuracy: 0.8665 - val_loss: 1.9907 - val_accuracy: 0.5395\n",
            "Epoch 44/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.3371 - accuracy: 0.8653 - val_loss: 1.9357 - val_accuracy: 0.5364\n",
            "Epoch 45/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.3288 - accuracy: 0.8670 - val_loss: 1.9239 - val_accuracy: 0.5389\n",
            "Epoch 46/90\n",
            "600/600 [==============================] - 9s 16ms/step - loss: 0.3288 - accuracy: 0.8705 - val_loss: 1.8771 - val_accuracy: 0.5466\n",
            "Epoch 47/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.3411 - accuracy: 0.8713 - val_loss: 1.8404 - val_accuracy: 0.5541\n",
            "Epoch 48/90\n",
            "600/600 [==============================] - 9s 16ms/step - loss: 0.3308 - accuracy: 0.8692 - val_loss: 2.0722 - val_accuracy: 0.5289\n",
            "Epoch 49/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.3323 - accuracy: 0.8703 - val_loss: 1.8881 - val_accuracy: 0.5393\n",
            "Epoch 50/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.3230 - accuracy: 0.8730 - val_loss: 1.8225 - val_accuracy: 0.5790\n",
            "Epoch 51/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.3151 - accuracy: 0.8727 - val_loss: 2.0029 - val_accuracy: 0.5362\n",
            "Epoch 52/90\n",
            "600/600 [==============================] - 10s 17ms/step - loss: 0.3182 - accuracy: 0.8705 - val_loss: 2.0265 - val_accuracy: 0.5353\n",
            "Epoch 53/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.3073 - accuracy: 0.8738 - val_loss: 2.1127 - val_accuracy: 0.5331\n",
            "Epoch 54/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.3094 - accuracy: 0.8728 - val_loss: 2.0344 - val_accuracy: 0.5352\n",
            "Epoch 55/90\n",
            "600/600 [==============================] - 12s 19ms/step - loss: 0.3033 - accuracy: 0.8747 - val_loss: 2.0237 - val_accuracy: 0.5266\n",
            "Epoch 56/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.3045 - accuracy: 0.8713 - val_loss: 2.0555 - val_accuracy: 0.5312\n",
            "Epoch 57/90\n",
            "600/600 [==============================] - 10s 17ms/step - loss: 0.3050 - accuracy: 0.8720 - val_loss: 1.9573 - val_accuracy: 0.5713\n",
            "Epoch 58/90\n",
            "600/600 [==============================] - 10s 17ms/step - loss: 0.3094 - accuracy: 0.8743 - val_loss: 1.9764 - val_accuracy: 0.5395\n",
            "Epoch 59/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.3009 - accuracy: 0.8756 - val_loss: 1.9426 - val_accuracy: 0.5627\n",
            "Epoch 60/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2969 - accuracy: 0.8772 - val_loss: 2.0537 - val_accuracy: 0.5259\n",
            "Epoch 61/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2983 - accuracy: 0.8777 - val_loss: 1.8362 - val_accuracy: 0.5800\n",
            "Epoch 62/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2911 - accuracy: 0.8791 - val_loss: 2.0284 - val_accuracy: 0.5503\n",
            "Epoch 63/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2936 - accuracy: 0.8803 - val_loss: 2.1323 - val_accuracy: 0.5437\n",
            "Epoch 64/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.3062 - accuracy: 0.8779 - val_loss: 2.1864 - val_accuracy: 0.5269\n",
            "Epoch 65/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2929 - accuracy: 0.8784 - val_loss: 1.9768 - val_accuracy: 0.5623\n",
            "Epoch 66/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2911 - accuracy: 0.8789 - val_loss: 2.1318 - val_accuracy: 0.5241\n",
            "Epoch 67/90\n",
            "600/600 [==============================] - 9s 16ms/step - loss: 0.2907 - accuracy: 0.8786 - val_loss: 2.0878 - val_accuracy: 0.5334\n",
            "Epoch 68/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2916 - accuracy: 0.8809 - val_loss: 1.9816 - val_accuracy: 0.5587\n",
            "Epoch 69/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2928 - accuracy: 0.8785 - val_loss: 2.0662 - val_accuracy: 0.5420\n",
            "Epoch 70/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2853 - accuracy: 0.8805 - val_loss: 2.1784 - val_accuracy: 0.5334\n",
            "Epoch 71/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2805 - accuracy: 0.8805 - val_loss: 2.0386 - val_accuracy: 0.5431\n",
            "Epoch 72/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2853 - accuracy: 0.8801 - val_loss: 2.1822 - val_accuracy: 0.5387\n",
            "Epoch 73/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2811 - accuracy: 0.8790 - val_loss: 2.0617 - val_accuracy: 0.5615\n",
            "Epoch 74/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2942 - accuracy: 0.8790 - val_loss: 2.1949 - val_accuracy: 0.5327\n",
            "Epoch 75/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2824 - accuracy: 0.8806 - val_loss: 2.2527 - val_accuracy: 0.5141\n",
            "Epoch 76/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2738 - accuracy: 0.8818 - val_loss: 1.9745 - val_accuracy: 0.5606\n",
            "Epoch 77/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2759 - accuracy: 0.8794 - val_loss: 2.2162 - val_accuracy: 0.5373\n",
            "Epoch 78/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2725 - accuracy: 0.8827 - val_loss: 2.1256 - val_accuracy: 0.5708\n",
            "Epoch 79/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2737 - accuracy: 0.8836 - val_loss: 1.9823 - val_accuracy: 0.5689\n",
            "Epoch 80/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2707 - accuracy: 0.8831 - val_loss: 2.0202 - val_accuracy: 0.5689\n",
            "Epoch 81/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2765 - accuracy: 0.8808 - val_loss: 2.2257 - val_accuracy: 0.5022\n",
            "Epoch 82/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2710 - accuracy: 0.8821 - val_loss: 2.1009 - val_accuracy: 0.5713\n",
            "Epoch 83/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2733 - accuracy: 0.8819 - val_loss: 2.1457 - val_accuracy: 0.5571\n",
            "Epoch 84/90\n",
            "600/600 [==============================] - 10s 17ms/step - loss: 0.2674 - accuracy: 0.8843 - val_loss: 2.0905 - val_accuracy: 0.5403\n",
            "Epoch 85/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2685 - accuracy: 0.8837 - val_loss: 2.2539 - val_accuracy: 0.4999\n",
            "Epoch 86/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2674 - accuracy: 0.8832 - val_loss: 2.0662 - val_accuracy: 0.5774\n",
            "Epoch 87/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2685 - accuracy: 0.8830 - val_loss: 2.0068 - val_accuracy: 0.5797\n",
            "Epoch 88/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2700 - accuracy: 0.8835 - val_loss: 2.1167 - val_accuracy: 0.5408\n",
            "Epoch 89/90\n",
            "600/600 [==============================] - 10s 16ms/step - loss: 0.2661 - accuracy: 0.8844 - val_loss: 2.0628 - val_accuracy: 0.5593\n",
            "Epoch 90/90\n",
            "600/600 [==============================] - 10s 17ms/step - loss: 0.2615 - accuracy: 0.8863 - val_loss: 2.0213 - val_accuracy: 0.5849\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7faa38a826d8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZEuBVj6zzp1I"
      },
      "source": [
        "## Step 11. CONV2D with 9x8 gray image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iVF5urfzzMN"
      },
      "source": [
        "X_train_enlarge = np.append(X_train, np.zeros([len(X_train),1]),1)\r\n",
        "X_test_enlarge = np.append(X_test, np.zeros([len(X_test),1]),1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dgiNKZx30DA_"
      },
      "source": [
        "img_row = 9\r\n",
        "img_col = 8\r\n",
        "\r\n",
        "X_train_gray = np.array([x.reshape(img_row, img_col, 1) for x in X_train_enlarge])\r\n",
        "X_test_gray = np.array([x.reshape(img_row, img_col, 1) for x in X_test_enlarge])"
      ],
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "9Y0bn6k90bD5",
        "outputId": "b2178795-8dc8-496f-f331-afd1f013132e"
      },
      "source": [
        "n = 40  # how many digits we will display\r\n",
        "plt.figure(figsize=(80, 30))\r\n",
        "for i in range(6,11):\r\n",
        "    # display original\r\n",
        "    ax = plt.subplot(1, n, i + 1)\r\n",
        "    plt.imshow(X_train_gray[i])\r\n",
        "    plt.gray()\r\n",
        "    ax.get_xaxis().set_visible(False)\r\n",
        "    ax.get_yaxis().set_visible(False)\r\n",
        "plt.show()\r\n",
        "plt.close()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-69-758a1ac25062>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;31m# display original\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_gray\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_xaxis\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_visible\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/pyplot.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, data, **kwargs)\u001b[0m\n\u001b[1;32m   2649\u001b[0m         \u001b[0mfilternorm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilternorm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilterrad\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfilterrad\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimlim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mimlim\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2650\u001b[0m         resample=resample, url=url, **({\"data\": data} if data is not\n\u001b[0;32m-> 2651\u001b[0;31m         None else {}), **kwargs)\n\u001b[0m\u001b[1;32m   2652\u001b[0m     \u001b[0msci\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__ret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2653\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0m__ret\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/__init__.py\u001b[0m in \u001b[0;36minner\u001b[0;34m(ax, data, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1563\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1564\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1565\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanitize_sequence\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1566\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1567\u001b[0m         \u001b[0mbound\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_sig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0max\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/cbook/deprecation.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    356\u001b[0m                 \u001b[0;34mf\"%(removal)s.  If any parameter follows {name!r}, they \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    357\u001b[0m                 f\"should be pass as keyword, not positionally.\")\n\u001b[0;32m--> 358\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    360\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/axes/_axes.py\u001b[0m in \u001b[0;36mimshow\u001b[0;34m(self, X, cmap, norm, aspect, interpolation, alpha, vmin, vmax, origin, extent, shape, filternorm, filterrad, imlim, resample, url, **kwargs)\u001b[0m\n\u001b[1;32m   5624\u001b[0m                               resample=resample, **kwargs)\n\u001b[1;32m   5625\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5626\u001b[0;31m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5627\u001b[0m         \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_alpha\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5628\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_clip_path\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/matplotlib/image.py\u001b[0m in \u001b[0;36mset_data\u001b[0;34m(self, A)\u001b[0m\n\u001b[1;32m    697\u001b[0m                 or self._A.ndim == 3 and self._A.shape[-1] in [3, 4]):\n\u001b[1;32m    698\u001b[0m             raise TypeError(\"Invalid shape {} for image data\"\n\u001b[0;32m--> 699\u001b[0;31m                             .format(self._A.shape))\n\u001b[0m\u001b[1;32m    700\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_A\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: Invalid shape (9, 8, 1) for image data"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 5760x2160 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_-sgJteF-t7M"
      },
      "source": [
        "## Step 12. RNN Method"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "78CA10VnqsdX"
      },
      "source": [
        "The dataset is not right. It should be time serial data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zfllyVNUkzOP"
      },
      "source": [
        "from tensorflow.keras.layers import SimpleRNN"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E1EthXf--2s6",
        "outputId": "2bccdc8d-8587-43e3-959d-3bf953614177"
      },
      "source": [
        "modelrnn = Sequential()\r\n",
        "modelrnn.add(SimpleRNN(120, input_shape = (71,1), return_sequences=True))\r\n",
        "modelrnn.add(Dropout(0.2))\r\n",
        "\r\n",
        "modelrnn.add(SimpleRNN(120, return_sequences=True))\r\n",
        "modelrnn.add(Dropout(0.2))\r\n",
        "\r\n",
        "modelrnn.add(SimpleRNN(120, return_sequences=False))\r\n",
        "modelrnn.add(Dropout(0.2))\r\n",
        "\r\n",
        "# multiclass\r\n",
        "modelrnn.add(Dense(num_class))\r\n",
        "modelrnn.add(Activation('softmax'))\r\n",
        "\r\n",
        "modelrnn.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_5\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "simple_rnn_7 (SimpleRNN)     (None, 71, 120)           14640     \n",
            "_________________________________________________________________\n",
            "dropout_12 (Dropout)         (None, 71, 120)           0         \n",
            "_________________________________________________________________\n",
            "simple_rnn_8 (SimpleRNN)     (None, 71, 120)           28920     \n",
            "_________________________________________________________________\n",
            "dropout_13 (Dropout)         (None, 71, 120)           0         \n",
            "_________________________________________________________________\n",
            "simple_rnn_9 (SimpleRNN)     (None, 120)               28920     \n",
            "_________________________________________________________________\n",
            "dropout_14 (Dropout)         (None, 120)               0         \n",
            "_________________________________________________________________\n",
            "dense_10 (Dense)             (None, 15)                1815      \n",
            "_________________________________________________________________\n",
            "activation_4 (Activation)    (None, 15)                0         \n",
            "=================================================================\n",
            "Total params: 74,295\n",
            "Trainable params: 74,295\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ItOu4406lS7T"
      },
      "source": [
        "learning_rates = 1e-4\r\n",
        "optim = tf.keras.optimizers.Adam(lr=learning_rates, beta_1=0.9, beta_2=0.999, epsilon=1e-8)\r\n",
        "modelrnn.compile(loss='categorical_crossentropy', optimizer=optim, metrics=['accuracy']) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 687
        },
        "id": "88RBpD3GlVBr",
        "outputId": "7b67e028-2b61-4879-90d5-28f70f03bb66"
      },
      "source": [
        "# fit network\r\n",
        "modelrnn.fit(X_train, y_train_v, epochs=100, batch_size=batch_size, validation_data=(X_val, y_val_v), verbose=1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-62-f43279c74dd7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmodelrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train_v\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_val_v\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    869\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    870\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 871\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    872\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    724\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    725\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 726\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    727\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    728\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2967\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2968\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2969\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2970\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2971\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3360\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3361\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3362\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3363\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3204\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3205\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3206\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3207\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3208\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m    988\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 990\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    991\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    992\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    632\u001b[0m             \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    633\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 634\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    635\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    636\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    975\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    976\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"ag_error_metadata\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 977\u001b[0;31m               \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mag_error_metadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    978\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    979\u001b[0m               \u001b[0;32mraise\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:805 train_function  *\n        return step_function(self, iterator)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:795 step_function  **\n        outputs = model.distribute_strategy.run(run_step, args=(data,))\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:1259 run\n        return self._extended.call_for_each_replica(fn, args=args, kwargs=kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:2730 call_for_each_replica\n        return self._call_for_each_replica(fn, args, kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/distribute/distribute_lib.py:3417 _call_for_each_replica\n        return fn(*args, **kwargs)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:788 run_step  **\n        outputs = model.train_step(data)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/training.py:754 train_step\n        y_pred = self(x, training=True)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/base_layer.py:998 __call__\n        input_spec.assert_input_compatibility(self.input_spec, inputs, self.name)\n    /usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/engine/input_spec.py:223 assert_input_compatibility\n        str(tuple(shape)))\n\n    ValueError: Input 0 of layer sequential_5 is incompatible with the layer: expected ndim=3, found ndim=2. Full shape received: (100, 71)\n"
          ]
        }
      ]
    }
  ]
}